<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>希尔的博客</title>
  
  <subtitle>兰之猗猗，扬扬其香。不采而佩，于兰何伤？</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zshell.cc/"/>
  <updated>2018-05-13T08:05:13.962Z</updated>
  <id>http://zshell.cc/</id>
  
  <author>
    <name>zshell.zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>nginx module 使用总结: ngx_http_geo_module</title>
    <link href="http://zshell.cc/2018/05/13/nginx-module--nginx_module_%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93_ngx_http_geo_module/"/>
    <id>http://zshell.cc/2018/05/13/nginx-module--nginx_module_使用总结_ngx_http_geo_module/</id>
    <published>2018-05-13T07:45:02.000Z</published>
    <updated>2018-05-13T08:05:13.962Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在处理与 ip 地址相关的 nginx 逻辑上, ngx_http_geo_module 往往能发挥一些有力的作用; 其封装了大量与 ip 地址相关的匹配逻辑, 使得处理问题更加便捷高效;</p></blockquote><a id="more"></a><hr><p>ngx_http_geo_module 最主要的事情是作了一个 ip 地址到其他变量的映射; 一说到映射, 我们便会想起另一个模块: ngx_http_map_module; 从抽象上讲, geo 模块确实像是 map 模块在 ip (geography) 细分领域内的针对性功能实现;</p><h3 id="geo-模块的安装"><a href="#geo-模块的安装" class="headerlink" title="geo 模块的安装"></a><strong>geo 模块的安装</strong></h3><p>ngx_http_geo_module 编译默认安装, 无需额外操作;</p><h3 id="geo-模块的配置"><a href="#geo-模块的配置" class="headerlink" title="geo 模块的配置"></a><strong>geo 模块的配置</strong></h3><p>geo 模块的配置只能在 nginx.conf 中的 http 指令下, 这与 ngx_http_map_module 模块是一致的:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">ngx_command_t</span>  ngx_http_geo_commands[] = &#123;</span><br><span class="line"></span><br><span class="line">    &#123; ngx_string(<span class="string">"geo"</span>),</span><br><span class="line">      NGX_HTTP_MAIN_CONF|NGX_CONF_BLOCK|NGX_CONF_TAKE12,</span><br><span class="line">      ngx_http_geo_block,</span><br><span class="line">      NGX_HTTP_MAIN_CONF_OFFSET,</span><br><span class="line">      <span class="number">0</span>,</span><br><span class="line">      <span class="literal">NULL</span> &#125;,</span><br><span class="line"></span><br><span class="line">      ngx_null_command</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>geo 模块的配置模式如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">geo [<span class="variable">$address</span>] <span class="variable">$variable</span> &#123;</span><br><span class="line">    default     0;</span><br><span class="line">    127.0.0.1   1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中, \$address 可选, 默认从 <code>$remote_addr</code> 变量中获取目标 client ip address; 如果使用其他变量作为 ip 地址, 该变量须要是一个合法的 ip 地址, 否则将以 “255.255.255.255” 作为代替;<br>以下是一个典型的 geo 模块配置, \$address 已缺省默认为 <code>$remote_addr</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">geo <span class="variable">$flag</span> &#123;</span><br><span class="line">    <span class="comment"># 以下是一些设置项</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义可信地址, 若 $remote_addr 匹配了其中之一, 将从 request header X-Forwarded-For 获得目标 client ip address</span></span><br><span class="line">    proxy           192.168.100.0/24;</span><br><span class="line">    delete          127.0.0.0/16;</span><br><span class="line">    <span class="comment"># 默认兜底逻辑</span></span><br><span class="line">    default         -1;</span><br><span class="line">    <span class="comment"># 定义外部的映射内容</span></span><br><span class="line">    include         conf/geo.conf;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 以下是具体的映射内容</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 可以使用 CIDR 匹配</span></span><br><span class="line">    192.168.1.0/24  0;</span><br><span class="line">    <span class="comment"># 精确匹配</span></span><br><span class="line">    10.64.0.5       1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>除了以上的典型用法之外, geo 模块还有一种地址段范围的匹配模式:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">geo <span class="variable">$flag</span> &#123;</span><br><span class="line">    <span class="comment"># 需放在第一行</span></span><br><span class="line">    ranges;</span><br><span class="line">    192.168.1.0-192.168.1.100       0;</span><br><span class="line">    192.168.1.100-192.168.1.200     1;</span><br><span class="line">    192.168.1.201-192.168.1.255     2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="http://nginx.org/en/docs/http/ngx_http_geo_module.html" target="_blank" rel="noopener">Module ngx_http_geo_module</a></li><li><a href="http://www.ttlsa.com/nginx/using-nginx-geo-method/" target="_blank" rel="noopener">nginx geo 使用方法</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在处理与 ip 地址相关的 nginx 逻辑上, ngx_http_geo_module 往往能发挥一些有力的作用; 其封装了大量与 ip 地址相关的匹配逻辑, 使得处理问题更加便捷高效;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="nginx" scheme="http://zshell.cc/categories/nginx/"/>
    
      <category term="module" scheme="http://zshell.cc/categories/nginx/module/"/>
    
    
      <category term="nginx" scheme="http://zshell.cc/tags/nginx/"/>
    
      <category term="nginx:module" scheme="http://zshell.cc/tags/nginx-module/"/>
    
  </entry>
  
  <entry>
    <title>chattr / lsattr 使用总结</title>
    <link href="http://zshell.cc/2018/04/06/linux-disk--chattr_lsattr%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>http://zshell.cc/2018/04/06/linux-disk--chattr_lsattr使用总结/</id>
    <published>2018-04-06T13:23:22.000Z</published>
    <updated>2018-04-07T10:08:56.222Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对于在机器上操作的人来说, 如果有 sudo 权限, 那 chattr 根本就不是事, 这也不是 chattr 的意义所在;<br>对 chattr 来说, 其所要阻止的, 是那些有意无意想要修改机器上重要文件的程序, 从而保证机器上重要的文件不会因非人为因素而遭到非预期的操作;</p></blockquote><a id="more"></a><hr><h3 id="chattr-命令"><a href="#chattr-命令" class="headerlink" title="chattr 命令"></a><strong>chattr 命令</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># + 在原有参数基础上追加设置</span></span><br><span class="line"><span class="comment"># - 在原有参数基础上移除设置</span></span><br><span class="line"><span class="comment"># = 将设置更改为指定的参数</span></span><br><span class="line"><span class="comment"># mode 指定的设置项</span></span><br><span class="line">sudo chattr +|-|=mode file_path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归设置指定目录下的所有文件</span></span><br><span class="line">sudo chattr -R +|-|=mode file_path</span><br></pre></td></tr></table></figure><p>其中, mode 中常用的设置项如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a   设置只能向指定文件中追加内容, 不能删除</span><br><span class="line">i   设置文件不能修改, 删除, 不能被设置链接关系, 是最常用的 mode</span><br><span class="line">s   security, 当 rm 该文件时, 从磁盘上彻底删除它;</span><br></pre></td></tr></table></figure></p><p>chattr 并非万能, 以下几个目录 chattr 并不能干预:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/</span><br><span class="line">/dev</span><br><span class="line">/tmp</span><br><span class="line">/var</span><br></pre></td></tr></table></figure></p><h3 id="lsattr-命令"><a href="#lsattr-命令" class="headerlink" title="lsattr 命令"></a><strong>lsattr 命令</strong></h3><p>lsattr 命令用于查看文件被 chattr 设置的情况;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; lsattr file_path</span><br><span class="line">----i--------e- file_path</span><br></pre></td></tr></table></figure></p><p>可以发现, 有的时候 lsattr 所展示的文件属性掩码中, 有一个 <code>e</code>, 这在 chattr 的 manual 文档里是这么说的:</p><blockquote><p>The <code>e</code> attribute indicates that the file is using extents for mapping the blocks on disk. It may not be removed using chattr(1).</p></blockquote><p>所以说, 对 chattr 来说, 这个掩码并不意味着什么;</p><h3 id="常用的情景"><a href="#常用的情景" class="headerlink" title="常用的情景"></a><strong>常用的情景</strong></h3><p>对于生产环境中的机器, 有如下一些重要文件一般会将其用 chattr 设为不可修改, 不可删除:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo chattr +i /etc/resolv.conf</span><br><span class="line">sudo chattr +i /etc/hosts.allow</span><br><span class="line">sudo chattr +i /etc/hosts.deny</span><br></pre></td></tr></table></figure></p><p>其中, /etc/hosts.allow 与 /etc/hosts.deny 是关于 ssh 的登陆白名单/黑名单信息, 安全考虑, 正常只允许跳板机 ssh 到本机, 而禁止其他所有的机器; 这两个文件绝不允许被无故修改;<br>而 /etc/resolv.conf 则是关于 dns 解析的文件, 一旦被修改, 会导致一些网络请求中的域名无法正常解析, 所以也需要被 chattr 锁定防止无故修改;</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="http://www.ha97.com/5172.html" target="_blank" rel="noopener">Linux的chattr与lsattr命令详解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对于在机器上操作的人来说, 如果有 sudo 权限, 那 chattr 根本就不是事, 这也不是 chattr 的意义所在;&lt;br&gt;对 chattr 来说, 其所要阻止的, 是那些有意无意想要修改机器上重要文件的程序, 从而保证机器上重要的文件不会因非人为因素而遭到非预期的操作;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://zshell.cc/categories/linux/"/>
    
      <category term="disk" scheme="http://zshell.cc/categories/linux/disk/"/>
    
    
      <category term="linux:disk" scheme="http://zshell.cc/tags/linux-disk/"/>
    
      <category term="系统安全" scheme="http://zshell.cc/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch 6.x 升级调研报告</title>
    <link href="http://zshell.cc/2018/03/24/elasticsearch--elasticsearch6.x%E5%8D%87%E7%BA%A7%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/"/>
    <id>http://zshell.cc/2018/03/24/elasticsearch--elasticsearch6.x升级调研报告/</id>
    <published>2018-03-24T14:11:48.000Z</published>
    <updated>2018-04-07T10:01:40.155Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>关于 elasticsearch, 吐槽最多的就是其前后版本的兼容性问题; 在任何一个上规模的系统体系里, 要将部署在生产环境中的 elasticsearch 提升一个 major 版本是一件非常有挑战性的事情; 为了迎接这一挑战, 作者所在部门专门抽调人力资源作前期调研, 故为此文以记之;<br>在这篇文章中, 我将从 client 端, 索引创建, query dsl, search api, plugins, 监控体系等多方面讨论了从 2.4.2 版本迁移到 6.2.2 版本的一系列可能遇到的兼容性问题及解决方案;<br>希望能给各位读者带来工作上的帮助!</p></blockquote><a id="more"></a><hr><p><strong>万字长文, 高能预警! 如只希望了解最终结论, 请点击:</strong> <em><a href="#本文总结">本文总结</a>;</em><br>&nbsp;<br>戊戌年春, 历时余月, 本文终于迎来了收尾;<br>这篇文章缘起于部门自建 elasticsearch 集群的一个线上故障, 这是我们技术 TL 在 elastic 论坛的提问: <a href="https://discuss.elastic.co/t/es-consume-high-cpu-with-threadlocal/117402" target="_blank" rel="noopener">ES consume high cpu with threadlocal</a>; 随着业务规模的扩大, 业务数据的积累, 我们意识到当前 2.4.2 版本的 elasticsearch 已经满足不了我们的需求, 此刻亟需升级我们的集群; 比较之后, 我们打算将 6.2.2 版本作为升级的目标, 并着手开始调研; 本文即是该升级调研的一个总结报告;<br>相比于公司内部发表的版本, 本篇博客对所有涉及公司内部的信息作了脱敏处理, 并在开篇第一节补充介绍了一下我们使用 elasticsearch 的方式, 以方便外部读者更好得理解本文的其余部分内容;</p><h2 id="客户端兼容性问题"><a href="#客户端兼容性问题" class="headerlink" title="客户端兼容性问题"></a><strong>客户端兼容性问题</strong></h2><p>在这篇文章的编排结构中, 我将客户端兼容性问题摆在了第一的位置: 因为不管 rest api 如何变化, 或者如何不变, 都只能算是 “术”; 我们真正跑在生产环境中的系统, 使用的是 elasticsearch java client; client 端的基础兼容性问题才是根本之 “道”;</p><h3 id="巨轮转向的前提-es-adapter"><a href="#巨轮转向的前提-es-adapter" class="headerlink" title="巨轮转向的前提: es-adapter"></a><strong>巨轮转向的前提: es-adapter</strong></h3><p>我相信, 搞过 elasticsearch major 版本升级的人都对 elastic 公司深有体会: 从不按牌理出牌, 一个毫不妥协的技术理想主义者, 在其世界里根本没有兼容性这个词; 对于这样的公司做出的产品, 升级必定是一个痛苦的过程;<br>如果请求 elasticsearch 的代码逻辑散落在部门众多业务线的众多系统里, 要推动他们修改代码势必比登天还难: 因为这个过程对他们的 PKI 没有任何帮助, 只会挤占他们的工时, 增加他们的额外负担和 “无效” 工作量, 他们一定不会积极配合, 我们将无法推动进展;<br>还好部门的 VP 有技术远见,在各系统建立之初, 就定下了访问 elasticsearch 的规范: 禁止各系统自己主动连接 elasticsearch, 必须统一由专门的系统代理, 负责语法校验, 行为规范, 请求监控, 以及统一的调优; 其余的系统必须通过调用其暴露出去的 dubbo 接口间接访问 elasticsearch; 这个系统被命名为 es-adapter;<br>当然, es-adapter 系统设计的早期也有一些硬伤, 并直接诱发了一个严重的线上故障: <a href="">apache httpclient 初始化参数设置总结</a>; 那次事故之后, 甚至有技术 TL 开始怀疑 es-adapter 成为了当前体系的瓶颈, 需要评估有无必要废弃该系统; 但是船大掉头难, 整改谈何容易? 最后还是老老实实完善了 es-adapter 的逻辑继续使用;<br>有的时候 es-adapter 也会做一些语法兼容性的逻辑, 比如之前从 1.7.3 升级到 2.4.2 的时候, 部分 dsl 语法的改动就完全在 es-adapter 上代理了, 对业务线无感知, 轻描淡写地升级了一个 major 版本; 尽管这么做带来了一些技术债务, 但确实为有限时间内的快速升级提供了可能性; 在后面的时间, 业务线可以慢慢地迭代版本, 逐渐适配新 elasticsearch 版本的 api, 偿还债务; 正所谓: 万事之先, 圆方门户; 虽覆能复, 不失其度;<br>不得不说, 当系统规模与复杂度发展到了一个 “船大难掉头” 的程度时, es-adapter 就像是《三体》中描述的 “水滴” 一样, 带领整个体系从一个更高的维度完成 “平滑” 转向; 没有 es-adapter, 升级 elasticsearch 到 6.2.2 就无从谈起; 只不过这次的情形相比上一次有些难看, 没法做到完全透明了, es-adapter 部分特有的逻辑设计在这次升级可能会栽一个跟头, 具体的内容请见下文: <a href="#search-api-的兼容性">search api 的兼容性</a>;</p><h3 id="升级过渡期-client-端的技术选型"><a href="#升级过渡期-client-端的技术选型" class="headerlink" title="升级过渡期 client 端的技术选型"></a><strong>升级过渡期 client 端的技术选型</strong></h3><p>关于 elasticsearch java 官方客户端, 除了 TransportClient 之外, 最近又新出了一个 HighLevelClient, 而且官方准备在接下来的一两个 major 版本中, 让 HighLevelClient 逐步取代 TransportClient, 官方原话是这样描述的:</p><blockquote><p>We plan on deprecating the <code>TransportClient</code> in Elasticsearch 7.0 and removing it completely in 8.0.</p></blockquote><p>所以没有什么好对比的, 我们必须选择 HighLevelClient, 否则没两年 TransportClient 就要被淘汰了; 现在唯一需要考虑的是, 在升级过渡期, 怎么处理 es-adapter 中新 client 和旧 client的关系, 如何同时访问 6.2.2 与 2.4.2 两个集群;<br>值得注意的是, HighLevelClient 是基于 http 的 rest client, 这样一来, 在客户端方面, elasticsearch 将 java, python, php, javascript 等各种语言的底层接口就都统一起来了; 与此同时, 使用 rest api, 还可以屏蔽各版本之前的差异, 之前的 TransportClient 使用 serialized java object, 各版本之前的微小差异便会导致不兼容;<br>要使用 HighLevelClient, 其 maven 坐标需要引到如下三个包:<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- elasticsearch core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>6.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- low level rest client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch.client<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-rest-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>6.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- high level rest client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch.client<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-rest-high-level-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>6.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>后两者没的说, 都是新引入的坐标; 但是第一个坐标, elasticsearch 的核心 package, 就无法避免与现在 es-adapter 引的 2.4.2 版本冲突了;<br>之前从 1.7.3 升 2.4.2 时, 由于 TransportClient 跨 major 版本不兼容, 导致 es-adapter 无法用同一个 TransportClient 访问两个集群, 只能苦苦寻找有没有 rest 的解决方案, 后来总算找到一个: Jest (github 地址: <a href="https://github.com/searchbox-io/Jest" target="_blank" rel="noopener">searchbox-io/Jest</a>), 基本囊括了 elasticsearch 各种类别的请求功能;<br>但这还是架不住各业务线种种小众的需求(比如 nested_filter, function_score, aggregations 等等), 以致于对两个不同版本的集群, es-adapter 不能完美提供一致的功能;<br>这一次升 6.2.2, 又遇到了和上一次差不多的问题, 不过一个很大的不同是: 现在官方推荐的 HighLevelClient 是 rest client, 所以很有必要尝试验证下其向下兼容的能力;<br>我们经过 demo 快速测试验证, 初步得出了结论:<br>&nbsp;<br><strong>6.2.2 版本的 RestHighLevelClient 可以兼容 2.4.2 版本的 elasticsearch;</strong><br>&nbsp;<br>这也体现了 elasticsearch 官方要逐步放弃 TransportClient 并推荐 HighLevelClient 的原因: 基于 http 屏蔽底层差异, 最大限度地提升 client 端的兼容性; 后来我在其官方文档中也看到了相关的观点: <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-rest/6.2/java-rest-high-compatibility.html" target="_blank" rel="noopener">Compatibility</a>;<br>所以, 本次升级过渡期就不需要像上次 1.7.3 升 2.4.2 那么繁琐, 还要再引入一个第三方的 rest client; 现在唯一需要做的就是直接把 client 升级到 6.2.2, 使用 HighLevelClient 同时访问 2.4.2 和 6.2.2 两个版本;</p><h3 id="HighLevelClient-的使用注意事项"><a href="#HighLevelClient-的使用注意事项" class="headerlink" title="HighLevelClient 的使用注意事项"></a><strong>HighLevelClient 的使用注意事项</strong></h3><p><strong>(1) 初始化的重要选项</strong><br>HighLevelClient 底层基于 org.apache.httpcomponents, 一提起这个老牌 http client, 就不得不提起与它相关的几个关键 settings:</p><ol><li><code>CONNECTION_REQUEST_TIMEOUT</code></li><li><code>CONNECT_TIMEOUT</code></li><li><code>SOCKET_TIMEOUT</code></li><li><code>MAX_CONN_TOTAL</code></li><li><code>MAX_CONN_PER_ROUTE</code></li></ol><p>不过, HighLevelClient 关于这几个参数的设置有些绕人, 它是通过如下两个回调实现的:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">List&lt;HttpHost&gt; httpHosts = Lists.newArrayListWithExpectedSize(serverNum);</span><br><span class="line">serverAddressList.forEach((server) -&gt; httpHosts.add(<span class="keyword">new</span> HttpHost(server.getAddr(), server.getPort(), <span class="string">"http"</span>)));</span><br><span class="line"><span class="keyword">private</span> RestHighLevelClient highLevelClient = <span class="keyword">new</span> RestHighLevelClient(</span><br><span class="line">        RestClient.builder(httpHosts.toArray(<span class="keyword">new</span> HttpHost[<span class="number">0</span>]))</span><br><span class="line">        <span class="comment">// timeout settings</span></span><br><span class="line">        .setRequestConfigCallback((callback) -&gt; callback</span><br><span class="line">                .setConnectTimeout(CONNECT_TIMEOUT_MILLIS)</span><br><span class="line">                .setSocketTimeout(SOCKET_TIMEOUT_MILLIS)</span><br><span class="line">                .setConnectionRequestTimeout(CONNECTION_REQUEST_TIMEOUT_MILLIS))</span><br><span class="line">        <span class="comment">// connections total and connections per host</span></span><br><span class="line">        .setHttpClientConfigCallback((callback) -&gt; callback</span><br><span class="line">                .setMaxConnPerRoute(MAX_CONN_PER_ROUTE)</span><br><span class="line">                .setMaxConnTotal(MAX_CONN_TOTAL)</span><br><span class="line">        )</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><p><strong>(2) request timeout 的设置</strong><br>对于 index, update, delete, bulk, query 这几个请求动作, HighLevelClient 与它们相关的 Request 类都提供了 timeout 设置, 都比较方便; 但是, 偏偏 get 与 multiGet 请求没有提供设置 timeout 的地方;<br>这就有点麻烦了, get 与 multiGet 是重要的请求动作, 绝对不能没有 timeout 机制: 之前遇到过的几次惨痛故障, 都无一例外强调了合理设置 timeout 的重要性;<br>那么, 这种就只能自己动手了, 还好 HighLevelClient 对每种请求动作都提供了 async 的 api, 我可以结合 CountDownLatch 的超时机制, 来实现间接的 timeout 控制;<br>首先需要定义一个 response 容器来盛装异步回调里拿到的 result:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResponseWrapper</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> T response;</span><br><span class="line">    <span class="keyword">private</span> Exception exception;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> T <span class="title">getResponse</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> response; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setResponse</span><span class="params">(T response)</span> </span>&#123; <span class="keyword">this</span>.response = response; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Exception <span class="title">getException</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> exception; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setException</span><span class="params">(Exception exception)</span> </span>&#123; <span class="keyword">this</span>.exception = exception;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>下面是使用 CountDownLatch 实现 timeout 的 get 请求具体逻辑:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* get request with timeout */</span></span><br><span class="line"><span class="keyword">final</span> ResponseWrapper&lt;GetResponse&gt; wrapper = <span class="keyword">new</span> ResponseWrapper&lt;&gt;();</span><br><span class="line"><span class="keyword">final</span> CountDownLatch latch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line">highLevelClient.getAsync(request, <span class="keyword">new</span> ActionListener&lt;GetResponse&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onResponse</span><span class="params">(GetResponse documentFields)</span> </span>&#123;</span><br><span class="line">        wrapper.setResponse(documentFields);</span><br><span class="line">        latch.countDown();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Exception e)</span> </span>&#123;</span><br><span class="line">        wrapper.setException(e);</span><br><span class="line">        wrapper.setResponse(<span class="keyword">null</span>);</span><br><span class="line">        latch.countDown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    latch.await(getTimeOutTime(indexName, TimeUnit.MILLISECONDS);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ElasticsearchTimeoutException(<span class="string">"timeout"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (wrapper.getResponse() == <span class="keyword">null</span>) &#123; <span class="comment">// 异常处理 &#125; </span></span><br><span class="line"><span class="keyword">else</span> &#123; 处理 wrapper.getResponse() 的返回结果 &#125;</span><br></pre></td></tr></table></figure></p><p><strong>(3) query 请求 dsl 的传参问题</strong><br>es-adapter 之前查询相关的请求动作, 对业务线提供的接口是基于 search api 设计的, 就是下面这样的模型:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"query"</span>: &#123; ... &#125;,</span><br><span class="line">    <span class="string">"_source"</span>: &#123;</span><br><span class="line">        <span class="string">"include"</span>: [ ... ],</span><br><span class="line">        <span class="string">"exclude"</span>: [ ... ]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"from"</span>: xxx,</span><br><span class="line">    <span class="string">"size"</span>: yyy,</span><br><span class="line">    <span class="string">"sort"</span>: [ ... ],</span><br><span class="line">    <span class="string">"aggs"</span>: &#123; ... &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>业务线需要提供以上参数给 es-adapter, 而这里面最重要的就是第一个 query 参数, 这里原先设计的是传一个 dsl 字符串; 但是现在我发现 HighLevelClient 的 SearchSourceBuilder 不能直接 set 一个字符串, 而必须是使用各种 QueryBuilder 去构造对应的 Query 对象;<br>这个问题就比较严重了, 如果要改就是牵涉到所有的业务线; 而且即便是想改, 也没那么简单: 这些 QueryBuilders 都没有实现 Serializable 接口, 根本没法被 dubbo 序列化;<br>权衡之下, 感觉还是要努力想办法把 dsl 字符串 set 进去; 我看到 SearchSourceBuilder 有一个方法是 fromXContent(XContentParser parser), 考虑到 dsl 字符串其实都是 json, 可以使用 JsonXContent 将 dsl 反序列化成各种 QueryBuilders; 摸索了一阵子, 验证了以下代码是可行的:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String dslStr = <span class="string">"..."</span>;</span><br><span class="line">SearchModule searchModule = <span class="keyword">new</span> SearchModule(Settings.EMPTY, <span class="keyword">false</span>, Collections.emptyList());</span><br><span class="line">XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(</span><br><span class="line">        <span class="keyword">new</span> NamedXContentRegistry(searchModule.getNamedXContents()), dslStr);</span><br><span class="line"></span><br><span class="line">SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.fromXContent(parser);</span><br></pre></td></tr></table></figure></p><p><strong>(4) 无厘头的 <code>adjust_pure_negative</code></strong><br>整个 HighLevelClient 中, 最让人感到费解的一个东西就是一个神秘的属性:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* org.elasticsearch.index.query.BoolQueryBuilder */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ParseField ADJUST_PURE_NEGATIVE = <span class="keyword">new</span> ParseField(<span class="string">"adjust_pure_negative"</span>);</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> adjustPureNegative = ADJUST_PURE_NEGATIVE_DEFAULT;</span><br></pre></td></tr></table></figure></p><p>先是看官方文档, 搜不到;<br>然后搜 google, 就找到这么一个稍微相关一点的帖子: <a href="https://discuss.elastic.co/t/what-does-adjust-pure-negative-flag-do/92348" target="_blank" rel="noopener">What does “adjust_pure_negative” flag do?</a>, 而其给出的唯一回复是 “<strong>You can ignore it</strong>“;<br>实在搜不到有效的信息, 我只好去扒源码; 然而, 除了如上所述的 BoolQueryBuilder 中的这坨, 再加上一些测试类, 就再也没在其他地方看到与 <code>adjust_pure_negative</code> 相关的逻辑了;<br>也许真的如 elastic 讨论组中所说的 <em>You can ignore it?</em> 但是现在有一个问题让我无法忽略它: 这个属性无法被 2.4.2 的 elasticsearch 识别, 但在 6.2.2 的 elasticsearch 中, 各个 QueryBuilder 的 toString() 方法会自动将其带上:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"query"</span>: &#123;</span><br><span class="line">    <span class="string">"bool"</span>: &#123;</span><br><span class="line">      <span class="string">"must"</span>: &#123; ... &#125;,</span><br><span class="line">      <span class="string">"should"</span>: &#123; ... &#125;,</span><br><span class="line">      <span class="string">"must_not"</span>: &#123; ... &#125;,</span><br><span class="line">      <span class="string">"adjust_pure_negative"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="string">"boost"</span>: <span class="number">1.0</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在上一节中提到, es-adapter 接受业务线传来的 query dsl str, 使用 6.2.2 的 elasticsearch 便会将上述语句传给 es-adapter; 如果其访问的索引已经迁移到 6.2.2 新集群, 那么该语句没问题; 但如果其访问的索引还未来得及迁移到新集群, es-adapter 会将该请求路由到旧的 2.4.2 集群, 接着便会发生语法解析异常;<br>&nbsp;<br><em>这意味着, 在某个系统所需要访问的所有索引迁移到 6.2.2 新集群之前, 其 maven 依赖的 elasticsearch 版本, 不能提前升级到 6.2.2, 以阻止 adjust_pure_negative 的生成;</em><br><em>当然考虑到 major 版本升级所带来的语法规则的巨变已被 es-adapter 缓冲掉了绝大部分, 我相信各业务线也不希望把 elasticsearch 的 maven 版本给直接升上去的; 毕竟那意味着代码将红成一大片, 要花费大量的精力修改代码, 这等于把 es-adapter 原本要替其做的事, 提前自己给办了;</em></p><p><strong>(5) 其他小众的需求</strong><br>以上展示的是业务线普遍会遇到的情况, 然后还有两个比较小众的需求, 在个别系统中会使用到,  也是上面所提到的 nested_filter 和 aggregations;<br>关于 nested_filter 还稍顺利些, api 有一些变化但是新的 api 有新的解决方案:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 6.2.2 版本: 构造一个 携带 nested_filter 的 sort</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> SortBuilder <span class="title">buildNestedSort</span><span class="params">(NestedSort nestedSort)</span> </span>&#123;</span><br><span class="line">    QueryBuilder termFilter = QueryBuilders.termsQuery(nestedSort.getTermField(), nestedSort.getTermValue());</span><br><span class="line">    <span class="keyword">return</span> SortBuilders.fieldSort(nestedSort.getSortName())</span><br><span class="line">            .setNestedSort(<span class="keyword">new</span> NestedSortBuilder(nestedSort.getNestedPath()).setFilter(termFilter))</span><br><span class="line">            .order(nestedSort.getOrder())</span><br><span class="line">            .sortMode(SortMode.fromString(nestedSort.getSortMode()))</span><br><span class="line">            .missing(nestedSort.getMissing());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>只不过这种顺利是建立在之前的不顺利基础上的: org.elasticsearch.search.sort.SortBuilders 没有实现 java.io.Serializable 接口, 各业务线的系统没法通过 dubbo 接口把参数传给我, 所以不得不自定义了上面的 NestedSort 类用于盛装 nested sort filter 的相关参数:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NestedSort</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String sortName; <span class="comment">// 排序用的字段名</span></span><br><span class="line">    <span class="keyword">private</span> SortOrder order = SortOrder.ASC;</span><br><span class="line">    <span class="keyword">private</span> String missing; <span class="comment">// _first/_last,如果指定的字段不存在的排序逻辑</span></span><br><span class="line">    <span class="keyword">private</span> String sortMode; <span class="comment">// max/min/sum/avg</span></span><br><span class="line">    <span class="keyword">private</span> String nestedPath;</span><br><span class="line">    <span class="keyword">private</span> String termField; <span class="comment">// filter对应的 term 的field,现在只支持terms;</span></span><br><span class="line">    <span class="keyword">private</span> Collection&lt;String&gt; termValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>好在 nested_filter 相关参数类别可以固化, 比较稳定, 自定义类也算是个解决方案了;<br>&nbsp;<br>但是另一个小众需求就没那么省事了: aggregations; 之前 2.4.2 的 agg api 中, 有一个通用的方法:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SearchRequestBuilder <span class="title">setAggregations</span><span class="params">(<span class="keyword">byte</span>[] aggregations)</span> </span>&#123;</span><br><span class="line">    sourceBuilder().aggregations(aggregations);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>elasticsearch 聚合的 api 比较丰富自由, 而上面方法中的 aggregations 参数是以字节的形式传过来的, 所以业务线可以自由发挥, 不受 es-adapter 的约束, 但可惜这个方法在 6.2.2 版本中取消了;<br>这样一来不得不回到束缚之中, 针对不同的聚合类型作各自的处理了; 可惜各个聚合类型依然没有实现 java.io.Serializable 接口, 所以还是得自定义类型去盛装参数了; 比如以下是针对分位数的聚合:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PercentileAggregation</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String aggName;</span><br><span class="line">    <span class="keyword">private</span> String aggField;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span>[] percents;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PercentilesAggregationBuilder percentileAggBuilder = AggregationBuilders.percentiles(param.getPercentileAggregation().getAggName())</span><br><span class="line">        .field(param.getPercentileAggregation().getAggField())</span><br><span class="line">        .percentiles(param.getPercentileAggregation().getPercents());</span><br><span class="line">searchSourceBuilder.aggregation(percentileAggBuilder)</span><br></pre></td></tr></table></figure><p>其他的聚合类型不再一一列举; 关于 aggregations 的 api 变化着实比较大, 好在使用它的系统比较少, 推动其修改逻辑阻力亦不是很大;<br>&nbsp;<br>HighLevelClient 的使用基本上要解决的就是以上几个问题了; 解决了客户端的问题, 就是解决了 “道” 的问题, 剩下的 “术” 的问题, 都已不是主要矛盾了;</p><h2 id="语法兼容性问题"><a href="#语法兼容性问题" class="headerlink" title="语法兼容性问题"></a><strong>语法兼容性问题</strong></h2><p>语法兼容性问题便是上文所提及 “术” 的问题的主要表现形式; 这一节主要讨论三个方面: 索引创建的兼容性, query dsl 的兼容性, search api 的兼容性;</p><h3 id="索引创建的兼容性"><a href="#索引创建的兼容性" class="headerlink" title="索引创建的兼容性"></a><strong>索引创建的兼容性</strong></h3><p>es 6.2 在索引创建方面, 有如下几点与 es 2.4 有区别:<br>&nbsp;<br><strong>首先是 settings 中的区别;</strong><br>&nbsp;<br>部分字段不能出现在索引创建语句中了, 只能由 elasticsearch 自动生成;<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"settings"</span>:&#123;</span><br><span class="line">    <span class="string">"index"</span>:&#123;</span><br><span class="line">        <span class="comment">// creation_date 不能出现在索引创建的定义语句里</span></span><br><span class="line">        <span class="string">"creation_date"</span>: <span class="string">"1502713848656"</span>,</span><br><span class="line">        <span class="string">"number_of_shards"</span>:<span class="string">"2"</span>,</span><br><span class="line">        <span class="string">"analysis"</span>:&#123;</span><br><span class="line">            <span class="string">"analyzer"</span>:&#123;</span><br><span class="line">                <span class="string">"comma_analyzer"</span>:&#123;</span><br><span class="line">                    <span class="string">"type"</span>:<span class="string">"custom"</span>,</span><br><span class="line">                    <span class="string">"tokenizer"</span>:<span class="string">"comma_tk"</span></span><br><span class="line">                &#125;   </span><br><span class="line">            &#125;,  </span><br><span class="line">            <span class="string">"tokenizer"</span>:&#123;</span><br><span class="line">                <span class="string">"comma_tk"</span>:&#123;</span><br><span class="line">                    <span class="string">"pattern"</span>:<span class="string">","</span>,</span><br><span class="line">                    <span class="string">"type"</span>:<span class="string">"pattern"</span></span><br><span class="line">                &#125;   </span><br><span class="line">            &#125;   </span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"number_of_replicas"</span>:<span class="string">"1"</span>,</span><br><span class="line">        <span class="comment">// uuid 不能出现在索引创建的定义语句里</span></span><br><span class="line">        <span class="string">"uuid"</span>:<span class="string">"Oa0tz0x-SpSfuC591_ASIQ"</span>,</span><br><span class="line">        <span class="comment">// version.create, version.update 不能出现在索引创建的定义语句里</span></span><br><span class="line">        <span class="string">"version"</span>:&#123;</span><br><span class="line">            <span class="string">"created"</span>:<span class="string">"1070399"</span>,</span><br><span class="line">            <span class="string">"upgraded"</span>:<span class="string">"2040299"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><em>这算是一个规范化, 这些字段原本就不该自己定义, 之前我们是复制的时候图省事, 懒得删掉, 现在不行了;</em><br>&nbsp;<br><strong>然后是 mappings 中的区别;</strong><br>&nbsp;<br><strong>(1) 布尔类型的取值内容规范化</strong><br>elasticsearch 索引定义的 settings/mappings 里有很多属性是布尔类型的开关; 在 6.x 之前的版本, elasticsearch 对布尔类型的取值内容限制很宽松: true, false, on, off, yes, no, 0, 1 都可以接受, 产生了一些混乱, 对初学者造成了困扰:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// elasticsearch 2.4.2</span></span><br><span class="line"><span class="comment">// xxx_idx/_mapping/field/xxx_field</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"xxx_idx"</span>:&#123;</span><br><span class="line">        <span class="string">"mappings"</span>:&#123;</span><br><span class="line">            <span class="string">"xxx_type"</span>:&#123;</span><br><span class="line">                <span class="string">"xxx_field"</span>:&#123;</span><br><span class="line">                    <span class="string">"full_name"</span>:<span class="string">"xxx_field"</span>,</span><br><span class="line">                    <span class="string">"mapping"</span>:&#123;</span><br><span class="line">                        <span class="string">"xxx_field"</span>:&#123;</span><br><span class="line">                            <span class="string">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                            <span class="string">"index_name"</span>:<span class="string">"xxx_field"</span>,</span><br><span class="line">                            <span class="comment">// 以下属性都有布尔类型的含义, 但取值五花八门, 容易造成歧义</span></span><br><span class="line">                            <span class="string">"index"</span>:<span class="string">"not_analyzed"</span>, </span><br><span class="line">                            <span class="string">"store"</span>:<span class="literal">false</span>,</span><br><span class="line">                            <span class="string">"doc_values"</span>:<span class="literal">false</span>,</span><br><span class="line">                            <span class="string">"term_vector"</span>:<span class="string">"no"</span>,</span><br><span class="line">                            <span class="string">"norms"</span>:&#123;</span><br><span class="line">                                <span class="string">"enabled"</span>:<span class="literal">false</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            <span class="string">"null_value"</span>:<span class="literal">null</span>,</span><br><span class="line">                            <span class="string">"include_in_all"</span>:<span class="literal">false</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>从 6.x 版本开始, 所有的布尔类型的属性 elasticsearch 只接受两个值: <code>true</code> 或 <code>false</code>;<br><em>从当前 2.4.2 集群的使用状况来看, 这个改动对我们的影响不是特别大, 因为我们在定义索引创建 DSL 语句时, 很多布尔类型的选项都是用的默认值, 并未显式定义, 只有 <code>index</code> 属性可能会经常用到;</em></p><p><strong>(2) _timestamp 字段被废弃</strong><br><em>这个改变对我们的影响不是很大, 我们现在绝大部分索引都会自己定义 createTime / updateTime 字段, 用于记录该文档的创建 / 更新时间, 几乎不依赖系统自带的 _timestamp 字段;</em><br>&nbsp;<br>况且, _timestamp 字段在 2.4.2 版本时, 就已经默认不自动创建了, 要想添加 _timestamp 字段, 必须这样定义:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"_timestamp"</span>: &#123;</span><br><span class="line">    <span class="string">"enabled"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当然, 在 6.2.2 版本中, 以上定义就直接报 unsupported parameter 错误了;</p><p><strong>(3) _all 字段被 deprecated, include_in_all 属性被废弃</strong><br>在 elasticsearch 6.x, _all 字段被 deprecated 了, 与此同时, _all 字段的 enabled 属性默认值也由 true 改为了 false;<br>之前, 为了阻止 _all 字段生效, 我们都会不遗余力得在每个索引创建语句中加上如下内容:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"_all"</span>: &#123;</span><br><span class="line">    <span class="string">"enabled"</span>: <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>从 6.0 版本开始, 这些语句就不需要再出现了, 出现了反而会导致 elasticsearch 打印 WARN 级别的日志, 告诉我们 _all 字段已经被 deprecated, 不要再对其作配置了;<br>与 _all 密切相关的属性是 include_in_all, 在 6.0 版本之前, 这个属性值默认也是 true; 不过不像 _all 的过渡那么温和, 从 6.0 开始, 我在 elasticsearch reference 官方文档里就找不到这个属性的介绍了, 直接被废弃; 而在其上一个版本 5.6 中, 我还能看到它, 也没有被 deprecated, 着实有些突然;<br>elasticsearch 放弃 _all 这个概念, 是希望让 query_string 时能够更加灵活, 其给出的替代者是 <code>copy_to</code> 属性:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"properties"</span>: &#123;</span><br><span class="line">    <span class="string">"first_name"</span>: &#123;</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">        <span class="string">"copy_to"</span>: <span class="string">"full_name"</span> </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"last_name"</span>: &#123;</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">        <span class="string">"copy_to"</span>: <span class="string">"full_name"</span> </span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"full_name"</span>: &#123;</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"text"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这样, 把哪些字段 merge 到一起, merge 到哪个字段里, 都是可以自定义的, 而不用束缚在固定的 _all 字段里;<br>&nbsp;<br><em>无论如何, _all 与 include_in_all 的废弃对我们来说影响都是很小的, 首先我们就很少有全文检索的场景, 其次我们也没有使用 query_string 查询 merged fields 的需求, 甚至将 _all 禁用已被列入了我们索引创建的规范之中;</em></p><p><strong>(4) 史诗级大改变: string 类型被废弃</strong><br>string 类型被废弃, 代替者是分词的 <code>text</code> 类型和不分词的 <code>keyword</code> 类型;<br>当前正在使用的 2.4.2 版本的集群里, string 类型大概是被使用最多的类型了; 保守估计, 一个普通的索引里, 60%  以上的字段类型都是 string; 现在 6.x 把这个类型废弃了, 就意味着几乎所有索引里的大多数字段都要修改;<br>&nbsp;<br><em>不过好在, 这种修改也只是停留在 index 的 schema 映射层面, 对 store 于底层的 document 而言是完全透明的, 所有原始数据都不需要有任何修改;</em><br>&nbsp;<br>经过搜索发现, 其实早在 elasticsearch 5.0 时, string 类型就已经被 deprecated 了, 然后在 6.1 时被彻底废弃, 详细的 changelog 见官方文档: <a href="https://github.com/elastic/elasticsearch-dsl-py/blob/master/Changelog.rst" target="_blank" rel="noopener">Changelog</a>;<br>仔细一想, 这个改变是有道理的: elasticsearch 想要结束掉目前混乱的概念定义;<br>比如说, 在 5.0 之前的版本, 一个字符串类型的字段, 是这样定义的:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"xxx"</span>: &#123;</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">    <span class="string">"index"</span>: <span class="string">"not_analyzed"</span> <span class="comment">// 不需要分词, 但要索引</span></span><br><span class="line">&#125;,</span><br><span class="line"><span class="string">"yyy"</span>: &#123;</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">    <span class="string">"index"</span>: <span class="string">"no"</span> <span class="comment">// 不需要分词, 也不需要索引</span></span><br><span class="line">&#125;,</span><br><span class="line"><span class="string">"zzz"</span>: &#123;</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"string"</span> <span class="comment">// 默认情况, 需要索引, 也需要分词</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>index</code> 的原本含义是定义是否需要索引, 是一个布尔概念; 但由于字符串类型的特殊性, 索引的同时还需要再区分是否需要分词, 结果 index 属性被设计为允许设置成 <code>not_analyzed</code>, <code>analyzed</code>, <code>no</code> 这样的内容; 然后其他诸如数值类型, 亦被其拖累, index 属性的取值也需要在 <code>not_analyzed</code>, <code>no</code> 中作出选择; 不得不说这非常混乱;<br>要把这块逻辑理清楚, 第一个选择是再引入一个控制分词的开关 word_split, 只允许字符串类型使用, 第二种选择就是把字符串类型拆分成 text 和 keyword;<br>至于 elasticsearch 为何选择了第二种方案, 我猜主要还是默认值不好确定; 对初学者而言, 一般都习惯于使用默认值, 但是究竟默认要不要分词? 以 elasticsearch 的宗旨和初衷来看, 要分词, search every where; 但是以实际使用者的情况来看, 很多的场景下都不需要分词; 如果是把类型拆分, 那么就得在 text 和 keyword 中二选一, 不存在默认值, 使用者自然会去思考自己真正的需求;<br>现在逻辑理清楚了, <code>index</code> 的取值类型, 也就如上一节所说的, 必须要在 <code>true</code> 或 <code>false</code> 中选择, 非常清晰;</p><p><strong>(5) mapping 中取消 multi types</strong><br>从 elasticsearch 6.1 开始, 同一个 index(mapping) 下不允许创建多个 type, index 与 type 必须一一对应; 从下一个 major 版本开始, elasticsearch 将废弃 type 的概念, 详见官方文档: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.2/removal-of-types.html" target="_blank" rel="noopener">Removal of mapping types</a>;<br>由于底层 Lucene 的限制, 同一个 index 下的不同 type 中的同名的字段, 其背后是共享的同一个 lucene segment; 这就意味着, 同一个 index 下不同 type 中的同名字段, 类型定义也必须相同; 原文如下:</p><blockquote><p>In an Elasticsearch index, fields that have the same name in different mapping types are backed by the same Lucene field internally; In other words, both fields must have the same mapping (definition) in both types.</p></blockquote><p>&nbsp;<br><em>这个改变对我们是有些影响的, 我们有小一部分的索引都存在 multi types 的问题, 这就意味着需要新建索引来承接多出来的 type, 这些索引的使用者必须要修改代码, 使用新的索引名访问不同的 type;</em></p><h3 id="query-dsl-的兼容性"><a href="#query-dsl-的兼容性" class="headerlink" title="query dsl 的兼容性"></a><strong>query dsl 的兼容性</strong></h3><p>索引创建的兼容性调研只能算是一个热身, 按照以往经验, elasticsearch 一旦有 major 版本升级, query dsl 变动都不会小, 这次也不例外;</p><p><strong>(1) filtered query 被废弃</strong><br>其实早在 2.0 版本时, filtered query 就已经被 deprecated 了, 5.0 就彻底废弃了; 这的确是一个不太优雅的设计, 在本来就很复杂的 query dsl 中又增添了一个绕人的概念;<br>filtered query 原本的设计初衷是想在一个 query context 中引入一个 filter context 作前置过滤: </p><blockquote><p>Exclude as many document as you can with a filter, then query just the documents that remain.</p></blockquote><p>然而, filtered query 这样的命名方式, 让人怎么也联系不了上面的描述; 其实要实现上述功能, elasticsearch 有另一个更加清晰的语法: bool query, 详细的内容在接下来的第 (2) 小节介绍;<br>&nbsp;<br><em>从目前 es-adapter 的使用情况来看, 依然有请求会使用到 filtered query; 好在 filtered 关键字一般出现在 dsl 的最外层, 比较固定, 这块可以在 es-adapter 中代理修改:</em><br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="comment">// 在 es-adapter 中删除 filtered</span></span><br><span class="line">  <span class="string">"filtered"</span>: &#123;</span><br><span class="line">    <span class="comment">// 如果有 filter, 将其移动到 query -&gt; bool 中</span></span><br><span class="line">    <span class="string">"filter"</span>: &#123; ... &#125;,</span><br><span class="line">    <span class="string">"query"</span>: &#123;</span><br><span class="line">      <span class="string">"bool"</span>: &#123;</span><br><span class="line">        <span class="string">"must"</span>: &#123; ... &#125;,</span><br><span class="line">        <span class="string">"should"</span>: &#123; ... &#125;,</span><br><span class="line">        <span class="string">"must_not"</span>: &#123; ... &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>(2) filter context 被限定在 bool query 中使用</strong><br>如下所示, 以下 dsl 是 elasticsearch 6.x 中能够使用 filter context 的唯一方式, 用于取代第 (1) 小节所说的 filtered query:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"query"</span>: &#123;</span><br><span class="line">    <span class="string">"bool"</span>: &#123;</span><br><span class="line">      <span class="comment">// 引入 filter context 作前置过滤</span></span><br><span class="line">      <span class="string">"filter"</span>: &#123; ... &#125;,</span><br><span class="line">      </span><br><span class="line">      <span class="string">"must"</span>: &#123; ... &#125;,</span><br><span class="line">      <span class="string">"should"</span>: &#123; ... &#125;,</span><br><span class="line">      <span class="string">"must_not"</span>: &#123; ... &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>&nbsp;<br><em>由于这个规范只是一个限定, 而不是废弃, 所以对目前生产环境肯定是没有影响, 只是需要各业务线慢慢将使用方式改成这种规范, 否则以后也会带来隐患;</em></p><p><strong>(3) and/or/not query 被废弃</strong><br>与 filtered query 不同, and query, or query, not query 这三个是语义清晰, 见名知意的 query dsl, 但是依然被 elasticsearch 废弃了, 所有 and, or, not 逻辑, 现在只能使用 bool query 去实现, 如第 (2) 小节所示;<br>可以发现, elasticsearch 以前为了语法的灵活丰富, 定义了各种各样的关键字; 要实现同一个语义的查询, 可以使用几种不同的 query dsl; 很多时候, 这样导致的结果, 就是让新人感到眼花缭乱, 打击了学习热情;<br>现在 and query, or query, not query 被废弃, 干掉了冗余的设计, 精简了 query dsl 的体系, 不得不说这是一件好事;<br>但从另一个角度讲, 每逢 major 版本升级就来一次大动作, 破坏了前后版本的兼容性, 让使用者很头疼; 想想 java 为了兼容性到现在都还不支持真正的泛型, 要是换 elastic 公司来操作, 估计 JDK 1.6 就准备放弃兼容了;<br>&nbsp;<br><em>从 es-adapter 的使用情况来看, 目前业务线基本没有 and/or/not query 的使用, 相关逻辑大家都使用的 bool query, 所以这一点对我们影响有限;</em></p><p><strong>(4) missing query 被废弃</strong><br>要实现 missing 语义的 query, 现在必须统一使用 must_not exists:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"query"</span>: &#123;</span><br><span class="line">        <span class="string">"bool"</span>: &#123;</span><br><span class="line">            <span class="string">"must_not"</span>: &#123;</span><br><span class="line">                <span class="string">"exists"</span>: &#123;</span><br><span class="line">                    <span class="string">"field"</span>: <span class="string">"xxx"</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这也算是对 query dsl 体系的精简化: 可以用 exists query 实现的功能, 就不再支持冗余的语法了;<br>&nbsp;<br><em>这个改动对我们是有一定影响的, 目前不少的 query 都还在使用 missing;</em><br><em>另外, 由于从 missing 改为 must_not exists 结构变化大, 而且 missing 的使用比较灵活, 在 dsl 中出现的位置不固定, 这两个因素叠加, 导致在 es-adapter 中代理修改的难度非常高, 基本不可行;</em><br><em>所以, 关于 missing , 必须由业务线自己来修改相关代码了;</em></p><h3 id="search-api-的兼容性"><a href="#search-api-的兼容性" class="headerlink" title="search api 的兼容性"></a><strong>search api 的兼容性</strong></h3><p>相比于 query dsl 的巨大改变, search api 总体上延续了之前的设计, 仅有部分 search type 被废弃; 感觉上比较温和, 可惜却因为 es-adapter 一些没有前瞻性的设计而闪着了腰;</p><p><strong>(1) search_type <code>scan</code> 被废弃</strong><br>关于这一点, 我们早就作好了心理准备; 早在从 1.7.3 升 2.4.2 的时候, 我们就已经发现 scan 这种 search type 被 deprecated 了, 从 5.0 开始, 就要被彻底废弃了, 所以 es-adapter 同期开始支持真正的 scroll 请求 (可惜业务线使用得不多);<br>从类别上说, scan 只不过是 scroll 操作中的一种特例: 不作 sort, fetch 后不作 merge; 从执行效果上看, scan 相比 scroll 可能稍微快一些, 并会获得 shards_num * target_size 数量的结果集大小; 除此之外, 没有其他什么区别;<br>&nbsp;<br><em>然而, 理论上很简单, 实际上却很棘手: 这源于 es-adapter 一个比较糟糕的设计:</em><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* es-adapter 的查询服务 */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ResponseContent <span class="title">query</span><span class="params">(RequestParam param)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (param.getSearchType().equals(SearchType.SCAN) || param.getUseScroll()) &#123;</span><br><span class="line">        <span class="keyword">return</span> scroll(param);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (param.getSearchType().equals(SearchType.COUNT)) &#123;</span><br><span class="line">        <span class="keyword">return</span> count(param);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> normallyQuery(param);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><em>可以发现, 在当前的逻辑中, 业务线的 scan 请求, 是通过调用 query 方法并设置 search type 为 scan 来实现的; 这里的 scroll(param) 方法是个 private 方法; 当 es-adapter 升级 api 到 6.2.2 后, 就识别不了 scan 了;</em><br><em>这就要求 es-adapter 修改 scroll(param) 方法为 public, 然后各业务线直接调 scroll(param) 方法; 这需要一定的修改工作量;</em></p><p><strong>(2) search_type <code>count</code> 被废弃</strong><br>count 与 scan 一样早在 2.4.2 时就已经被 deprecated 了, 不过之前我们对 count 的关注度没有 scan 高; 在 2.4.2 版本 SearchType 类的源码注释中, elastic 官方是这么说明的:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Only counts the results, will still execute aggregations and the like.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@deprecated</span> does not any improvements compared to &#123;<span class="doctag">@link</span> #QUERY_THEN_FETCH&#125; with a `size` of &#123;<span class="doctag">@code</span> 0&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Deprecated</span></span><br><span class="line">COUNT((<span class="keyword">byte</span>) <span class="number">5</span>);</span><br></pre></td></tr></table></figure></p><p>对于 es-adapter 来说, 修改方法很明显, 正如注释中所述: 该怎么请求就怎么请求, 拿到 response 后从里面取出 totalHits 就行了;<br>&nbsp;<br><em>可惜, 如上一节所述, 同样由于 es-adapter 中糟糕的逻辑, 业务线需要通过调用 query 方法并设置 search type 为 count 来实现 count 请求; 现在没有 count 这个 search type 了, 需要业务线改成直接调用 count(param) 方法;</em></p><p><strong>(3) search_type <code>query_and_fetch</code> 被 deprecated, <code>dfs_query_and_fetch</code> 被废弃</strong><br>这两个 search type 被 deprecated 的时间比 scan 和 count 稍晚一些; 好在这两个 search type 比较冷门, 业务线知道的不多, 所以用的也不多; 后来只要发现有人这么用, 我们就会告诉他们这个 api 已经不推荐了;<br>&nbsp;<br><em>所以, 相比 scan 和 count, query_and_fetch 和 dfs_query_and_fetch 被废弃的影响十分有限;</em></p><h2 id="底层索引数据兼容性问题"><a href="#底层索引数据兼容性问题" class="headerlink" title="底层索引数据兼容性问题"></a><strong>底层索引数据兼容性问题</strong></h2><p>根据官方文档, 6.x 版本可以兼容访问 5.x 创建的索引; 5.x 版本可以兼容 2.x 创建的索引;<br>背后其实是 lucene 版本的兼容性问题, 目前我们 2.4.2 版本的集群使用的 lucene 版本是 5.5.2, 而 6.2.2 版本的 elasticsearch 使用的 lucene 版本是 7.2.1;</p><ul><li>由于主机资源有限, 没办法再弄出一组机器来搭建新集群, 我首先想到的是: 能否以 5.x 作跳板, 先原地升级到 5.x, 再从 5.x 升到 6.x;<br>但是看了官方文档, 这个想法是不可行的: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.2/reindex-upgrade.html" target="_blank" rel="noopener">Reindex before upgrading</a>; elasticsearch 只认索引是在哪个版本的集群中创建的, 并不关心这个索引现在在哪个集群; 一个索引在 2.4.2 集群中创建, 现在运行在 5.x 版本的 elasticsearch 中, 这时候将 5.x 的集群升级到 6.x, 该索引是无法在 6.x 中访问的;</li><li><p>其次我想到的是使用 hdfs snapshot / restore 插件来升级索引; 这种方式曾在之前 1.7.3 升级 2.4.2 版本时大量使用, 总体来说速度比普通的 scroll / index 全量同步要快很多; 但是看了官方文档, 发现这个想法也是不可行的, (文档链接: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html" target="_blank" rel="noopener">Snapshot And Restore</a>):</p><blockquote><p>A snapshot of an index created in 5.x can be restored to 6.x.<br>A snapshot of an index created in 2.x can be restored to 5.x.<br>A snapshot of an index created in 1.x can be restored to 2.x.</p></blockquote></li><li><p>接着我又想到了 elasticsearch 自带的 reindex 模块; reindex 模块也是官方文档推荐的从 5.x 升 6.x 时的索引升级方法; 经过 beta 测试, 我发现这个方法基本可行, 速度也尚可, 唯一需要注意的就是在 elasticsearch.yml 配置文件中要加上一段配置: <code>reindex.remote.whitelist: oldhost:port</code> 以允许连接远程主机作 reindex;<br>以下是 _reindex api 的使用方法:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"source"</span>: &#123;</span><br><span class="line">    <span class="string">"remote"</span>: &#123;</span><br><span class="line">      <span class="string">"host"</span>: <span class="string">"http://oldhost:9273"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"index"</span>: <span class="string">"source_idx"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"source_type"</span>,</span><br><span class="line">    <span class="string">"query"</span>: &#123;</span><br><span class="line">      <span class="string">"match_all"</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"dest"</span>: &#123;</span><br><span class="line">    <span class="string">"index"</span>: <span class="string">"dest_idx"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"dest_type"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>除了 reindex 模块之外, 其实还有一种更保守的方法, 就是用基于 es-spark 的索引迁移工具来完成迁移, 这也是之前经常使用的工具;</p></li></ul><h2 id="工具兼容性问题"><a href="#工具兼容性问题" class="headerlink" title="工具兼容性问题"></a><strong>工具兼容性问题</strong></h2><h3 id="http-访问工具兼容性"><a href="#http-访问工具兼容性" class="headerlink" title="http 访问工具兼容性"></a><strong>http 访问工具兼容性</strong></h3><p>目前我们经常使用的基于 http 的访问工具主要是 elasticsearch-head 和 cerebro;<br>关于 http 请求, elasticsearch 6.2.2 也有一个重大的改变: <a href="https://www.elastic.co/blog/strict-content-type-checking-for-elasticsearch-rest-requests" target="_blank" rel="noopener">Strict Content-Type Checking for Elasticsearch REST Requests</a>;<br>现在所有带 body 的请求都必须要加上 <code>Content-Type</code> 头, 否则会被拒绝; 我们目前正在使用的 elasticsearch-head:2 和 cerebro v0.6.1 肯定是不支持这点的, head 是所有针对数据的 CRUD 请求使用不了, cerebro 甚至连接机器都会失败;<br>&nbsp;<br>目前, cerebro 在 github 上已经发布了最新支持 elasticsearch 6.x 的 docker 版本: <a href="https://github.com/yannart/docker-cerebro" target="_blank" rel="noopener">yannart/docker-cerebro</a>; 经过部署测试, 完全兼容 elasticsearch 6.2.2;<br>不过, elasticsearch-head 就没那么积极了, 目前最近的一次 commit 发生在半年之前, 那个时候 elasticsearch 的最新版本还是 v 5.5;<br>&nbsp;<br>没有 elasticsearch-head 肯定是不行的, 这个时候就只能自己动手了;<br>首先, 肯定是希望从源码入手, 看能不能改一改, 毕竟只是加一个 <code>Content-Type</code>, 并不需要动大手术; 只可惜, 我 clone 下了 elasticsearch-head 的源码, 发现这个纯 javascript 的工程, 复杂度远远超出我的想象, 早已不是一个非前端工程师所能驾驭的了的; 我全局搜索了一些疑似 post 请求的逻辑, 但终究也没把握这些是不是真正要改的地方; 思来忖去, 只得作罢;<br>然后, 我开始思考能否通过间接的方式解决问题; 我注意到一个现象, 凡是带 body 的请求, body 必定是一个 json, 无论是 POST 还是 PUT; 那就是说, 如果必须要指定 <code>Content-Type</code> 的时候, 那就指定为 <code>application/json</code> 就 OK 了; 与此同时, 如果是一个不带 body 的 GET 请求, 携带上该 header 理论上也不会造成额外影响;<br>如果这个假设成立, 那我只需要对所有 elasticsearch-head 发起的请求挂一层代理, 全部转到 nginx 上去, 并统一加上个 header:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">   listen 80;</span><br><span class="line">   server_name esbetae.corp.11bee.com;</span><br><span class="line"></span><br><span class="line">   location / &#123;</span><br><span class="line">      proxy_pass http://l-es5.beta.p1.11bee.com:9273/;</span><br><span class="line"></span><br><span class="line">      proxy_set_header X-Real-Scheme <span class="variable">$scheme</span>;</span><br><span class="line">      proxy_set_header Host <span class="variable">$host</span>;</span><br><span class="line">      proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">      proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">      <span class="comment"># 统一加上 application/json 的 Content-Type</span></span><br><span class="line">      proxy_set_header Content-Type application/json;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试环境下的实验验证了这个方案是完全可行的, 原本正常访问的请求以及原本不能正常访问的请求, 现在都没有任何问题了;<br>其实, 这个方案相比之前还是有自己的好处的: 它隐藏了真正的 elasticsearch 节点地址与端口号, 只对业务线暴露了一个代理 url, 从而更加灵活与可控;</p><h3 id="插件兼容性"><a href="#插件兼容性" class="headerlink" title="插件兼容性"></a><strong>插件兼容性</strong></h3><p>笼统上讲, cerebro 与 elasticsearch-head 也是插件, 只不过它们是独立部署的, 所以被划归到 http 访问工具的类别中了; 而这一小节要讲的, 则是真正的需要依赖于具体的 elasticsearch 节点的插件;<br><strong>(1) elasticfence</strong><br>这个插件追踪溯源的话是这个项目: <a href="https://github.com/elasticfence/elasticsearch-http-user-auth" target="_blank" rel="noopener">elasticfence</a>; 后来由于各种各样的需求, 我们在这个插件的基础之上, 作了大量的修改; 到目前为止, 跑在我们节点上的该插件代码已经与 github 上的原项目代码没有半毛钱关系了;<br>当前我们版本的 elasticfence 最大的功能是整合了 qconfig, 使得其拥有热配置及时生效的能力; 然而, 也正是这个功能, 成了该插件本次兼容 elasticsearch 6.x 的噩梦;<br>首先第一道困难是, 2.4 与 6.2 版本的插件 api 彻底大改变; 但这与接下来的困难相比, 也只不过是热个身而已;<br>当我把 pom.xml 中的 elasticsearch 版本从 2.4.2 改成 6.2.2 时, 意料之中地发现代码红了一片, 不过仔细一看, 发现 api 变化的尺度之大, 还是超出了我的预计: RestFilter 接口直接被干掉了;<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A filter allowing to filter rest operations.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RestFilter</span> <span class="keyword">implements</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">order</span><span class="params">()</span> </span>&#123;<span class="keyword">return</span> <span class="number">0</span>;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Process the rest request. Using the channel to send a response, or the filter chain to continue processing the request.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(RestRequest request, RestChannel channel, RestFilterChain filterChain)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>原本在 2.4.2 版本中, RestFilter 是该插件的核心组件, 所有的请求都经过该过滤器, 由其中的逻辑判断是否具有访问权限; 现在该类被干掉, 我又搜不到其他类似 filter 的代替者, 这就没法操作了;<br>经过一段时间的努力, 我终于在 google 和 github 的帮助下找到了解决该问题的线索, 6.2 版本其实是提供了一个类似的 api 的:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// public interface ActionPlugin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a function used to wrap each rest request before handling the request.</span></span><br><span class="line"><span class="comment"> * Note: Only one installed plugin may implement a rest wrapper.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">default</span> UnaryOperator&lt;RestHandler&gt; <span class="title">getRestHandlerWrapper</span><span class="params">(ThreadContext threadContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>让插件的 main class 继承此接口, 使用 lambda 表达式十分简洁地解决问题:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// public class ElasticfencePlugin extends Plugin implements ActionPlugin</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> UnaryOperator&lt;RestHandler&gt; <span class="title">getRestHandlerWrapper</span><span class="params">(ThreadContext threadContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (isPluginDisabled()) &#123;</span><br><span class="line">        <span class="comment">// 透传请求</span></span><br><span class="line">        <span class="keyword">return</span> (originRestHandler) -&gt; authRestFilter.wrapNone(originRestHandler);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 权限控制</span></span><br><span class="line">        <span class="keyword">return</span> (originRestHandler) -&gt; authRestFilter.wrap(originRestHandler);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>本以为搞定了 api 就万事大吉了, 然后就遇到了第二道困难: java security manager;<br>换句话说, 就是基于安全考虑, 默认情况下不允许插件往任何磁盘路径写入东西, 大部分磁盘路径的内容不允许读取, 不允许发起 http 请求或 socket 连接, 不允许使用反射或者 Unsafe 类; 还有其他无数的动作限制…… 要想使用, 就必须申请权限!<br>当前版本的 elasticfence 由于使用了 qconfig, 所以首先需要引入公司的 common 客户端以初始化标准 web 应用, 期间需要申请磁盘路径读写权限以及一些系统变量的读写权限; qconfig-client 本身也有定时任务发起 http 请求, 所以还需要申请 http 资源的请求权限;<br>然而实际上, 申请权限却不是那么顺利: 我按照官方文档 <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/6.2/plugin-authors.html#_java_security_permissions" target="_blank" rel="noopener">Help for plugin authors</a> 的步骤申请了对应的权限, 重启节点, 发现无济于事: 该被禁止的依然被禁止; 我对 java security manager 的机制不熟悉, google 求助但所获甚少, 按正常的思路似乎遇到了阻碍;<br>&nbsp;<br>根据官方的描述, 从 6.x 开始, security manager 已无法被 disable, 要想在当前版本里 run 起来, 安全机制就是绕不开的问题; 听起来似乎已经绝了, 遂内心生发出一个狠想法: 去改 elasticsearch 源码, 把 security manager 相关代码全部注释掉, 然后重新编译, 堂而皇之, 若无其事!<br>想了下我们确实没有代码行为方面的安全需求, 这个 security manager 对我们而言其实是可有可无, 现在它阻碍了其他对我们很有必要的东西, 那么它就是可无的;<br>不过 elasticsearch 可不是一般的 java 项目, 其体系之复杂, 依赖之错综, 让人望而生畏; 小心翼翼得 pull 下来最新的代码, checkout 到目标 tag v6.2.2, 然后傻了: gradle 下载不了任何依赖, 代码全是红色的一片;<br>在网上搜了一阵子, 按部就班地操作, 还算顺利, 总算在 Intellij IDEA 里将项目正常加载起来了; 不得不感叹, 关于 elasticsearch 6.x, 即便是本地 IDE 的环境问题, 也值得写一篇文章好好总结一下;<br>源码中与 java security manager 相关的代码主要有以下几个地方:<br>首先是 elasticsearch 的主方法( elasticsearch 启动后执行的第一个逻辑便是设置 security manager):<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.elasticsearch.bootstrap.Elasticsearch</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// we want the JVM to think there is a security manager installed so that if internal policy </span></span><br><span class="line">    <span class="comment">// decisions that would be based on the presence of a security manager</span></span><br><span class="line">    <span class="comment">// or lack thereof act as if there is a security manager present (e.g., DNS cache policy)</span></span><br><span class="line">    System.setSecurityManager(<span class="keyword">new</span> SecurityManager() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">checkPermission</span><span class="params">(Permission perm)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// grant all permissions so that we can later set the security manager to the one that we want</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    LogConfigurator.registerErrorListener();</span><br><span class="line">    <span class="keyword">final</span> Elasticsearch elasticsearch = <span class="keyword">new</span> Elasticsearch();</span><br><span class="line">    <span class="keyword">int</span> status = main(args, elasticsearch, Terminal.DEFAULT);</span><br><span class="line">    <span class="keyword">if</span> (status != ExitCodes.OK) &#123;</span><br><span class="line">        exit(status);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>接着是 Bootstrap 类:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.elasticsearch.bootstrap.Bootstrap</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">boolean</span> addShutdownHook, Environment environment)</span> <span class="keyword">throws</span> BootstrapException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// install SM after natives, shutdown hooks, etc.</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException | NoSuchAlgorithmException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BootstrapException(e);</span><br><span class="line">    &#125;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>最后是 BootstrapChecks 类:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.elasticsearch.bootstrap.BootstrapChecks</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// the list of checks to execute</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> List&lt;BootstrapCheck&gt; <span class="title">checks</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> List&lt;BootstrapCheck&gt; checks = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    ......</span><br><span class="line">    checks.add(<span class="keyword">new</span> AllPermissionCheck());</span><br><span class="line">    <span class="keyword">return</span> Collections.unmodifiableList(checks);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AllPermissionCheck</span> <span class="keyword">implements</span> <span class="title">BootstrapCheck</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> BootstrapCheckResult <span class="title">check</span><span class="params">(BootstrapContext context)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (isAllPermissionGranted()) &#123;</span><br><span class="line">            <span class="keyword">return</span> BootstrapCheck.BootstrapCheckResult.failure(<span class="string">"granting the all permission effectively disables security"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> BootstrapCheckResult.success();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isAllPermissionGranted</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> SecurityManager sm = System.getSecurityManager();</span><br><span class="line">        <span class="keyword">assert</span> sm != <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            sm.checkPermission(<span class="keyword">new</span> AllPermission());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (<span class="keyword">final</span> SecurityException e) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>与 java security manager 相关的代码就在以上三个类中了; 可以发现它们都在 org.elasticsearch.bootstrap 包中;<br>重新编译后, 使用新处理过的 elasticsearch, 重启节点, 加载插件, 完美启动; 尽管这个问题暂时解决了, 但总是 “不太光彩”; 如果有人知道如何通过常规方法解决 security manager 的问题, 还请不吝赐教;</p><p><strong>(2) elasticsearch-analysis-ik</strong><br>这个插件没的说, 作为唯一一个在 elastic 公司任职的中国人, <a href="https://github.com/medcl" target="_blank" rel="noopener">medcl</a> 一定会在新版本发布第一时间更新 <a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">elasticsearch-analysis-ik</a>, 与公司共进退;<br>安装了最新的 6.2.2 版本的 elasticsearch-analysis-ik, 重启节点, 加载插件, 完美运行;</p><p><strong>(3) 其余插件</strong><br>在 2.4.2 中, 还有两个使用到的插件, marvel 和 licence; 在 6.x 中, 这些插件已经被 x-pack 取代了, 下一节将会介绍, 此处不再赘述;</p><h2 id="监控体系"><a href="#监控体系" class="headerlink" title="监控体系"></a><strong>监控体系</strong></h2><h3 id="基于-rest-api-graphite-grafana-的方案"><a href="#基于-rest-api-graphite-grafana-的方案" class="headerlink" title="基于 rest api + graphite + grafana 的方案"></a><strong>基于 rest api + graphite + grafana 的方案</strong></h3><p>基于 elasticsearch 的 rest api, 我们可以使用脚本定时收集到集群内各种状态的指标; 使用 graphite 收集 elasticsearch 汇报的指标, 并以 grafana 作为前端展示; 使用以上开源框架自建的监控系统, 已经成为我们监控 elasticsearch 集群健康状况的主力工具 (这篇文章详细介绍了 elasticsearch 各种 rest api 收集到的指标以及将其可视化的方法: <a href="">使用 rest api 可视化监控 elasticsearch 集群</a>);<br>将收集指标的脚本部署到 elasticsearch 6.x 测试节点, 发现 rest api 有了一些变化;<br>首先是 rest api 调用的参数的细微变化:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.4.2 的 _stats api 可以加一个不痛不痒的 all 参数</span></span><br><span class="line">_nodes/stats?all=<span class="literal">true</span></span><br><span class="line">_stats?all=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>all 参数在 6.x 中已经不支持了, 不过这是个不痛不痒的参数, 加与不加对结果的输出似没有任何影响;<br>其余的 api 在调用的路径和参数上都没有什么变化, 比较顺利;<br>然后是调用 api 返回的内容有一些细微的变化:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.4.2 的 load 指标</span></span><br><span class="line"><span class="variable">$node_name</span>.os.load_avergae</span><br><span class="line"><span class="comment"># 6.2.2 的 load 指标</span></span><br><span class="line"><span class="variable">$node_name</span>.os.cpu.load_average.1m</span><br><span class="line"><span class="variable">$node_name</span>.os.cpu.load_average.5m</span><br><span class="line"><span class="variable">$node_name</span>.os.cpu.load_average.15m</span><br></pre></td></tr></table></figure></p><p>6.2.2 的机器 load 指标收集, 随系统细分为了 1min, 5min 和 15min 三种, 也算是更精致了;</p><h3 id="elastic-官方组件-x-pack"><a href="#elastic-官方组件-x-pack" class="headerlink" title="elastic 官方组件 x-pack"></a><strong>elastic 官方组件 x-pack</strong></h3><p>在 x-pack 诞生之前, elastic 官方提供了如下几个辅助工具: kibana, shield, marvel, watcher, 分别用于数据可视化, 权限控制, 性能监控和系统报警; 功能很强大, 可惜除了基础功能外, 进阶功能都要收费;<br>从 elasticsearch 5.0 开始, 这些独立的工具被 elastic 公司打成了一个包: x-pack, 同时在原有的基础之上, 又进一步提供了机器学习, 可视化性能诊断 (visual search profiler) 等其他特性, 并以 kibana 为呈现这些功能的载体; 只不过, 收费的功能还是一个都没少:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/elasticsearch/elasticsearch_6.2_升级调研纪实/x-pack-fee.png" alt="x-pack-fee-table" title="">                </div>                <div class="image-caption">x-pack-fee-table</div>            </figure><br>对我们来说, 之前我们主要使用到的是 marvel, 用于观察索引分片转移的源目节点与复制进度 (shard activity), 偶尔也会用于辅助自建的监控系统, 观察一些请求的 qps 和 latency;<br>我分别在 elasticsearch node 与 kibana 上安装了 x-pack 套件, 剔除了需要付费的 security, watcher, ml, graph 模块;<br>可以看到, monitoring 部分相比以前的 marvel, 总体结构上没有太大变化:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/elasticsearch/elasticsearch_6.2_升级调研纪实/x-pack-monitor.png" alt="x-pack-monitor" title="">                </div>                <div class="image-caption">x-pack-monitor</div>            </figure><br>另外, 在 x-pack 免费的功能里, 还有一个比较实用的工具: dev-tools; 这里面有两个子栏目: search profiler 和 grok debugger; 其中, search profiler 在之前的 search api 基础上实现了可视化的诊断, 相比之前在 response json 字符串里面分析查询性能瓶颈, 这样的工具带来了巨大的直观性:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/elasticsearch/elasticsearch_6.2_升级调研纪实/x-pack-search-profiler.png" alt="x-pack-search-profiler" title="">                </div>                <div class="image-caption">x-pack-search-profiler</div>            </figure><br>除了以上免费功能, kibana 本身还有最基础的 Discover 和 Visualize 数据可视化功能, 只不过各业务线都习惯于使用 head 工具来访问线上数据, 并且 kibana 的该部分功能较之以前无显著变化, 此处便不再详述;<br>以上便是 elasticsearch 6.x 下 x-pack 最常见的使用情况;</p><h2 id="本文总结"><a href="#本文总结" class="headerlink" title="本文总结"></a><strong>本文总结</strong></h2><p>本文主要讨论当前生产环境下从 elasticsearch 2.4.2 升级到 6.2.2 的可行性与兼容性问题;<br><strong>首先是客户端兼容性问题:</strong><br>elastic 公司新推出的 <code>RestHighLevelClient</code> 从 http 层面最大限度得屏蔽了各版本间的差异, 使得跨版本调用成为了可能; 使用 6.2.2 的 <code>RestHighLevelClient</code> 可以正常访问 2.4.2 的集群, 这为集群升级带来了便利; 对各业务线而言, 只有有限的 (诸如 aggregations) api 被迫需要修改, 其余的都可以延续下去;<br><strong>其次是语法兼容性问题:</strong><br>此处仍需细分为三个方面: <strong>create index</strong>, <strong>query dsl</strong> 和 <strong>search api</strong> ;<br><strong>create index</strong> 方面, 其他的零碎变化都显得不痛不痒, 对我们的影响微乎其微, 唯一一个显著的大改变就是废弃了 <code>string</code> 类型, 改而细分出两个司职更明确的类型: <code>text</code> 与 <code>keyword</code>, 分别对应于分词和不分词的情形; 这个大改变需要我们对现有所有的索引作一次大整改;<br><strong>query dsl</strong> 方面, 对我们的影响也在控制范围之内: 只有 <code>missing</code> 语句被废弃需要业务线作一定的修改, 其他的大多可以由 es-adapter 代理兼容;<br><strong>search api</strong> 方面, 可能影响就比较大了: <code>scan</code> 和 <code>count</code> 两种 search type 被废弃, 并在 es-adapter 糟糕的设计之下, 影响被放大, 需要麻烦各业务线配合修改;<br><strong>然后是索引数据迁移兼容性问题:</strong><br>经过多方测试, 发现只有两种方法可以在我们这种跨两个 major 版本的情况下迁移索引数据: reindex 模块和 es-spark 工具; 好在这两种方法 (由其是后者) 之前就是我们主要的索引迁移工具;<br><strong>接着是工具兼容性问题:</strong><br>经过不断探索与变通, 最后 <code>cerebro</code>, <code>elasticsearch-head</code>, <code>elasticfence</code>, <code>elasticsearch-analysis-ik</code>, <code>curator</code> 等一系列原有生产环境下的 elasticsearch 工具 (插件) 都 “顺利” 实现了对 6.2.2 版本的兼容;<br>这其中, <code>elasticfence</code> 实现兼容的过程比较坎坷, 甚至还重新编译了 elasticsearch 的源码才解决了 security manager 的问题; 如果以后能通过常规方式解决安全的问题, 一定还得弄回去;<br><strong>最后是监控体系兼容性问题:</strong><br>得益于 6.x 版本 rest api 对先前的延续 (除了极个别 api 有细微调整之外), 之前生产环境使用的基于一系列开源方案的自建监控系统, 在 6.x 下依然做到了正常运转;<br>另外, 从 5.0 开始横空出世的 x-pack, 也在本次调研中被部署测试; 其中 monitoring, search-profiler 等功能都展示出了其实用的价值;</p><p>&nbsp;<br><strong>以上便是本文的全部内容;</strong></p><h2 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h2><ul><li><a href="">apache httpclient 初始化参数设置总结</a></li><li><a href="">使用 rest api 可视化监控 elasticsearch 集群</a></li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="https://github.com/elastic/elasticsearch-dsl-py/blob/master/Changelog.rst" target="_blank" rel="noopener">Changelog</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.2/removal-of-types.html" target="_blank" rel="noopener">Removal of mapping types</a></li><li><a href="https://www.elastic.co/blog/strict-content-type-checking-for-elasticsearch-rest-requests" target="_blank" rel="noopener">Strict Content-Type Checking for Elasticsearch REST Requests</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/client/java-rest/6.2/java-rest-high-compatibility.html" target="_blank" rel="noopener">Compatibility</a></li><li><a href="https://www.elastic.co/blog/state-of-the-official-elasticsearch-java-clients" target="_blank" rel="noopener">State of the official Elasticsearch Java clients</a></li><li><a href="http://blog.csdn.net/napoay/article/details/79135136" target="_blank" rel="noopener">Elasticsearch 6 新特性与重要变更解读</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/6.2/plugin-authors.html#_java_security_permissions" target="_blank" rel="noopener">Help for plugin authors</a></li><li><a href="https://elasticsearch.cn/article/338" target="_blank" rel="noopener">Intellij Idea 编译 Elasticsearch 源码</a></li><li><a href="https://github.com/elastic/elasticsearch#building-from-source" target="_blank" rel="noopener">elasticsearch: Building from Source</a></li><li><a href="https://www.elastic.co/blog/elasticsearch-sequence-ids-6-0" target="_blank" rel="noopener">Sequence IDs: Coming Soon to an Elasticsearch Cluster Near You</a></li><li><a href="https://www.cnblogs.com/Leo_wl/p/6181563.html" target="_blank" rel="noopener">Kibana+X-Pack</a></li><li><a href="https://www.elastic.co/subscriptions" target="_blank" rel="noopener">Subscriptions that Go to Work for You</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;关于 elasticsearch, 吐槽最多的就是其前后版本的兼容性问题; 在任何一个上规模的系统体系里, 要将部署在生产环境中的 elasticsearch 提升一个 major 版本是一件非常有挑战性的事情; 为了迎接这一挑战, 作者所在部门专门抽调人力资源作前期调研, 故为此文以记之;&lt;br&gt;在这篇文章中, 我将从 client 端, 索引创建, query dsl, search api, plugins, 监控体系等多方面讨论了从 2.4.2 版本迁移到 6.2.2 版本的一系列可能遇到的兼容性问题及解决方案;&lt;br&gt;希望能给各位读者带来工作上的帮助!&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="elasticsearch" scheme="http://zshell.cc/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="http://zshell.cc/tags/elasticsearch/"/>
    
      <category term="httpcomponents" scheme="http://zshell.cc/tags/httpcomponents/"/>
    
  </entry>
  
  <entry>
    <title>java gc 日志学习笔记</title>
    <link href="http://zshell.cc/2018/03/06/jvm-gc--java_gc%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://zshell.cc/2018/03/06/jvm-gc--java_gc日志学习笔记/</id>
    <published>2018-03-06T10:04:22.000Z</published>
    <updated>2018-05-20T12:12:36.041Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>关于 gc 日志, 网上有丰富的资料, 另外周志明的《深入理解 Java 虚拟机: JVM 高级特性与最佳实践》一书, 在第二版中的第 3.5.8 小节也补充了关于 gc 日志的介绍;<br>但是真等拿到一个具体的 gc log, 才发现网上很多的内容都停留在一个比较初级的层次, 只是介绍了最基本的情况; 其对于生产环境中正在使用的 CMS, G1 收集器涉及很少, 对各种 gc 相关的 jvm 参数, 它们在 gc 日志中的具体作用, 也少见一个详细的整理; 另外, 对 gc 日志的管理运维, 我也很难看到一篇好文章来认真讨论;<br>基于以上状况, 我决定在这里写下这篇文章, 从我自己的角度去对 java gc 作一个全面的总结;<br>补充说明: 本文所述内容涉及的 jvm 版本是: Java HotSpot(TM) 64-Bit Server VM (25.60-b23) for linux-amd64 JRE (1.8.0_60-b27);</p></blockquote><a id="more"></a><hr><h3 id="与-gc-相关的-jvm-选项"><a href="#与-gc-相关的-jvm-选项" class="headerlink" title="与 gc 相关的 jvm 选项"></a><strong>与 gc 相关的 jvm 选项</strong></h3><p>以下选项可以开启 gc 日志:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印 gc 的基本信息</span></span><br><span class="line">-verbose:gc</span><br><span class="line"><span class="comment"># 与 -verbose:gc 功能相同</span></span><br><span class="line">-XX:+PrintGC</span><br><span class="line"><span class="comment"># 打印 gc 的详细信息</span></span><br><span class="line">-XX:+PrintGCDetails</span><br></pre></td></tr></table></figure></p><p>-verbose:gc 与 -XX:+PrintGC 在功能上是一样的; 其区别在于 -verbose 是 jvm 的标准选项, 而 -XX 是 jvm 的非稳定选项; 另外, -XX:+PrintGCDetails 在启动脚本中可以自动开启 PrintGC 选项;</p><p>以下选项可以控制 gc 打印的内容:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 gc 发生的时间, 形如: yyyy-MM-dd'T'HH:mm:ss.SSSZ +0800</span></span><br><span class="line">-XX:+PrintGCDateStamps</span><br><span class="line"><span class="comment"># 输出 gc 发生时, 从进程启动到当前时刻总共经历的时间长度, 单位为秒</span></span><br><span class="line">-XX:+PrintGCTimeStamps</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印 gc 的原因, jdk7 以上支持, 从 jdk8 开始默认打印 gc 原因</span></span><br><span class="line">-XX:+PrintGCCause</span><br><span class="line"><span class="comment"># 打印 jvm 进入 safepoint 时的状态统计</span></span><br><span class="line">-XX:+PrintSafepointStatistics</span><br><span class="line"><span class="comment"># 打印每次 "stop the world" 持续的时间</span></span><br><span class="line">-XX:+PrintGCApplicationStoppedTime</span><br><span class="line"></span><br><span class="line"><span class="comment"># gc 发生前打印堆的状态</span></span><br><span class="line">-XX:+PrintHeapAtGC</span><br><span class="line"><span class="comment"># gc 发生时打印每一个岁数上对象存活数量分布图</span></span><br><span class="line">-XX:+PrintTenuringDistribution</span><br></pre></td></tr></table></figure></p><h3 id="gc-日志开头的元信息输出"><a href="#gc-日志开头的元信息输出" class="headerlink" title="gc 日志开头的元信息输出"></a><strong>gc 日志开头的元信息输出</strong></h3><p>一般在 jvm 启动时, gc.log 都会在开头打印出与当前 jvm 相关的一些元信息:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jvm 版本信息</span></span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (25.60-b23) <span class="keyword">for</span> linux-amd64 JRE (1.8.0_60-b27), built on Aug  4 2015 12:19:40 by <span class="string">"java_re"</span> with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)</span><br><span class="line"><span class="comment"># 内存信息</span></span><br><span class="line">Memory: 4k page, physical 65859796k(37547692k free), swap 0k(0k free)</span><br><span class="line"><span class="comment"># jvm 选项</span></span><br><span class="line">CommandLine flags: -XX:+DisableExplicitGC -XX:+FlightRecorder -XX:+G1SummarizeConcMark -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=33285996544 -XX:MaxHeapSize=33285996544 -XX:+PrintClassHistogram -XX:+Pr</span><br><span class="line">intGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution -XX:+UnlockCommercialFeatures -XX:+UnlockDiagnosticVMOptions -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC</span><br></pre></td></tr></table></figure></p><p>在 jvm 选项中使用不同的收集器, 所输出的 gc 日志格式会有所不同, 尤其是 G1, CMS 等实现复杂的收集器; 不过, 在一些细节的区别之外, 大部分收集器在整体结构上都会维持一定的共性, 以方便使用者阅读;</p><h3 id="gc-日志内容分析"><a href="#gc-日志内容分析" class="headerlink" title="gc 日志内容分析"></a><strong>gc 日志内容分析</strong></h3><p>我选取了几段有代表性的 gc 日志, 包括 ParNew, PS, CMS, G1, 其中相同或相似的内容我作了合并, 内容不同的部分则单独整理; 同时我也选取了一些有代表性的 gc 日志选项所打印的内容, 作重点介绍;</p><p><strong>(1) 打印 gc 发生的时间点, 与 jvm 启动后经历的时间长度</strong><br>对应的 jvm 选项是 PrintGCDateStamps 和 PrintGCTimeStamps; 其中, gc timestamp 是从 jvm 启动至日志打印当时所经历的秒数;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCDateStamps</span></span><br><span class="line"><span class="comment"># -XX:+PrintGCTimeStamps</span></span><br><span class="line">2018-02-06T11:46:09.444+0800: 30.455: [...]</span><br></pre></td></tr></table></figure></p><p><strong>(2) 打印 gc 发生的原因及类型</strong><br>对应的 jvm 选项是 PrintGCCause; 当然, 在 jdk8 之后, 默认会打印 gc cause;<br>这里 G1 和其他的收集器在格式上稍有区别, 因为它的设计与其他收集器差异较大, gc 的条件及类型都不尽相同;<br>G1 收集器的格式, 第一个括号内是 gc 原因, 第二个括号内为 gc 类型:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCCause</span></span><br><span class="line"><span class="comment"># G1 收集器的四种基本 gc 原因</span></span><br><span class="line">GC pause (G1 Evacuation Pause) (mixed)</span><br><span class="line">GC pause (GCLocker Initiated GC) (young)</span><br><span class="line">GC pause (G1 Humongous Allocation) (young)</span><br><span class="line">GC pause (Metadata GC Threshold) (young)</span><br></pre></td></tr></table></figure></p><p>其他收集器的格式, 括号内是 gc 原因, 括号之前是 gc 类型:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCCause</span></span><br><span class="line">GC (Allocation Failure)</span><br><span class="line">Full GC (Metadata GC Threshold)</span><br></pre></td></tr></table></figure></p><p>其中 gc 类型是这样界定的: 没有 STW 则为 GC, 若发生了 STW, 则为 Full GC;</p><p><strong>(3) 打印 gc 发生时每一个岁数上对象存活量分布图</strong><br>对应的 jvm 选项是 PrintTenuringDistribution, 这是一个十分有用的性能调参选项;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintTenuringDistribution</span></span><br><span class="line"><span class="comment"># from / to survivor 的大小为 104857600 bytes, 所以 survivor 区的总大小需要 * 2</span></span><br><span class="line">Desired survivor size 104857600 bytes, new threshold 15 (max 15) <span class="comment"># -XX:MaxTenuringThreshold=15</span></span><br><span class="line"><span class="comment">#            |--当前 age 的对象大小--|  |--各 age 累积总大小--|</span></span><br><span class="line">- age   1:   38129576 bytes,            38129576 total</span><br><span class="line">- age   2:   34724160 bytes,            72853736 total</span><br><span class="line">- age   3:    4290896 bytes,            77144632 total</span><br></pre></td></tr></table></figure></p><p>这里需要注意的是, 最后一列 “各 age 累积总大小”, 是将从 age 1 到当前 age 的所有 size 累积相加而成的; 它们驻留在 survivor 区中, 如果其累积 size 超过了 “Desired survivor size”, 将会有部分装不下 survivor 的对象晋升至年老代;<br>与此同时, 即便没有超过 “Desired survivor size”, 达到 “MaxTenuringThreshold” 的对象也将进入年老代; 为了避免数据频繁地从年轻代晋升至年老代, MaxTenuringThreshold 的合理值应该在 15 左右;<br>如果 survivor 区的 size 设置过小, 则每次达到 “Desired survivor size” 时的最大 age 都将远小于 15, 这同样会造成数据频繁地从年轻代晋升至年老代, 此时就需要考虑是否要调大 survivor 区的大小了;</p><p><strong>(4) 打印 jvm 运行时间与 STW 的时间</strong><br>对应的 jvm 选项是 PrintGCApplicationStoppedTime 和 PrintGCApplicationConcurrentTime;<br>当发生 “stop the world”, jvm 便会在 gc 日志里记录应用运行的时间:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCApplicationConcurrentTime</span></span><br><span class="line">2018-02-14T21:35:19.896+0800: 9.433: Application time: 0.0000892 seconds</span><br></pre></td></tr></table></figure></p><p>而当 gc 结束时, jvm 便会在 gc 日志中记录停顿的时间:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCApplicationStoppedTime</span></span><br><span class="line">2018-02-14T17:45:06.305+0800: 4.189: Total time <span class="keyword">for</span> <span class="built_in">which</span> application threads were stopped: 0.0553667 seconds, Stopping threads took: 0.0000412 seconds</span><br></pre></td></tr></table></figure></p><p><strong>(5) 打印 jvm 进入 safepoint 的统计信息</strong><br>对应的 jvm 选项是 PrintSafepointStatistics; 除了 gc pause 之外, 还有很多因素会导致 jvm STW 进入 safepoint, 例如: 反优化(deoptimize), 偏向锁生成(enable biased locking) 与偏向锁撤销(revoke bias), thread dump 等;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintSafepointStatistics  –XX:PrintSafepointStatisticsCount=1</span></span><br><span class="line">        vmop                [threads: total initially_running wait_to_block][time: spin block sync cleanup vmop] page_trap_count</span><br><span class="line">0.169:  Deoptimize              [      11          0              0    ]       [     0     0     0     0     0    ]  0</span><br><span class="line">        vmop                [threads: total initially_running wait_to_block][time: spin block sync cleanup vmop] page_trap_count</span><br><span class="line">9.933:  RevokeBias              [      52          0              0    ]       [     0     0     0     0     0    ]  0</span><br><span class="line">        vmop                [threads: total initially_running wait_to_block][time: spin block sync cleanup vmop] page_trap_count</span><br><span class="line">49.785: BulkRevokeBias          [      52          1              1    ]       [     0     0     0     0     0    ]  0</span><br><span class="line">        vmop                [threads: total initially_running wait_to_block][time: spin block sync cleanup vmop] page_trap_count</span><br><span class="line">49.821: GenCollectForAllocation [      52          2              2    ]       [     0     0     0     0    22    ]  0</span><br></pre></td></tr></table></figure></p><p>其中:<br>第一列 vmop 是 vm operation, 本次 stw 要做的事情;<br>total 是 stw 发生时, jvm 的总线程数;<br>initially_running 是正在运行, 尚未进入 safepoint 的线程数, 对应后面的时间是 spin;<br>wait_to_block 是进入 safepoint 后尚未阻塞的线程数, 对应后面的时间是 block;<br>所以, sync = spin + block + cleanup;<br>最后一列 vmop 则是 jvm 进入 safepoint 实际动作所消耗的时间;</p><p>&nbsp;<br>以上就是一些典型的与 gc 相关的 jvm 选项; 下面要说的是各典型的收集器日志;</p><p><strong>(6) ParNew 收集器的日志</strong><br>这种是最标准的 gc 日志格式, 也是各种资料上介绍得最多的内容, 日志的含义我已标注在注释上:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCDetails</span></span><br><span class="line"><span class="comment"># |--gc 前后该区域的 size 变化---|  |--gc 的时间--|  |--gc 前后整个堆的 size 变化--|  |--gc 总耗时--|</span></span><br><span class="line">  [275184K-&gt;1998K(306688K),         0.0151232 secs]  598728K-&gt;325543K(2063104K),      0.0152605 secs] [Times: user=0.05 sys=0.00, real=0.02 secs]</span><br></pre></td></tr></table></figure></p><p><strong>(7) Paravel Scavenge 收集器的日志</strong><br>PS 虽然也不是按照标准框架实现的收集器, 但是其 gc 日志与 ParNew 等相比几乎是一脉相承, 几无二致, 此处便不再赘述;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCDetails</span></span><br><span class="line">[PSYoungGen: 585755K-&gt;2888K(640000K)] 1625561K-&gt;1042703K(2038272K), 0.0278206 secs] [Times: user=0.03 sys=0.02, real=0.03 secs]</span><br></pre></td></tr></table></figure></p><p><strong>(8) CMS 收集器的日志</strong><br>CMS 的 gc 日志基本上是按照 CMS 收集算法的执行过程详细记录的;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCDetails</span></span><br><span class="line"><span class="comment"># CMS-initial-mark</span></span><br><span class="line">2018-02-14T17:45:06.250+0800: 4.134: [GC (CMS Initial Mark) [1 CMS-initial-mark: 0K(1756416K)] 190272K(2063104K), 0.0550579 secs] [Times: user=0.18 sys=0.00, real=0.05 secs]</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMS-concurrent-mark-start</span></span><br><span class="line">2018-02-14T17:45:06.305+0800: 4.189: [CMS-concurrent-mark-start]</span><br><span class="line">2018-02-14T17:45:06.320+0800: 4.203: [CMS-concurrent-mark: 0.014/0.014 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] </span><br><span class="line"><span class="comment"># CMS-concurrent-preclean</span></span><br><span class="line">2018-02-14T17:45:06.320+0800: 4.203: [CMS-concurrent-preclean-start]</span><br><span class="line">2018-02-14T17:45:06.324+0800: 4.208: [CMS-concurrent-preclean: 0.004/0.004 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] </span><br><span class="line">2018-02-14T17:45:06.325+0800: 4.208: Total time <span class="keyword">for</span> <span class="built_in">which</span> application threads were stopped: 0.0009297 seconds, Stopping threads took: 0.0000444 seconds</span><br><span class="line">2018-02-14T17:45:06.325+0800: 4.209: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line"> CMS: abort preclean due to time 2018-02-14T17:45:11.592+0800: 9.475: [CMS-concurrent-abortable-preclean: 4.211/5.267 secs] [Times: user=10.30 sys=0.27, real=5.27 secs] </span><br><span class="line">2018-02-14T17:45:11.592+0800: 9.476: Total time <span class="keyword">for</span> <span class="built_in">which</span> application threads were stopped: 0.0001745 seconds, Stopping threads took: 0.0000389 seconds</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMS-final-remark</span></span><br><span class="line">2018-02-14T17:45:11.592+0800: 9.476: [GC (CMS Final Remark) [YG occupancy: 173321 K (306688 K)]2018-02-14T17:45:11.592+0800: 9.476: [Rescan (parallel) , 0.0380948 secs]2018-02-14T17:45:11.630+0800: 9.514: [weak </span><br><span class="line">refs processing, 0.0001539 secs]2018-02-14T17:45:11.630+0800: 9.514: [class unloading, 0.0082249 secs]2018-02-14T17:45:11.639+0800: 9.522: [scrub symbol table, 0.0051294 secs]2018-02-14T17:45:11.644+0800: 9.528:</span><br><span class="line"> [scrub string table, 0.0010024 secs][1 CMS-remark: 19239K(1756416K)] 192561K(2063104K), 0.0549428 secs] [Times: user=0.17 sys=0.00, real=0.05 secs] </span><br><span class="line">``` </span><br><span class="line">``` bash</span><br><span class="line"><span class="comment"># CMS-concurrent-sweep</span></span><br><span class="line">2018-02-14T17:45:11.647+0800: 9.531: [CMS-concurrent-sweep-start]</span><br><span class="line">2018-02-14T17:45:11.651+0800: 9.535: [CMS-concurrent-sweep: 0.004/0.004 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] </span><br><span class="line"><span class="comment"># CMS-concurrent-reset</span></span><br><span class="line">2018-02-14T17:45:11.651+0800: 9.535: [CMS-concurrent-reset-start]</span><br><span class="line">2018-02-14T17:45:11.668+0800: 9.552: [CMS-concurrent-reset: 0.015/0.017 secs] [Times: user=0.05 sys=0.02, real=0.01 secs]</span><br></pre></td></tr></table></figure><p>关于 CMS 收集算法的流程逻辑, 请参见另一篇文章: <a href="">CMS 收集算法学习与整理</a>;</p><p><strong>(9) G1 收集器的日志</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -XX:+PrintGCDetails</span></span><br><span class="line"><span class="comment"># 老生代的 gc 时间状况</span></span><br><span class="line">   [Parallel Time: 107.6 ms, GC Workers: 23]</span><br><span class="line">      [GC Worker Start (ms): Min: 30455.3, Avg: 30455.6, Max: 30455.8, Diff: 0.5]</span><br><span class="line">      [Ext Root Scanning (ms): Min: 0.7, Avg: 1.6, Max: 16.4, Diff: 15.7, Sum: 37.8]</span><br><span class="line">      [Update RS (ms): Min: 0.0, Avg: 0.5, Max: 0.9, Diff: 0.9, Sum: 12.6]</span><br><span class="line">         [Processed Buffers: Min: 0, Avg: 0.9, Max: 2, Diff: 2, Sum: 21]</span><br><span class="line">      [Scan RS (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 1.3]</span><br><span class="line">      [Code Root Scanning (ms): Min: 0.0, Avg: 2.3, Max: 26.9, Diff: 26.9, Sum: 52.4]</span><br><span class="line">      [Object Copy (ms): Min: 78.3, Avg: 102.5, Max: 105.7, Diff: 27.4, Sum: 2357.9]</span><br><span class="line">      [Termination (ms): Min: 0.0, Avg: 0.1, Max: 0.1, Diff: 0.1, Sum: 1.3]</span><br><span class="line">         [Termination Attempts: Min: 1, Avg: 63.0, Max: 79, Diff: 78, Sum: 1448]</span><br><span class="line">      [GC Worker Other (ms): Min: 0.0, Avg: 0.1, Max: 0.1, Diff: 0.1, Sum: 1.5]</span><br><span class="line">      [GC Worker Total (ms): Min: 106.9, Avg: 107.2, Max: 107.5, Diff: 0.6, Sum: 2464.8]</span><br><span class="line">      [GC Worker End (ms): Min: 30562.7, Avg: 30562.7, Max: 30562.8, Diff: 0.1]</span><br><span class="line">   [Code Root Fixup: 3.2 ms]</span><br><span class="line">   [Code Root Purge: 0.1 ms]</span><br><span class="line">   [Clear CT: 0.6 ms]</span><br><span class="line">   [Other: 4.4 ms]</span><br><span class="line">      [Choose CSet: 0.0 ms]</span><br><span class="line">      [Ref Proc: 1.6 ms]</span><br><span class="line">      [Ref Enq: 0.0 ms]</span><br><span class="line">      [Redirty Cards: 0.4 ms]</span><br><span class="line">      [Humongous Register: 0.2 ms]</span><br><span class="line">      [Humongous Reclaim: 0.1 ms]</span><br><span class="line">      [Free CSet: 1.0 ms]</span><br><span class="line"><span class="comment"># 新生代的 gc 状况</span></span><br><span class="line">   [Eden: 1472.0M(1464.0M)-&gt;0.0B(1384.0M) Survivors: 120.0M-&gt;200.0M Heap: 1781.0M(31.0G)-&gt;682.3M(31.0G)]</span><br><span class="line"><span class="comment"># gc 时间统计</span></span><br><span class="line"> [Times: user=2.36 sys=0.06, real=0.11 secs]</span><br></pre></td></tr></table></figure></p><p>关于 CMS 收集算法的流程逻辑, 请参见另一篇文章: <a href="">G1 收集算法学习与整理</a>;</p><h3 id="gc-日志文件的运维最佳实践"><a href="#gc-日志文件的运维最佳实践" class="headerlink" title="gc 日志文件的运维最佳实践"></a><strong>gc 日志文件的运维最佳实践</strong></h3><p>关于 gc 日志文件本身的管理运维, 也是存在一些经验的, 错误的运维方法将为性能排查甚至系统运行带来麻烦与阻碍;<br><strong>(1) 避免新的 gc 日志覆盖旧的 gc 日志</strong><br>使用 -Xloggc 可以指定 gc 日志的输出路径, 错误的经验会引导我们作如下设置:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xloggc:<span class="variable">$&#123;CATALINA_BASE&#125;</span>/logs/gc.log</span><br></pre></td></tr></table></figure></p><p>这种设置带来的问题是: 当系统重启后, 新的 gc 日志的路径与老的 gc 日志路径相同, 新日志便会将旧日志覆盖;<br>系统一般不会随便重启, 如果重启, 很可能是出现了故障, 或者性能问题; 在这种设置下, 如果重启前忘了备份当前的 gc 日志, 那重启后就没有性能诊断的依据了 (当然也可能事先使用 jstack, jmap 等工具作了部分现场的保留, 并非完全无法作诊断, 这里只是单从 gc 的角度讨论);<br>所以, 最佳的方法是为 gc 日志的命名带上时间戳:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xloggc:<span class="variable">$&#123;CATALINA_BASE&#125;</span>/logs/gc.log-$(date +%Y-%m-%d-%H-%M)</span><br></pre></td></tr></table></figure></p><p>这样只要不是在同一分钟内两次重启, gc 日志都不会被覆盖;</p><p><strong>(2) gc 日志的 rolling 滚动</strong><br>如果系统一直在健康运行, 那么 gc 日志的大小就会稳定地增长, 占用磁盘空间, 最后导致磁盘空间报警; 显然我们需要对 gc 日志作 rolling, 主流的方式是使用 jvm 自己的选项作控制:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用 gc 日志 rolling</span></span><br><span class="line">-XX:+UseGCLogFileRotation</span><br><span class="line"><span class="comment"># 设置 gc 日志的最大 size, 一旦触发该条件就滚动切分</span></span><br><span class="line">-XX:GCLogFileSize=10M</span><br><span class="line"><span class="comment"># 设置保留滚动 gc 日志的最大文件数量</span></span><br><span class="line">-XX:NumberOfGCLogFiles=5</span><br></pre></td></tr></table></figure></p><p>最后日志目录下的内容类似于下面这个样子:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gc.log-2018-03-01-17-58.0</span><br><span class="line">gc.log-2018-03-01-17-58.1</span><br><span class="line">gc.log-2018-03-01-17-58.2</span><br><span class="line">gc.log-2018-03-01-17-58.3</span><br><span class="line">gc.log-2018-03-01-17-58.4.current</span><br></pre></td></tr></table></figure></p><p>最后一个带 “.current” 后缀的就是当前正在写的 gc 日志;<br>但这种方式有个问题, 日志文件名没有规范 当达到最大文件数量后, jvm 会选择回头覆盖最老的那个日志文件, 并把 “.current” 后缀也挪过去, 这种模式对日志收集相当得不友好, <a href="http://www.planetcobalt.net/sdb/forward_gc_logs.shtml" target="_blank" rel="noopener">我们很难定位当前正在写入的 log 文件</a>;<br>于是有另外一种思路想来解决 gc 日志收集的问题, 其采用了挪走并写空的方式:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按天作 rolling</span></span><br><span class="line">cp /home/q/www/<span class="variable">$i</span>/logs/gc.log /home/q/www/<span class="variable">$i</span>/logs/gc.log.$(date -d <span class="string">"yesterday"</span> +%F)</span><br><span class="line"><span class="built_in">echo</span> &gt; /home/q/www/<span class="variable">$i</span>/logs/gc.log</span><br><span class="line">gzip /home/q/www/<span class="variable">$i</span>/logs/gc.log.$(date -d <span class="string">"yesterday"</span> +%F)</span><br></pre></td></tr></table></figure></p><p>这种方法看似可行, 但忽略了一个问题: cp 和 echo 写空这两步并非原子操作, 在这个处理过程中, jvm 依然在试图往日志里写内容, 这就造成了写空后的 gc.log 接不上被 rolling 的老日志了, 甚至在字节层面上都不是完整的编码了, 打开看一下就报了这种错:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"gc.log"</span> may be a binary file.  See it anyway?</span><br></pre></td></tr></table></figure></p><p>即便是使用 strings 命令搜索文本内容, 也只能得到一些残缺的内容, 完全无法分析问题了;<br>说到底, gc 日志的收集陷入这样的困境, 其实是 jvm 自己的支持度不够好; 像 nginx, 使用 <code>kill -USER1</code> 便可以作到原子切割日志; logback 的 RollingFileAppender 也是自身提供了完整的支持; 可惜 jvm gc log 没有走类似的路线, 而是采用了一种古怪的类似于 rrd 的环状模式, 直接造成了收集日志的困难;</p><h3 id="gc-日志的辅助分析工具"><a href="#gc-日志的辅助分析工具" class="headerlink" title="gc 日志的辅助分析工具"></a><strong>gc 日志的辅助分析工具</strong></h3><p>其实拿到一个冗长的 gc 日志文件, 对着枯燥的数字, 我们很难对 待诊断的系统建立起一个直观而感性的性能健康状态的认识;<br>这里有一个十分出色的 gc 日志可视化分析工具: <a href="http://gceasy.io/" target="_blank" rel="noopener">gceasy</a>, 其对于 gc 问题的诊断可谓是如虎添翼;<br>上传 gc 日志并分析诊断, gceasy 可以给出多维度的可视化分析报告, 以友好的交互自动诊断系统的问题所在:</p><ul><li>Heap Statistics (堆状态统计)<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/jvm/gc/gc%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/heap_statistics.png" alt="heap_statistics" title="">                </div>                <div class="image-caption">heap_statistics</div>            </figure></li><li>GC Phases Statistics (gc 算法流程各阶段的时间统计)<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/jvm/gc/gc%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/gc_phase_statistics.png" alt="gc_phase_statistics" title="">                </div>                <div class="image-caption">gc_phase_statistics</div>            </figure></li><li>GC Time (gc 时间统计)<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/jvm/gc/gc%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/gc_time_statistics.png" alt="gc_time_statistics" title="">                </div>                <div class="image-caption">gc_time_statistics</div>            </figure></li><li>GC Cause (gc 原因分布统计)<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/jvm/gc/gc%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/gc_cause_statistics.png" alt="gc_cause_statistics" title="">                </div>                <div class="image-caption">gc_cause_statistics</div>            </figure></li><li>tenuring summary (survivor 区每一个岁数上存活对象 szie 统计)<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/jvm/gc/gc%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/tenuring_summary.png" alt="tenuring_summary" title="">                </div>                <div class="image-caption">tenuring_summary</div>            </figure></li></ul><h3 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h3><ul><li><a href="">CMS 收集算法学习与整理</a></li><li><a href="">G1 收集算法学习与整理</a></li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="https://blog.csdn.net/dac55300424/article/details/19124831" target="_blank" rel="noopener">深入理解 Java 虚拟机: JVM 高级特性与最佳实践 (第2版) 3.5.8 理解 GC 日志</a></li><li><a href="https://blogs.oracle.com/poonam/understanding-g1-gc-logs" target="_blank" rel="noopener">Understanding G1 GC Logs</a></li><li><a href="http://chenzhou123520.iteye.com/blog/1582163" target="_blank" rel="noopener">Java -verbose:gc 命令</a></li><li><a href="https://yq.aliyun.com/ask/11545" target="_blank" rel="noopener">jvm参数-verbose:gc和-XX:+PrintGC有区别? (阿里云)</a></li><li><a href="https://segmentfault.com/q/1010000004348215/a-1020000004351714" target="_blank" rel="noopener">jvm参数-verbose:gc和-XX:+PrintGC有区别? (segmentfault)</a></li><li><a href="https://developer.jboss.org/thread/148848?tstart=0&amp;_sscc=t" target="_blank" rel="noopener">Some junk characters displaying at start of jboss gc log file</a></li><li><a href="https://blog.csdn.net/zero__007/article/details/53842099" target="_blank" rel="noopener">GC日志中的 stop-the-world</a></li><li><a href="https://www.zhihu.com/question/57722838/answer/156390795" target="_blank" rel="noopener">java GC进入safepoint的时间为什么会这么长</a></li><li><a href="https://blog.gceasy.io/2016/11/15/rotating-gc-log-files/" target="_blank" rel="noopener">ROTATING GC LOG FILES</a></li><li><a href="http://www.planetcobalt.net/sdb/forward_gc_logs.shtml" target="_blank" rel="noopener">Forwarding JVM Garbage Collector Logs</a></li><li><a href="https://www.jianshu.com/p/e634955f3bbb" target="_blank" rel="noopener">jvm-对象年龄(-XX:+PrintTenuringDistribution)</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;关于 gc 日志, 网上有丰富的资料, 另外周志明的《深入理解 Java 虚拟机: JVM 高级特性与最佳实践》一书, 在第二版中的第 3.5.8 小节也补充了关于 gc 日志的介绍;&lt;br&gt;但是真等拿到一个具体的 gc log, 才发现网上很多的内容都停留在一个比较初级的层次, 只是介绍了最基本的情况; 其对于生产环境中正在使用的 CMS, G1 收集器涉及很少, 对各种 gc 相关的 jvm 参数, 它们在 gc 日志中的具体作用, 也少见一个详细的整理; 另外, 对 gc 日志的管理运维, 我也很难看到一篇好文章来认真讨论;&lt;br&gt;基于以上状况, 我决定在这里写下这篇文章, 从我自己的角度去对 java gc 作一个全面的总结;&lt;br&gt;补充说明: 本文所述内容涉及的 jvm 版本是: Java HotSpot(TM) 64-Bit Server VM (25.60-b23) for linux-amd64 JRE (1.8.0_60-b27);&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="jvm" scheme="http://zshell.cc/categories/jvm/"/>
    
      <category term="gc" scheme="http://zshell.cc/categories/jvm/gc/"/>
    
    
      <category term="jvm:gc" scheme="http://zshell.cc/tags/jvm-gc/"/>
    
      <category term="jvm 选项" scheme="http://zshell.cc/tags/jvm-%E9%80%89%E9%A1%B9/"/>
    
  </entry>
  
  <entry>
    <title>python module 使用总结: 定时调度器</title>
    <link href="http://zshell.cc/2018/02/23/python-module--python_module_%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93_%E5%AE%9A%E6%97%B6%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    <id>http://zshell.cc/2018/02/23/python-module--python_module_使用总结_定时调度器/</id>
    <published>2018-02-23T15:23:21.000Z</published>
    <updated>2018-03-24T07:08:34.770Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在 java 里, 第三方定时调度框架比较常用的是 quartz 和 springframework 提供的 schedule 功能; 不过在各大公司里, 一般都会开发自己能集中管理与灵活调控的调度组件; 这样一来, 第三方的调度框架反而就接触的少了;<br>我相信在以 python 为主要使用语言的公司里, 一定也有自己的调度中间件; 但是对于以 java 为主的公司里, 肯定不可能专为 python 维护一套调度系统, 所以就很有必要了解一下 python 里的定时调度模块; 本文将介绍几种常用的 python 定时调度框架:</p><ol><li>简单的实现: sched 与 schedule;</li><li>功能增强版: apscheduler;</li><li>分布式调度器: celery;</li></ol></blockquote><a id="more"></a><hr><h2 id="sched"><a href="#sched" class="headerlink" title="sched"></a><strong>sched</strong></h2><p>sched 是 python 官方提供的定时调度模块, 其实现非常的简单, sched.py 的代码量只有一百多行, 且只有一个类: scheduler;<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sched.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scheduler</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        @param timefunc     能够返回时间戳的计时函数, 要求该函数是一个无参函数</span></span><br><span class="line"><span class="string">        @param delayfunc    能够阻塞给定时间的阻塞函数, 要求该函数能接收一个数值类型的参数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    def __init__(self, timefunc, delayfunc);</span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        @param delay        需要延迟的时间</span></span><br><span class="line"><span class="string">        @param priority     当多个任务需要在同一个时间调度时的优先级</span></span><br><span class="line"><span class="string">        @param action       需要调度的函数</span></span><br><span class="line"><span class="string">        @param argument     需要调度函数的参数列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    def enter(self, delay, priority, action, argument);</span><br></pre></td></tr></table></figure></p><p>如上所示, scheduler 是使用构造器传进来的计时函数与阻塞函数去实现调度逻辑的;</p><h3 id="sched-的局限性与解决方案"><a href="#sched-的局限性与解决方案" class="headerlink" title="sched 的局限性与解决方案"></a><strong>sched 的局限性与解决方案</strong></h3><p>不过说真的, 把 sched 定义为定时调度模块真的很牵强:<br>常规意义上, 我们所理解的定时调度器, 应该是能够像 cron 那样, 按给定的时间间隔或在指定的时间点上循环执行指定的任务; 但是 sched 并不能做到这一点, sched 所做的, 只是从某一个时间点开始, delay 一段我们给定的延时时间, 然后执行给定方法, 仅执行这一次;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sched <span class="keyword">import</span> scheduler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task</span><span class="params">(time_str)</span>:</span></span><br><span class="line">    print(<span class="string">'task run: %s'</span> % time_str)</span><br><span class="line"></span><br><span class="line">s = scheduler(time.time, time.sleep)</span><br><span class="line"><span class="comment"># delay 5 秒后执行 do_task 函数, 仅执行一次</span></span><br><span class="line">s.enter(<span class="number">5</span>, <span class="number">0</span>, do_task, (time.time(),))</span><br><span class="line">s.run()</span><br></pre></td></tr></table></figure></p><p>要想 sched 做到循环执行, 还需要在其基础上包装上一层类似’递归’的概念:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">s = scheduler(time.time, time.sleep)</span><br><span class="line"><span class="comment"># 在任务函数中将自己再次用 sched 调度以实现循环</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task</span><span class="params">(time_str)</span>:</span></span><br><span class="line">    s.enter(<span class="number">5</span>, <span class="number">0</span>, do_task, (time.time(),))</span><br><span class="line">    print(<span class="string">'task run: %s'</span> % time_str)</span><br><span class="line">    </span><br><span class="line">s.enter(<span class="number">5</span>, <span class="number">0</span>, do_task, (time.time(),))</span><br><span class="line">s.run()</span><br></pre></td></tr></table></figure></p><p>当然, 这只是将函数自己的引用传给了 scheduler, 神似递归但并非递归, 所以也就不存在找不到递归出口而爆栈的问题了;<br>很明显, 采用这种方式才能实现真正的定时调度, 可谓非常麻烦而蹩脚;</p><h3 id="sched-的调度原理"><a href="#sched-的调度原理" class="headerlink" title="sched 的调度原理"></a><strong>sched 的调度原理</strong></h3><p>sched 使用 <code>heapq</code> 优先队列来管理需要调度的任务(关于 heapq 的详细内容请参考: <a href="">python module 使用总结: heapq</a>); 在调用了 scheduler 类的 enter 方法后, 其实是生成了一个任务的快照, 并放入了优先队列里:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">event = Event(time, priority, action, argument)</span><br><span class="line">heapq.heappush(self._queue, event)</span><br></pre></td></tr></table></figure></p><p>在调用 scheduler.run() 方法后, sched 在一个死循环里, 不断得从优先队列里取出任务执行, 计算最近的下一个任务的等待时间并阻塞:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">q = self._queue</span><br><span class="line"> <span class="keyword">while</span> q:</span><br><span class="line">    time, priority, action, argument = checked_event = q[<span class="number">0</span>]</span><br><span class="line">    now = timefunc()</span><br><span class="line">    <span class="keyword">if</span> now &lt; time:</span><br><span class="line">        delayfunc(time - now)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        event = pop(q)</span><br><span class="line">        <span class="keyword">if</span> event <span class="keyword">is</span> checked_event:</span><br><span class="line">            action(*argument)</span><br><span class="line">            delayfunc(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            heapq.heappush(q, event)</span><br></pre></td></tr></table></figure></p><p>总体来说, sched 的设计还是比较紧凑清晰的, 轻量化; 但是由于其固有的缺陷, 在复杂的场景中, 往往不能胜任, 我们需要功能更强大的调度框架;</p><h2 id="schedule"><a href="#schedule" class="headerlink" title="schedule"></a><strong>schedule</strong></h2><p>schedule 是一个广泛使用的 python 定时调度框架, 其 github 地址如下: <a href="https://github.com/dbader/schedule" target="_blank" rel="noopener">https://github.com/dbader/schedule</a>, 目前 4k 多个 stars;<br>和 python 官方的 sched 相比, schedule 的 API 要人性化得多, 而且它基本实现了真正意义上的定时调度:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> schedule</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task</span><span class="params">(time_str=time.time<span class="params">()</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'task run: %s'</span> % time_str)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每 10 分钟执行一次任务</span></span><br><span class="line">schedule.every(<span class="number">10</span>).minutes.do(do_task)</span><br><span class="line"><span class="comment"># 每隔 5 到 10 分钟之间的任意一个时间执行一次任务</span></span><br><span class="line">schedule.every(<span class="number">5</span>).to(<span class="number">10</span>).days.do(do_task)</span><br><span class="line"><span class="comment"># 每 1 小时执行一次任务</span></span><br><span class="line">schedule.every().hour.do(do_task, time_str=<span class="string">'1519351479.19554'</span>)</span><br><span class="line"><span class="comment"># 每天 10:30 执行一次任务</span></span><br><span class="line">schedule.every().day.at(<span class="string">"10:30"</span>).do(do_task)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    schedule.run_pending()</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h3 id="schedule-模块的-Scheduler-类"><a href="#schedule-模块的-Scheduler-类" class="headerlink" title="schedule 模块的 Scheduler 类"></a><strong>schedule 模块的 Scheduler 类</strong></h3><p>其实, schedule 模块的整体设计, 是把任务的自我管理部分做的很详细, 而把上层的调度做的很轻很薄, 关键逻辑点采用回调的方式, 依赖任务的自我管理去实现;<br>而上一节所讲的 sched 模块, 则是在上层调度部分使用了复杂的逻辑 (优先队列) 去统一管理, 而任务本身携带的信息很少; sched 与 schedule 两个模块, 在整体设计上, 形成了鲜明的对比;<br>Scheduler 类的实例中, 维护了一个列表: jobs, 专门存储注册进来的任务快照;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Class Scheduler(object):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.jobs = []</span><br></pre></td></tr></table></figure></p><p>Scheduler 类最重要的方法是 <code>run_pending(self)</code>, 其主要逻辑是遍历 jobs 列表中的所有 job, 从中找出当前时间点需要调度的 job, 并执行;<br>这其中最重要的逻辑是判断一个 job 当前时间点是否需要被调度, 而这个过程是一个回调, 具体的逻辑则封装在 job.should_run 方法里, 下一小节将会详细介绍;<br>可以发现, 总共只用了三行代码, 以此可见其轻量化;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_pending</span><span class="params">(self)</span>:</span></span><br><span class="line">    _jobs = (job <span class="keyword">for</span> job <span class="keyword">in</span> self.jobs <span class="keyword">if</span> job.should_run)</span><br><span class="line">    <span class="keyword">for</span> job <span class="keyword">in</span> sorted(runnable_jobs):</span><br><span class="line">        self._run_job(job)</span><br></pre></td></tr></table></figure></p><p>值得注意的是, schedule 模块中并没有专门的逻辑去定时执行 run_pending 方法, 要想让定时调度持续跑起来, 需要自己实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    schedule.run_pending()</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>相比 sched 模块的 ‘伪递归’ 而言, 这样的设计算是比较人性化的了, 可以认为它基本实现了真正意义上的定时调度;</p><h3 id="schedule-模块的-Job-类"><a href="#schedule-模块的-Job-类" class="headerlink" title="schedule 模块的 Job 类"></a><strong>schedule 模块的 Job 类</strong></h3><p>正如上一节所述, schedule 模块实现了非常详细的任务自我管理逻辑; 相比 sched 的 <code>Event</code> 类, schedule 定义了一个控制参数更丰富的 <code>Job</code> 类:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Job</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, interval, scheduler=None)</span>:</span></span><br><span class="line">        self.interval = interval  <span class="comment"># pause interval * unit between runs</span></span><br><span class="line">        self.latest = <span class="keyword">None</span>  <span class="comment"># upper limit to the interval</span></span><br><span class="line">        self.job_func = <span class="keyword">None</span>  <span class="comment"># the job job_func to run</span></span><br><span class="line">        self.unit = <span class="keyword">None</span>  <span class="comment"># time units, e.g. 'minutes', 'hours', ...</span></span><br><span class="line">        self.at_time = <span class="keyword">None</span>  <span class="comment"># optional time at which this job runs</span></span><br><span class="line">        self.last_run = <span class="keyword">None</span>  <span class="comment"># datetime of the last run</span></span><br><span class="line">        self.next_run = <span class="keyword">None</span>  <span class="comment"># datetime of the next run</span></span><br><span class="line">        self.period = <span class="keyword">None</span>  <span class="comment"># timedelta between runs, only valid for</span></span><br><span class="line">        self.start_day = <span class="keyword">None</span>  <span class="comment"># Specific day of the week to start on</span></span><br><span class="line">        self.tags = set()  <span class="comment"># unique set of tags for the job</span></span><br><span class="line">        self.scheduler = scheduler  <span class="comment"># scheduler to register with</span></span><br></pre></td></tr></table></figure></p><p>Job 类的参数众多, 单个任务的调度肯定不可能涉及到所有的参数, 这些参数往往是以局部几个为组合, 控制调度节奏的; 但是无论什么组合, 往往都会以 <code>scheduler.every(interval=1)</code> 方法开始, 以 <code>Job.do(self, job_func, *args, **kwargs)</code> 方法结束:<br>(1) schedule.every 方法构造出一个 <code>Job</code> 实例, 并设置该实例的第一个参数 interval;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">default_scheduler = Scheduler()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">every</span><span class="params">(interval=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> default_scheduler.every(interval)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scheduler</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">every</span><span class="params">(self, interval=<span class="number">1</span>)</span>:</span></span><br><span class="line">        job = Job(interval, self)</span><br><span class="line">        <span class="keyword">return</span> job</span><br></pre></td></tr></table></figure></p><p>这里想吐槽的是, 这个方法出现在 Scheduler 类中有点突兀, 而且方法名叫 every, 只体现了设置 interval 参数的含义, 但并不能从中看出其新构造一个 Job 实例的意图;<br>(2) Job.do 方法包装了传递进来的任务函数, 将其设置为自己的 job_func 参数, 并将自己作为一个任务快照放进 scheduler 的任务列表里;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Class Job(object):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do</span><span class="params">(self, job_func, *args, **kwargs)</span>:</span></span><br><span class="line">        self.job_func = functools.partial(job_func, *args, **kwargs)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            functools.update_wrapper(self.job_func, job_func)</span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="comment"># 计算下一次调度的时间</span></span><br><span class="line">        self._schedule_next_run()</span><br><span class="line">        self.scheduler.jobs.append(self)</span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure></p><p>在这两个方法之间, 就是通过建造者模式, 构造出其他控制参数的组合, 以实现各种各样的调度节奏;<br>下面来重点讲一下各参数组合如何实现调度节奏的控制;<br>从上一节关于 Scheduler 类的描述中可以看到, 上层调度中最关键的逻辑, 判断每一个注册的 job 是否应该被调度, 其实是 Job 类的一个回调方法 <code>should_run</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">should_run</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> datetime.datetime.now() &gt;= self.next_run</span><br></pre></td></tr></table></figure></p><p>而 should_run 方法中的判断的依据, 是当前时间有没有到达 <code>next_run</code> 这个实例字段给出的时间点;<br>next_run 字段的设置则通过在 <code>Job.do(self, job_func, *args, **kwargs)</code> 方法 (上文已给出) 和 <code>Job.run(self)</code> 方法中调用 <code>__schedule_next_run()</code> 方法来实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">    logger.info(<span class="string">'Running job %s'</span>, self)</span><br><span class="line">    ret = self.job_func()</span><br><span class="line">    self.last_run = datetime.datetime.now()</span><br><span class="line">    <span class="comment"># 计算下一次调度的时间</span></span><br><span class="line">    self._schedule_next_run()</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></p><p>所以, 所有的秘密就存在于 <code>_schedule_next_run()</code> 方法里了; 下面将结合几大类参数组合的设置, 拆开来分析 _schedule_next_run() 方法的逻辑;<br>这些 Job 类的参数组合, 大致可分为这几类:</p><h4 id="总基调-指定调度的周期"><a href="#总基调-指定调度的周期" class="headerlink" title="总基调: 指定调度的周期"></a><strong>总基调: 指定调度的周期</strong></h4><p>以下方法将会设置 unit 参数, 与 interval 参数结合, 定义调度区间间隔:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">second</span><span class="params">(self)</span>:</span>   <span class="function"><span class="keyword">def</span> <span class="title">seconds</span><span class="params">(self)</span>:</span>  <span class="comment"># self.unit = 'seconds'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minute</span><span class="params">(self)</span>:</span>   <span class="function"><span class="keyword">def</span> <span class="title">minutes</span><span class="params">(self)</span>:</span>  <span class="comment"># self.unit = 'minutes'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hour</span><span class="params">(self)</span>:</span>     <span class="function"><span class="keyword">def</span> <span class="title">hours</span><span class="params">(self)</span>:</span>    <span class="comment"># self.unit = 'hours'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">day</span><span class="params">(self)</span>:</span>      <span class="function"><span class="keyword">def</span> <span class="title">days</span><span class="params">(self)</span>:</span>     <span class="comment"># self.unit = 'days'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">week</span><span class="params">(self)</span>:</span>     <span class="function"><span class="keyword">def</span> <span class="title">weeks</span><span class="params">(self)</span>:</span>    <span class="comment"># self.unit = 'weeks'</span></span><br></pre></td></tr></table></figure></p><p>其对应的 _schedule_next_run() 逻辑如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def _schedule_next_run(self)</span></span><br><span class="line">    <span class="keyword">assert</span> self.unit <span class="keyword">in</span> (<span class="string">'seconds'</span>, <span class="string">'minutes'</span>, <span class="string">'hours'</span>, <span class="string">'days'</span>, <span class="string">'weeks'</span>)</span><br><span class="line">    <span class="keyword">if</span> self.latest <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> self.latest &gt;= self.interval</span><br><span class="line">        interval = random.randint(self.interval, self.latest)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        interval = self.interval</span><br><span class="line"></span><br><span class="line">    self.period = datetime.timedelta(**&#123;self.unit: interval&#125;)</span><br><span class="line">    self.next_run = datetime.datetime.now() + self.period</span><br></pre></td></tr></table></figure></p><p>先不管其中涉及到的 latest 字段 (下文描述), 其他的逻辑清晰可见: 使用 unit 和 interval 构造出一个指定的 timedelta, 加上当前时间得到下次调度的时间;<br>&nbsp;<br>这是最简单的一类, 定下了整个调度的总体节奏; 而下面几个类别的参数并不能单独决定调度周期, 而是在第一类参数的基础之上实施局部调整, 以达到综合控制;</p><h4 id="局部调整1-指定调度的起始-weekday"><a href="#局部调整1-指定调度的起始-weekday" class="headerlink" title="局部调整1: 指定调度的起始 weekday"></a><strong>局部调整1: 指定调度的起始 weekday</strong></h4><p>以下方法将会设置 start_day 参数, 确定调度开始的时间点; 同时统一设置 unit 参数为 ‘weeks’:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">monday</span><span class="params">(self)</span>:</span>       </span><br><span class="line">    self.start_day = <span class="string">'monday'</span></span><br><span class="line">    self.unit = <span class="string">'weeks'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tuesday</span><span class="params">(self)</span>:</span>      </span><br><span class="line">    self.start_day = <span class="string">'tuesday'</span></span><br><span class="line">    self.unit = <span class="string">'weeks'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wednesday</span><span class="params">(self)</span>:</span>    </span><br><span class="line">    self.start_day = <span class="string">'wednesday'</span></span><br><span class="line">    self.unit = <span class="string">'weeks'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">thursday</span><span class="params">(self)</span>:</span>   </span><br><span class="line">    self.start_day = <span class="string">'thurday'</span></span><br><span class="line">    self.unit = <span class="string">'weeks'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">friday</span><span class="params">(self)</span>:</span>       </span><br><span class="line">    self.start_day = <span class="string">'friday'</span></span><br><span class="line">    self.unit = <span class="string">'weeks'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saturday</span><span class="params">(self)</span>:</span>     </span><br><span class="line">    self.start_day = <span class="string">'saturday'</span></span><br><span class="line">    self.unit = <span class="string">'weeks'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sunday</span><span class="params">(self)</span>:</span>     </span><br><span class="line">    self.start_day = <span class="string">'sunday'</span></span><br><span class="line">    self.unit = <span class="string">'weeks'</span></span><br></pre></td></tr></table></figure></p><p>其对应的 _schedule_next_run() 逻辑如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def _schedule_next_run(self)</span></span><br><span class="line">    <span class="keyword">if</span> self.start_day <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> self.unit == <span class="string">'weeks'</span></span><br><span class="line">        weekdays = (<span class="string">'monday'</span>, <span class="string">'tuesday'</span>, <span class="string">'wednesday'</span>, <span class="string">'thursday'</span>, <span class="string">'friday'</span>, <span class="string">'saturday'</span>, <span class="string">'sunday'</span>)</span><br><span class="line">        <span class="keyword">assert</span> self.start_day <span class="keyword">in</span> weekdays</span><br><span class="line">        weekday = weekdays.index(self.start_day)</span><br><span class="line">        days_ahead = weekday - self.next_run.weekday()</span><br><span class="line">        <span class="keyword">if</span> days_ahead &lt;= <span class="number">0</span>:  <span class="comment"># Target day already happened this week</span></span><br><span class="line">            days_ahead += <span class="number">7</span></span><br><span class="line">        self.next_run += datetime.timedelta(days_ahead) - self.period</span><br></pre></td></tr></table></figure></p><p>可以发现, start_day 只是在 next_run 原有的 weekday 基础上增加了一个 offset, 相当于是 delay time;</p><h4 id="局部调整2-指定调度的起始时间"><a href="#局部调整2-指定调度的起始时间" class="headerlink" title="局部调整2: 指定调度的起始时间"></a><strong>局部调整2: 指定调度的起始时间</strong></h4><p>以下方法将会设置 at_time 参数, 其针对 unit == ‘hours’ 只设置 minute 变量, 而对 unit == ‘days’ 或 ‘weeks’ 才会设置 hour 变量;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">at</span><span class="params">(self, time_str)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> self.unit <span class="keyword">in</span> (<span class="string">'days'</span>, <span class="string">'hours'</span>) <span class="keyword">or</span> self.start_day</span><br><span class="line">    hour, minute = time_str.split(<span class="string">':'</span>)</span><br><span class="line">    minute = int(minute)</span><br><span class="line">    <span class="keyword">if</span> self.unit == <span class="string">'days'</span> <span class="keyword">or</span> self.start_day:</span><br><span class="line">        hour = int(hour)</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= hour &lt;= <span class="number">23</span></span><br><span class="line">    <span class="keyword">elif</span> self.unit == <span class="string">'hours'</span>:</span><br><span class="line">        hour = <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= minute &lt;= <span class="number">59</span></span><br><span class="line">    self.at_time = datetime.time(hour, minute)</span><br><span class="line">    <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure></p><p>其对应的 _schedule_next_run() 逻辑也与上面类似, 针对 unit == ‘days’ 或 ‘weeks’ 才设 hour 字段, 否则只设置 minute 和 second;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def _schedule_next_run(self)</span></span><br><span class="line">    <span class="keyword">if</span> self.at_time <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> self.unit <span class="keyword">in</span> (<span class="string">'days'</span>, <span class="string">'hours'</span>) <span class="keyword">or</span> self.start_day <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line">        kwargs = &#123;</span><br><span class="line">            <span class="string">'minute'</span>: self.at_time.minute,</span><br><span class="line">            <span class="string">'second'</span>: self.at_time.second,</span><br><span class="line">            <span class="string">'microsecond'</span>: <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> self.unit == <span class="string">'days'</span> <span class="keyword">or</span> self.start_day <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            kwargs[<span class="string">'hour'</span>] = self.at_time.hour</span><br><span class="line">        self.next_run = self.next_run.replace(**kwargs)</span><br></pre></td></tr></table></figure></p><h4 id="局部调整3-在给定范围内随机安排调度时刻"><a href="#局部调整3-在给定范围内随机安排调度时刻" class="headerlink" title="局部调整3: 在给定范围内随机安排调度时刻"></a><strong>局部调整3: 在给定范围内随机安排调度时刻</strong></h4><p>对应的就是上文提及的 latest 参数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to</span><span class="params">(self, latest)</span>:</span></span><br><span class="line">    self.latest = latest</span><br><span class="line">    <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure></p><p>其对应的 _schedule_next_run() 逻辑如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def _schedule_next_run(self)</span></span><br><span class="line">    <span class="keyword">if</span> self.latest <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> self.latest &gt;= self.interval</span><br><span class="line">        interval = random.randint(self.interval, self.latest)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        interval = self.interval</span><br></pre></td></tr></table></figure></p><p>具体逻辑就是在给定的 [interval, latest) 区间内, 生成一个随机数作为下次调度的 interval;<br>&nbsp;<br>至此, Job 类的逻辑就都分析完了;</p><h2 id="apscheduler"><a href="#apscheduler" class="headerlink" title="apscheduler"></a><strong>apscheduler</strong></h2><h2 id="celery"><a href="#celery" class="headerlink" title="celery"></a><strong>celery</strong></h2><h2 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h2><ul><li><a href="">python module 使用总结: heapq</a></li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="https://docs.python.org/2/library/sched.html" target="_blank" rel="noopener">8.8. sched — Event scheduler</a></li><li><a href="http://blog.csdn.net/leonard_wang/article/details/54017537" target="_blank" rel="noopener">python sched模块学习</a></li><li><a href="https://www.cnblogs.com/anpengapple/p/8051923.html" target="_blank" rel="noopener">python中的轻量级定时任务调度库: schedule</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在 java 里, 第三方定时调度框架比较常用的是 quartz 和 springframework 提供的 schedule 功能; 不过在各大公司里, 一般都会开发自己能集中管理与灵活调控的调度组件; 这样一来, 第三方的调度框架反而就接触的少了;&lt;br&gt;我相信在以 python 为主要使用语言的公司里, 一定也有自己的调度中间件; 但是对于以 java 为主的公司里, 肯定不可能专为 python 维护一套调度系统, 所以就很有必要了解一下 python 里的定时调度模块; 本文将介绍几种常用的 python 定时调度框架:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简单的实现: sched 与 schedule;&lt;/li&gt;
&lt;li&gt;功能增强版: apscheduler;&lt;/li&gt;
&lt;li&gt;分布式调度器: celery;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="python" scheme="http://zshell.cc/categories/python/"/>
    
      <category term="module" scheme="http://zshell.cc/categories/python/module/"/>
    
    
      <category term="python:module" scheme="http://zshell.cc/tags/python-module/"/>
    
      <category term="python" scheme="http://zshell.cc/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>一个 dev 的拙劣前端笔记: 使用 jQuery ajax 上传文件</title>
    <link href="http://zshell.cc/2018/02/03/web--%E4%B8%80%E4%B8%AAdev%E7%9A%84%E6%8B%99%E5%8A%A3%E5%89%8D%E7%AB%AF%E7%AC%94%E8%AE%B0_%E4%BD%BF%E7%94%A8jQuery_ajax%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6/"/>
    <id>http://zshell.cc/2018/02/03/web--一个dev的拙劣前端笔记_使用jQuery_ajax上传文件/</id>
    <published>2018-02-03T13:48:08.000Z</published>
    <updated>2018-02-07T15:43:29.215Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>从传统的刷新提交到 ajax 提交, 从间接的 iframe 刷新 ajax 提交到真正意义上的 ajax 提交, 关于前端文件上传的方法, 伴随着 web 技术与标准的演进, 不断推陈出新;<br>本文整理了从传统方式到 ajax 方式上传文件的各种方法;</p></blockquote><a id="more"></a><hr><h3 id="传统的上传文件方式"><a href="#传统的上传文件方式" class="headerlink" title="传统的上传文件方式"></a><strong>传统的上传文件方式</strong></h3><p>form 表单有三种可能的 MIME 编码类型: 默认的 <code>application/x-www-form-urlencoded</code>, 不对字符编码而保留原始信息的 <code>multipart/form-data</code>, 以及纯文本 <code>text/plain</code>;<br>如果没有异步刷新的需求, 只需要将 form 表单的 enctype 属性设置为 <code>multipart/form-data</code>, 便可以二进制的方式提交表单内容, 以达到上传文件的目的:<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">id</span>=<span class="string">"form_id"</span> <span class="attr">enctype</span>=<span class="string">"multipart/form-data"</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"str"</span> /&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span> <span class="attr">name</span>=<span class="string">"fileAttach"</span> /&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"upload"</span> /&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>关于 MIME 类型 <code>multipart/form-data</code> 的更多内容, 请参见: <a href="">一个 dev 的拙劣前端笔记: content-type 之 multipart/form-data 规范整理</a>;<br>&nbsp;<br><strong>下面来讨论如何使用 ajax 实现文件上传;</strong></p><h3 id="使用-jQuery-ajaxFileUpload-插件实现文件上传"><a href="#使用-jQuery-ajaxFileUpload-插件实现文件上传" class="headerlink" title="使用 jQuery ajaxFileUpload 插件实现文件上传"></a><strong>使用 jQuery ajaxFileUpload 插件实现文件上传</strong></h3><p>ajax 默认使用的 MIME 类型是 <code>application/x-www-form-urlencoded</code>, 这种方式只适用于传输普通字符串类型的数据; 由于在 HTML4 时代, 没有对 javascript 提供文件读取的接口, 使用 <code>document.getElementById(&#39;field_id&#39;).value</code> 也只能获得文件的 name, 并不能拿到文件的二进制数据; 所以, 想直接使用 ajax 无刷新提交表单是无法做到的;<br>所以只能采用间接的方案, 比如基于 jQuery 拓展的 ajaxFileUpload 插件, 其代码逻辑大致如下: </p><ol><li>function createUploadIframe():<br>创建一个独立的 iframe, 并追加到 body 中;</li><li>function createUploadForm(file_elem_id):<br>创建一个独立的 form, 设置 enctype 为 <code>multipart/form-data</code>;<br>根据 file_elem_id 找到页面里的目标 <code>&lt;input type=&quot;file&quot; /&gt;</code> 对象, 使用 jQuery.clone 方法, 将新的克隆对象替换到目标对象的位置, 而将原目标对象追加到新建的 form 中(偷梁换柱);<br>最后将新创建的 form 追加到 body 中;</li><li>function addOtherRequestsToForm(data, new_form):<br>将页面中目标表单的其他元素数据, 一并追加到新创建的 form 里;</li><li>function ajaxFileUpload:<br>调用 createUploadForm 方法创建新 form;<br>调用 addOtherRequestsToForm 方法捎带除 file 之外的其余元素数据;<br>调用 createUploadIFrame 方法创建 iframe;<br>将新 form 的 target 属性设置为新创建 iframe 的 id, 以实现间接的无刷新;<br>submit 提交新 form;</li></ol><p>&nbsp;<br>ajaxFileUpload 的实现逻辑并不复杂, 类似这样的插件在 github 上有各种各样的版本, 我选取了一个比较典型的实现: <a href="https://github.com/carlcarl/AjaxFileUpload/blob/master/ajaxfileupload.js" target="_blank" rel="noopener">carlcarl/AjaxFileUpload/ajaxfileupload.js</a>;<br>然后开发者在实际使用时需要调用的是 <code>jQuery.ajaxFileUpload</code> 方法, 设置一些参数与回调方法:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">ajax_submit</span>(<span class="params">field_id</span>) </span>&#123;</span><br><span class="line">    $.ajaxFileUpload(&#123;</span><br><span class="line">        fileElementId: field_id,    <span class="comment">// &lt;input id="field_id" type="file"&gt;, 对应元素的 id</span></span><br><span class="line">        data: fetch_form_data(<span class="string">'form_id'</span>),   <span class="comment">// 捎带其余元素的数据</span></span><br><span class="line">        url: <span class="string">'/xxx/yyy/upload'</span></span><br><span class="line">        type: <span class="string">'post'</span>,</span><br><span class="line">        dataType: <span class="string">'json'</span>,</span><br><span class="line">        secureuri: <span class="literal">false</span>,   <span class="comment">//是否启用安全提交，默认为false</span></span><br><span class="line">        <span class="keyword">async</span> : <span class="literal">true</span>,   <span class="comment">//是否是异步</span></span><br><span class="line">        success: <span class="function"><span class="keyword">function</span>(<span class="params">data</span>) </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (data[<span class="string">'status'</span>] == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="built_in">window</span>.location.reload();</span><br><span class="line">                alert(<span class="string">"提交成功"</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="built_in">window</span>.location.reload();</span><br><span class="line">                alert(<span class="string">"提交失败:"</span> + data[<span class="string">'message'</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        error: <span class="function"><span class="keyword">function</span>(<span class="params">data, status, e</span>) </span>&#123;</span><br><span class="line">            <span class="built_in">window</span>.location.reload();</span><br><span class="line">            alert(<span class="string">"提交失败:"</span> + data[<span class="string">'message'</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将给定的表单数据转为对象</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">fetch_form_data</span>(<span class="params">form_id</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> params = $(<span class="string">'#'</span> + form_id).serializeArray();  </span><br><span class="line">    <span class="keyword">var</span> values = &#123;&#125;;  </span><br><span class="line">    <span class="keyword">for</span>( x <span class="keyword">in</span> params ) &#123;  </span><br><span class="line">        values[params[x].name] = params[x].value;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> values</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>抛开 iframe 的性能影响不谈, 看起来这样的 api 还是相当友好的, 与 jQuery.ajax 同样方便, 还解决了 ajax 不能传输二进制流的问题;<br>另外, 由于这种方式真正提交的表单完全是 javascript 创建出来的, 页面上自己写的那个表单, 只作为数据 clone 的载体, 所以只需要确保表单和其中的 file input 元素有自己的 id, 最后提交按钮的 onclick 事件指向了目标方法即可;<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">id</span>=<span class="string">"form_id"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"str"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"file_attach"</span> <span class="attr">type</span>=<span class="string">"file"</span> <span class="attr">name</span>=<span class="string">"fileAttach"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">onclick</span>=<span class="string">"ajax_submit('file_attach')"</span>  <span class="attr">value</span>=<span class="string">"upload"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h3 id="使用-jQuery-ajax-结合-HTML5-API-实现文件上传"><a href="#使用-jQuery-ajax-结合-HTML5-API-实现文件上传" class="headerlink" title="使用 jQuery ajax 结合 HTML5 API 实现文件上传"></a><strong>使用 jQuery ajax 结合 HTML5 API 实现文件上传</strong></h3><p>使用 ajaxFileUplaod 插件, 无论怎么优化改造, 其需要使用 iframe 作间接无刷新的逻辑是没法绕开的; 而使用 iframe 必然会带来额外资源的消耗, 如果有更原生直接的解决方案, 我们一定乐于在项目中取代 ajaxFileUpload;<br>于是, 在 HTML5 时代, 出现了一个新的接口: <code>FormData</code>, 它给出了完美的解决方案;<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> form_content = <span class="keyword">new</span> FormData(<span class="built_in">document</span>.getElementById(<span class="string">"form_id"</span>));</span><br></pre></td></tr></table></figure></p><p>这行代码便拿到了目标表单对象的所有信息; 我们只需要确保表单的 enctype 属性为 <code>multipart/form-data</code>, 通过该接口获得的 FormData 对象, 便是完整的二进制序列化信息:<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">id</span>=<span class="string">"form_id"</span> <span class="attr">enctype</span>=<span class="string">"multipart/form-data"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"str"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span> <span class="attr">name</span>=<span class="string">"fileAttach"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">onclick</span>=<span class="string">"upload_file()"</span>  <span class="attr">value</span>=<span class="string">"upload"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>这样, 一个 onclick 事件触发 upload_file 方法, 使用原生的 jQuery ajax 就实现了上传文件的功能了, 同时表单内的其他字符串数据, 也一并以 multi part 的形式上传上去了;<br>对应的 javascript upload_file 方法如下:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">uplaod_file</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> form_content = <span class="keyword">new</span> FormData(<span class="built_in">document</span>.getElementById(<span class="string">'form_id'</span>));</span><br><span class="line">    $.ajax(&#123;</span><br><span class="line">        type: <span class="string">'POST'</span>,</span><br><span class="line">        url: <span class="string">'/xxx/yyy/upload'</span>,</span><br><span class="line">        data: form_content,</span><br><span class="line">        processData: <span class="literal">false</span>,     <span class="comment">// 阻止默认的 application/x-www-form-urlencoded 对象处理方法</span></span><br><span class="line">        contentType: <span class="literal">false</span>,     <span class="comment">// 与 processData 保持一直, 不使用默认的 application/x-www-form-urlencoded</span></span><br><span class="line">        success: <span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (data[<span class="string">'status'</span>] == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="built_in">window</span>.location.reload();</span><br><span class="line">                alert(<span class="string">"提交成功"</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="built_in">window</span>.location.reload();</span><br><span class="line">                alert(<span class="string">"提交失败:"</span> + data[<span class="string">'message'</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        fail: <span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span><br><span class="line">            <span class="built_in">window</span>.location.reload();</span><br><span class="line">            alert(<span class="string">"提交失败:"</span> + data[<span class="string">'message'</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>以上代码需要注意的是:<br><code>processData</code> 参数默认为 true, 即将 data 转为 url 键值对形式, 这里已经是序列化后的二进制数据, 不需要再次处理,  所以应主动设置其为 false;<br>同时, <code>contentType</code> 默认为 <code>application/x-www-form-urlencoded</code>, 这里不应该使用默认值;<br>关于 jQuery ajax 方法, 更多的内容请参见: <a href="">jQuery ajax 阅读与理解</a>;<br>&nbsp;<br>这便是 HTML5 时代下,  ajax 异步上传文件的最佳实践;</p><h3 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h3><ul><li><a href="">一个 dev 的拙劣前端笔记: content-type 之 multipart/form-data 规范整理</a></li><li><a href="">jQuery ajax 阅读与理解</a></li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="http://blog.csdn.net/qq_33556185/article/details/51086114" target="_blank" rel="noopener">jquery Ajax提交表单(使用jquery Ajax上传附件)</a></li><li><a href="https://www.cnblogs.com/zhanghaoliang/p/6513964.html" target="_blank" rel="noopener">JQuery的ajaxFileUpload的使用</a></li><li><a href="https://github.com/carlcarl/AjaxFileUpload/blob/master/ajaxfileupload.js" target="_blank" rel="noopener">carlcarl/AjaxFileUpload/ajaxfileupload.js</a></li><li><a href="http://blog.csdn.net/it_man/article/details/43800957" target="_blank" rel="noopener">jquery插件–ajaxfileupload.js上传文件原理分析</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;从传统的刷新提交到 ajax 提交, 从间接的 iframe 刷新 ajax 提交到真正意义上的 ajax 提交, 关于前端文件上传的方法, 伴随着 web 技术与标准的演进, 不断推陈出新;&lt;br&gt;本文整理了从传统方式到 ajax 方式上传文件的各种方法;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="web" scheme="http://zshell.cc/categories/web/"/>
    
    
      <category term="ajax" scheme="http://zshell.cc/tags/ajax/"/>
    
      <category term="jQuery" scheme="http://zshell.cc/tags/jQuery/"/>
    
      <category term="文件上传" scheme="http://zshell.cc/tags/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"/>
    
  </entry>
  
  <entry>
    <title>logrotate 配置与运维</title>
    <link href="http://zshell.cc/2018/01/15/linux-varlog--logrotate%E9%85%8D%E7%BD%AE%E4%B8%8E%E8%BF%90%E7%BB%B4/"/>
    <id>http://zshell.cc/2018/01/15/linux-varlog--logrotate配置与运维/</id>
    <published>2018-01-14T16:23:27.000Z</published>
    <updated>2018-01-27T14:53:07.577Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要讨论以下几个方面:</p><ol><li>logrotate 的关键配置文件和配置项语法;</li><li>logrotate 的使用与运维技巧;</li><li>logrotate 的运行原理;</li><li>特殊场景下 logrotate 的代替方案;</li></ol></blockquote><a id="more"></a><hr><h3 id="配置文件与配置语法"><a href="#配置文件与配置语法" class="headerlink" title="配置文件与配置语法"></a><strong>配置文件与配置语法</strong></h3><p>logrotate 的配置文件主要是 <code>/etc/logrotate.conf</code> 和 <code>/etc/logrotate.d</code> 目录;<br>/etc/logrotate.conf 文件作为主配置文件, include 了 /etc/logrotate.d 目录下具体的配置内容;<br>以下是 /etc/logrotate.conf 的默认内容:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认的历史日志保留周期单位: 周</span></span><br><span class="line">weekly</span><br><span class="line"><span class="comment"># 历史日志保留四个周期单位, 即四周, 一个月</span></span><br><span class="line">rotate 4</span><br><span class="line"><span class="comment"># use the syslog group by default, since this is the owning group of /var/log/syslog.</span></span><br><span class="line">su root syslog</span><br><span class="line"><span class="comment"># 当旧日志作了 rotate 之后, 将会创建一个和旧日志同名的新文件</span></span><br><span class="line">create</span><br><span class="line"><span class="comment"># 默认使用 gzip 压缩旧日志文件</span></span><br><span class="line">compress</span><br><span class="line"><span class="comment"># 将 /etc/logrotate.d 下面的所有独立配置文件都 include 进来</span></span><br><span class="line">include /etc/logrotate.d</span><br></pre></td></tr></table></figure></p><p>/etc/logrotate.conf 的默认配置优先级比 /etc/logrotate.d/ 目录下的独立配置要低, /etc/logrotate.d 下所有的独立配置文件中的配置项可以覆盖 /etc/logrotate.conf;<br>以 rsyslog 的配置文件为例, 以下是 /etc/logrotate.d/rsyslog 的内容:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">/var/<span class="built_in">log</span>/syslog &#123;</span><br><span class="line">    <span class="comment"># 以 天 为周期单位, 保留 7 天的日志</span></span><br><span class="line">    daily</span><br><span class="line">    rotate 7</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 忽略任何错误, 比如找不到文件</span></span><br><span class="line">    missingok</span><br><span class="line"></span><br><span class="line">    <span class="comment"># not if empty, 当日志内容为空时, 不作 rotate</span></span><br><span class="line">    notifempty</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 压缩日志, 但是采用延时压缩, 即本轮周期产生的日志不压缩, 而在下一个周期时压缩之</span></span><br><span class="line">    compress</span><br><span class="line">    delaycompress</span><br><span class="line"></span><br><span class="line">    <span class="comment"># postrotate/endscript 内的命令, 作为后处理, 会在本轮周期 rotate 之后回调执行</span></span><br><span class="line">    postrotate</span><br><span class="line">invoke-rc.d rsyslog rotate &gt; /dev/null</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以同时指定多个目标日志使用同一段配置</span></span><br><span class="line">/var/<span class="built_in">log</span>/mail.info</span><br><span class="line">/var/<span class="built_in">log</span>/mail.warn</span><br><span class="line">/var/<span class="built_in">log</span>/mail.err</span><br><span class="line">/var/<span class="built_in">log</span>/mail.log</span><br><span class="line">/var/<span class="built_in">log</span>/daemon.log</span><br><span class="line">/var/<span class="built_in">log</span>/kern.log</span><br><span class="line">/var/<span class="built_in">log</span>/auth.log</span><br><span class="line">/var/<span class="built_in">log</span>/user.log</span><br><span class="line">/var/<span class="built_in">log</span>/lpr.log</span><br><span class="line">/var/<span class="built_in">log</span>/cron.log</span><br><span class="line">/var/<span class="built_in">log</span>/debug</span><br><span class="line">/var/<span class="built_in">log</span>/messages &#123;</span><br><span class="line">    weekly</span><br><span class="line">    rotate 4</span><br><span class="line"></span><br><span class="line">    missingok</span><br><span class="line">    notifempty</span><br><span class="line"></span><br><span class="line">    compress</span><br><span class="line">    delaycompress</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 共享处理脚本, 仅对 prerotate/postrotate 定义时生效</span></span><br><span class="line">    sharedscripts</span><br><span class="line"></span><br><span class="line">    postrotate</span><br><span class="line">invoke-rc.d rsyslog rotate &gt; /dev/null</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>注意:</p><ol><li><code>sharedscripts</code> 选项打开后, 所有使用该段配置作 rotate 的目标日志名都会作为参数一次性传给 prerotate/postrotate;<br>而默认的选项 <code>nosharedscripts</code> 则是将每一个日志名分别作为参数传给 prerotate/postrotate;</li><li>logrotate 支持的周期单位, 有 hourly, daily, weekly, monthly; 但是这里有坑: hourly 默认是不生效的, 具体原因见本文第三节;</li></ol><p>&nbsp;<br>如上所叙, prerotate/postrotate 是一种在 rotate 过程中某个时机回调的一段脚本, 像这样类似的配置项总共有如下几种 (所有的配置项必须与 <code>endscript</code> 成对出现):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在所有匹配的日志 rotate 之前, 仅执行一次</span></span><br><span class="line">firstaction/endscript</span><br><span class="line"><span class="comment"># 在日志 rotate 之前回调</span></span><br><span class="line">prerotate/endscript</span><br><span class="line"><span class="comment"># 在日志 rotate 之后回调</span></span><br><span class="line">postrotate/endscript</span><br><span class="line"><span class="comment"># 在所有匹配的日志 rotate 之后, 仅执行一次</span></span><br><span class="line">lastaction/endscript</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在某个日志将要被删除之前回调执行</span></span><br><span class="line">preremove/endscript</span><br></pre></td></tr></table></figure></p><p>这几种回调时间点的设计, 不禁让人想到 junit 测试类几种注解的方法执行时机, 不得不说有异曲同工之妙;<br>&nbsp;<br>rsyslog 的 logrotate 配置是一个典型, 但同时 logrotate 还有着其他的个性化配置选项:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下是另一段案例</span></span><br><span class="line">/var/<span class="built_in">log</span>/test.log &#123;</span><br><span class="line">    <span class="comment"># 不以时间为周期单位, 而是以 日志size 为周期单位, 当日志大小达到 100MB 时, 作一次 rotate, 日志保留 5 个周期</span></span><br><span class="line">    size=100M</span><br><span class="line">    rotate 5</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用日期命名 rotate 后的旧文件, 日期格式采用 -%Y-%m-%d</span></span><br><span class="line">    dateext</span><br><span class="line">    dateformat -%Y-%m-%d</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 以指定的权限掩码, owner/group 创建 rotate 后的新文件</span></span><br><span class="line">    create 644 root root</span><br><span class="line">    </span><br><span class="line">    postrotate</span><br><span class="line">        /usr/bin/killall -HUP rsyslogd</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="logrotate-命令的常用运维选项"><a href="#logrotate-命令的常用运维选项" class="headerlink" title="logrotate 命令的常用运维选项"></a><strong>logrotate 命令的常用运维选项</strong></h3><p>1.指定目标配置文件, 手动执行:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将会执行 /etc/logrotate.d/ 下所有的配置</span></span><br><span class="line">logrotate /etc/logrotate.conf</span><br><span class="line"><span class="comment"># 将会只执行指定配置文件中的配置</span></span><br><span class="line">logrotate /etc/logrotate.d/xxx.log</span><br></pre></td></tr></table></figure></p><p>2.debug 验证配置文件正误:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -d:   --debug</span></span><br><span class="line">&gt; logrotate -d /etc/logrotate.d/redis-server.log</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">reading config file /etc/logrotate.d/redis-server</span><br><span class="line">Handling 1 logs</span><br><span class="line">rotating pattern: /var/<span class="built_in">log</span>/redis/redis-server*.<span class="built_in">log</span>  weekly (12 rotations)</span><br><span class="line">empty <span class="built_in">log</span> files are not rotated, old logs are removed</span><br><span class="line">considering <span class="built_in">log</span> /var/<span class="built_in">log</span>/redis/redis-server.log</span><br><span class="line">  <span class="built_in">log</span> does not need rotating</span><br></pre></td></tr></table></figure></p><p>3.强制 rotate:<br>即便当前不满足 rotate 的条件, force rotate 也会强制作一次 rotate, 而那些超过指定轮数的旧日志将会被删除;<br>force rotate 比较适用于加入了新的配置文件, 需要对其存量历史立即作一次 rotate;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -f:   --force</span></span><br><span class="line">logrotate -f /etc/logrotate.d/xxx.log</span><br></pre></td></tr></table></figure></p><p>4.verbose 详细信息:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -v:   --verbose</span></span><br><span class="line">logrotate -vf /etc/logrotate.d/xxx.log</span><br></pre></td></tr></table></figure></p><p>5.指定 logrotate 自身的日志文件路径:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -s:   --state</span></span><br><span class="line"><span class="comment"># 默认 logrotate 的日志路径: /var/lib/logrotate/status</span></span><br><span class="line">logrotate -s /tmp/logrotate.log /etc/logrotate.conf</span><br></pre></td></tr></table></figure></p><h3 id="logrotate-的运行原理及其缺陷"><a href="#logrotate-的运行原理及其缺陷" class="headerlink" title="logrotate 的运行原理及其缺陷"></a><strong>logrotate 的运行原理及其缺陷</strong></h3><p>logrotate 并不是一个 daemon service, 其本质上只是一个 ‘什么时候调用就什么时候立即执行一次’ 的 C 程序;<br>所以 logrotate 的执行, 依赖于其他 daemon service 的调用, 那么最自然的就是通过 crond 定时任务来调用了;<br>默认情况下, logrotate 是一天被调用一次的, 因为与它相关的 crontab 配置在 <code>/etc/cron.daily</code> 里:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Clean non existent log file entries from status file</span></span><br><span class="line"><span class="built_in">cd</span> /var/lib/logrotate</span><br><span class="line"><span class="built_in">test</span> -e status || touch status</span><br><span class="line">head -1 status &gt; status.clean</span><br><span class="line">sed <span class="string">'s/"//g'</span> status | <span class="keyword">while</span> <span class="built_in">read</span> logfile date</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    [ -e <span class="string">"<span class="variable">$logfile</span>"</span> ] &amp;&amp; <span class="built_in">echo</span> <span class="string">"\"<span class="variable">$logfile</span>\" <span class="variable">$date</span>"</span></span><br><span class="line"><span class="keyword">done</span> &gt;&gt; status.clean</span><br><span class="line">mv status.clean status</span><br><span class="line"></span><br><span class="line"><span class="built_in">test</span> -x /usr/sbin/logrotate || <span class="built_in">exit</span> 0</span><br><span class="line">/usr/sbin/logrotate /etc/logrotate.conf</span><br></pre></td></tr></table></figure></p><p>如本文第二节所述, 由于 logrotate 的执行方式是通过 cron 默认 1 天执行一次, 所以按小时 rotate 的 <code>hourly</code> 配置项, 默认是不生效的; logrotate 的 manual 文档里也有说明:</p><blockquote><p><code>hourly</code> Log files are rotated every hour. Note that usually logrotate is configured to be run by cron daily. You have to change this configuration and run logrotate hourly to be able to really rotate logs hourly.</p></blockquote><p>不过, 这还不是最大的问题, 毕竟我们只要把上述脚本放到 <code>cron.hourly</code> 里, 就能解决该问题;<br>这种靠定时任务来运行的方式, 最大的问题是: 当我们对某个日志配置成按 <code>size</code> 来 rotate 时, 无法做到当日志触达 size 条件时及时切分, 其所能实现的最小延时是一分钟 (当把 logrotate 脚本的定时任务配成 * * * * *, 即每分钟执行一次时), 没法更短了;</p><h3 id="其他的特殊场景"><a href="#其他的特殊场景" class="headerlink" title="其他的特殊场景"></a><strong>其他的特殊场景</strong></h3><p>logrotate 集日志切分, 日志压缩, 删除旧日志, 邮件提醒等功能为一体, 提供了非常完整的日志管理策略; 不过, 并不是所有的系统日志, 自身都不具有上述功能, 都需要依赖 logrotate 来管理自己;<br>有一个非常典型, 而且使用十分广泛的场景: tomcat web 服务器; 当我们在 tomcat 上部署的服务使用了诸如 logback 之类的第三方日志框架时, 日志切分, 日志压缩等服务它自己便能够胜任了 (与 logback 相关功能的文章请见: <a href="">logback appender 使用总结</a>), 而且我们绝大部分人 (去哪儿网), 即便不怎么接触 logback 的日志压缩功能, 也至少都习惯于使用 logback  <code>RollingFileAppender</code> 的基础功能去作日志切分;<br>基于以上, 我们只需要一个简单的脚本, 便能够满足日常的 tomcat web 服务器日志运维:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">HOUR1=$(date -d <span class="string">"1 hours ago"</span> +%F-%H)</span><br><span class="line">DATE7=$(date -d <span class="string">"7 days ago"</span> +%F-%H)</span><br><span class="line"><span class="comment"># for example: /home/web/my_server/logs</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `find /home/web/ -maxdepth 2 \( -<span class="built_in">type</span> d -o -<span class="built_in">type</span> l \) -name logs`; <span class="keyword">do</span></span><br><span class="line">        find -L <span class="variable">$i</span> -maxdepth 1 -<span class="built_in">type</span> f \( -name <span class="string">"*<span class="variable">$&#123;HOUR1&#125;</span>*"</span> -a ! -name <span class="string">"*.gz"</span> \) -<span class="built_in">exec</span> gzip &#123;&#125; \;</span><br><span class="line">        find -L <span class="variable">$i</span> -maxdepth 1 -<span class="built_in">type</span> f \( -name <span class="string">"*<span class="variable">$&#123;DATE7&#125;</span>*"</span> -a -name <span class="string">"*.gz"</span> \) -<span class="built_in">exec</span> rm -f &#123;&#125; \;</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p><p>本节内容讨论的是针对 tomcat web 系统上的日志切分, 压缩, 以及删除等常规运维内容; 其实, 针对公司各业务线 web 系统的业务日志, 除此之外至少还有另外两项重要的运维内容: <em>日志冷备份收集</em> 与 <em>日志实时收集及其可视化 (ELK)</em>; 与之相关的内容请参见如下文章: </p><ol><li><a href="">改造 flume-ng: 融入公司的技术体系</a>;</li><li><a href="">日志冷备份收集的方案选型</a>;</li></ol><h3 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h3><ul><li><a href="">cron 相关全梳理</a></li><li><a href="">logback appender 使用总结</a></li><li><a href="">改造 flume-ng: 融入公司的技术体系</a></li><li><a href="">日志冷备份收集的方案选型</a></li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="https://linux.cn/article-4126-1.html" target="_blank" rel="noopener">Linux日志文件总管——logrotate</a></li><li><a href="https://huoding.com/2013/04/21/246" target="_blank" rel="noopener">被遗忘的 Logrotate</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要讨论以下几个方面:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;logrotate 的关键配置文件和配置项语法;&lt;/li&gt;
&lt;li&gt;logrotate 的使用与运维技巧;&lt;/li&gt;
&lt;li&gt;logrotate 的运行原理;&lt;/li&gt;
&lt;li&gt;特殊场景下 logrotate 的代替方案;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://zshell.cc/categories/linux/"/>
    
      <category term="varlog" scheme="http://zshell.cc/categories/linux/varlog/"/>
    
    
      <category term="linux:varlog" scheme="http://zshell.cc/tags/linux-varlog/"/>
    
  </entry>
  
  <entry>
    <title>财富先锋 2017 年各股池成绩单</title>
    <link href="http://zshell.cc/2017/12/31/%E8%AF%81%E5%88%B8-%E8%B4%A2%E5%AF%8C%E5%85%88%E9%94%8B--%E8%B4%A2%E5%AF%8C%E5%85%88%E9%94%8B2017%E5%B9%B4%E5%90%84%E8%82%A1%E6%B1%A0%E6%88%90%E7%BB%A9%E5%8D%95/"/>
    <id>http://zshell.cc/2017/12/31/证券-财富先锋--财富先锋2017年各股池成绩单/</id>
    <published>2017-12-31T14:00:00.000Z</published>
    <updated>2018-01-27T14:53:07.581Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>综合来看, 同花顺财富先锋 2017 年的几个股池系统的年度收益率还是比较令人满意的;<br>‘热点轮动’, ‘股东增持’ 两个股池系统的收益率达到了 200%, ‘支撑压力’ 股池系统的收益率超过 150%;<br>不过 ‘多头趋势’ 股池系统的表现比较糟糕, 2017 年净收益为负;<br>另外还有 ‘深一度’ 股池系统的收益率未显示相关指标, 暂无法统计;<br>各个股池系统 2017 年度收益率的指标, 反映出了各个选股策略在 2017 年 A 股市场上的成效; 以此为鉴, 2018 年的中国资本市场, 我们继续前行;</p></blockquote><a id="more"></a><hr><h3 id="支撑压力"><a href="#支撑压力" class="headerlink" title="支撑压力"></a><strong>支撑压力</strong></h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/finance/rcmd_diary/2017_recommend_review_%E6%94%AF%E6%92%91%E5%8E%8B%E5%8A%9B.png" alt="2017 支撑压力 final review" title="">                </div>                <div class="image-caption">2017 支撑压力 final review</div>            </figure><h3 id="热点轮动"><a href="#热点轮动" class="headerlink" title="热点轮动"></a><strong>热点轮动</strong></h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/finance/rcmd_diary/2017_recommend_review_%E7%83%AD%E7%82%B9%E8%BD%AE%E5%8A%A8.png" alt="2017 热点轮动 final review" title="">                </div>                <div class="image-caption">2017 热点轮动 final review</div>            </figure><h3 id="股东增持"><a href="#股东增持" class="headerlink" title="股东增持"></a><strong>股东增持</strong></h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/finance/rcmd_diary/2017_recommend_review_%E8%82%A1%E4%B8%9C%E5%A2%9E%E6%8C%81.png" alt="2017 股东增持 final review" title="">                </div>                <div class="image-caption">2017 股东增持 final review</div>            </figure><h3 id="多头趋势"><a href="#多头趋势" class="headerlink" title="多头趋势"></a><strong>多头趋势</strong></h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/finance/rcmd_diary/2017_recommend_review_多头趋势.png" alt="2017 多头趋势 final review" title="">                </div>                <div class="image-caption">2017 多头趋势 final review</div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;综合来看, 同花顺财富先锋 2017 年的几个股池系统的年度收益率还是比较令人满意的;&lt;br&gt;‘热点轮动’, ‘股东增持’ 两个股池系统的收益率达到了 200%, ‘支撑压力’ 股池系统的收益率超过 150%;&lt;br&gt;不过 ‘多头趋势’ 股池系统的表现比较糟糕, 2017 年净收益为负;&lt;br&gt;另外还有 ‘深一度’ 股池系统的收益率未显示相关指标, 暂无法统计;&lt;br&gt;各个股池系统 2017 年度收益率的指标, 反映出了各个选股策略在 2017 年 A 股市场上的成效; 以此为鉴, 2018 年的中国资本市场, 我们继续前行;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="证券" scheme="http://zshell.cc/categories/%E8%AF%81%E5%88%B8/"/>
    
      <category term="财富先锋" scheme="http://zshell.cc/categories/%E8%AF%81%E5%88%B8/%E8%B4%A2%E5%AF%8C%E5%85%88%E9%94%8B/"/>
    
    
      <category term="证券:财富先锋" scheme="http://zshell.cc/tags/%E8%AF%81%E5%88%B8-%E8%B4%A2%E5%AF%8C%E5%85%88%E9%94%8B/"/>
    
  </entry>
  
  <entry>
    <title>nginx module 使用总结: ngx_http_gzip_module</title>
    <link href="http://zshell.cc/2017/12/21/nginx-module--nginx_module_%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93_ngx_http_gzip_module/"/>
    <id>http://zshell.cc/2017/12/21/nginx-module--nginx_module_使用总结_ngx_http_gzip_module/</id>
    <published>2017-12-21T07:13:33.000Z</published>
    <updated>2018-05-13T06:36:48.762Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ngx_http_gzip_module 是十分有用的 nginx 模块, 其有效压缩了 http 请求大小, 节省了流量, 加快了传输速度, 提升了用户体验;<br>当然, 其在使用上也有一些坑, 本文将具体讨论一下相关内容;</p></blockquote><a id="more"></a><p>ngx_http_gzip_module 这个模块的名字其实是 <a href="http://nginx.org/en/docs/http/ngx_http_gzip_module.html" target="_blank" rel="noopener">官方文档</a> 里定义的; 然而在 nginx 源码里 (v1.11.2), 这个模块所在的源码文件名叫 <code>src/http/ngx_http_gzip_filter_module.c</code>;</p><h3 id="gzip-模块的安装"><a href="#gzip-模块的安装" class="headerlink" title="gzip 模块的安装"></a><strong>gzip 模块的安装</strong></h3><p>ngx_http_gzip_module 编译默认安装, 无需额外操作;</p><h3 id="gzip-模块的配置"><a href="#gzip-模块的配置" class="headerlink" title="gzip 模块的配置"></a><strong>gzip 模块的配置</strong></h3><p>gzip 模块的配置可以在如下位置:</p><ol><li>nginx.conf 中的 http 指令域下;</li><li>某个具体 vhost.conf 配置下的 server 指令域下;</li><li>某个具体 server 指令下的 location 指令域下;</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">ngx_command_t</span>  ngx_http_gzip_filter_commands[] = &#123;</span><br><span class="line"></span><br><span class="line">    &#123; ngx_string(<span class="string">"gzip"</span>),</span><br><span class="line">      NGX_HTTP_MAIN_CONF|NGX_HTTP_SRV_CONF|NGX_HTTP_LOC_CONF|NGX_HTTP_LIF_CONF</span><br><span class="line">                        |NGX_CONF_FLAG,</span><br><span class="line">      ngx_conf_set_flag_slot,</span><br><span class="line">      NGX_HTTP_LOC_CONF_OFFSET,</span><br><span class="line">      offsetof(<span class="keyword">ngx_http_gzip_conf_t</span>, enable),</span><br><span class="line">      <span class="literal">NULL</span> &#125;,</span><br><span class="line">      </span><br><span class="line">    &#123; ngx_string(<span class="string">"gzip_buffers"</span>),</span><br><span class="line">      NGX_HTTP_MAIN_CONF|NGX_HTTP_SRV_CONF|NGX_HTTP_LOC_CONF|NGX_CONF_TAKE2,</span><br><span class="line">      ngx_conf_set_bufs_slot,</span><br><span class="line">      NGX_HTTP_LOC_CONF_OFFSET,</span><br><span class="line">      offsetof(<span class="keyword">ngx_http_gzip_conf_t</span>, bufs),</span><br><span class="line">      <span class="literal">NULL</span> &#125;,</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">      ngx_null_command</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>以上代码片段列举了 <code>gzip</code> 指令与 <code>gzip_buffers</code>, 其余的指令与 <code>gzip_buffers</code> 在使用上下文设置上基本相同, 都是 <code>NGX_HTTP_MAIN_CONF|NGX_HTTP_SRV_CONF|NGX_HTTP_LOC_CONF</code>;<br>从源码中可以看出, 在 gzip 模块里, <code>gzip</code> 指令相比其他指令有一个特别的地方: 除了 http, server, location 之外, 还有一个地方可以使用 <code>gzip</code> 指令, <code>NGX_HTTP_LIF_CONF</code>, 即 location 指令域中的 if 配置下;</p><p>以下是一个典型完整的 gzip 配置:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启/关闭, 默认 off</span></span><br><span class="line">gzip on | off;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当 response header 中包含 Via 头信息时, 根据 request header 中某些头信息决定是否需要开启 gzip, 默认 off</span></span><br><span class="line">gzip_proxied off | expired | no-cache | no-store | private | no_last_modified | no_etag | auth | any;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据 User-Agent 的值匹配, 针对部分请求不使用 gzip, 比如老旧的 IE6</span></span><br><span class="line">gzip_disable <span class="string">"msie6"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 response header 中添加 Vary: Accept-Encoding, 告诉 cache/cdn 同时缓存 压缩与非压缩两种版本的 response, 默认 off</span></span><br><span class="line">gzip_vary on | off;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 gzip 的最小 size, size 值取决于 Content-length 的值, 默认 20 bytes</span></span><br><span class="line">gzip_min_length 1k;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于 gzip 压缩缓冲区的 num 与 size</span></span><br><span class="line"><span class="comment"># 建议 num 为 cpu 核心数, size 为 cpu cache page 大小</span></span><br><span class="line">gzip_buffers 32 8k;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 支持 gzip 模块的最低 http 版本, 默认 1.1</span></span><br><span class="line">gzip_http_version 1.0;</span><br><span class="line"></span><br><span class="line"><span class="comment"># gzip 压缩级别, [1-9], 默认 1, 级别越高, 压缩率越高, 同时消耗的 cpu 资源越高</span></span><br><span class="line">gzip_comp_level 1;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对哪些 Content-type 使用 gzip, 默认是 text/html</span></span><br><span class="line"><span class="comment"># text/html 不需要设置到 gzip_types 中, 在其他条件满足时, text/html 会自动被压缩</span></span><br><span class="line"><span class="comment"># 若设置了 text/html 反而会输出 warn </span></span><br><span class="line">gzip_types text/css application/javascript application/json;</span><br></pre></td></tr></table></figure></p><h3 id="gzip-模块实践中遇到的坑"><a href="#gzip-模块实践中遇到的坑" class="headerlink" title="gzip 模块实践中遇到的坑"></a><strong>gzip 模块实践中遇到的坑</strong></h3><p><strong>(1) gzip_comp_level 级别的选择</strong><br>在 <a href="https://serverfault.com/questions/253074/what-is-the-best-nginx-compression-gzip-level" target="_blank" rel="noopener">stackoverflow 上的一个问题</a> 里曾讨论到, gzip_comp_level 压缩级别, 虽然其越高压缩率越高, 但是压缩边际提升率在 level = 1 之后却是在下降的:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 针对 text/html</span></span><br><span class="line">0    55.38 KiB (100.00% of original size)</span><br><span class="line">1    11.22 KiB ( 20.26% of original size)</span><br><span class="line">2    10.89 KiB ( 19.66% of original size)</span><br><span class="line">3    10.60 KiB ( 19.14% of original size)</span><br><span class="line">4    10.17 KiB ( 18.36% of original size)</span><br><span class="line">5     9.79 KiB ( 17.68% of original size)</span><br><span class="line">6     9.62 KiB ( 17.37% of original size)</span><br><span class="line">7     9.50 KiB ( 17.15% of original size)</span><br><span class="line">8     9.45 KiB ( 17.06% of original size)</span><br><span class="line">9     9.44 KiB ( 17.05% of original size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对 application/x-javascript</span></span><br><span class="line">0    261.46 KiB (100.00% of original size)</span><br><span class="line">1     95.01 KiB ( 36.34% of original size)</span><br><span class="line">2     90.60 KiB ( 34.65% of original size)</span><br><span class="line">3     87.16 KiB ( 33.36% of original size)</span><br><span class="line">4     81.89 KiB ( 31.32% of original size)</span><br><span class="line">5     79.33 KiB ( 30.34% of original size)</span><br><span class="line">6     78.04 KiB ( 29.85% of original size)</span><br><span class="line">7     77.85 KiB ( 29.78% of original size)</span><br><span class="line">8     77.74 KiB ( 29.73% of original size)</span><br><span class="line">9     77.75 KiB ( 29.74% of original size)</span><br></pre></td></tr></table></figure></p><p>随着压缩级别的提高, 更高的 cpu 消耗却换不来有效的压缩提升效率; 所以 gzip_comp_level 的最佳实践是将其设为 1, 便足够了;<br>&nbsp;</p><p><strong>(2) gzip_min_length 的陷阱</strong><br>一般经验上, 我们会将 gzip_min_length 设置为 1KB, 以防止 response size 太小, 压缩后反而变大;<br>只是, ngx_http_gzip_module 取决于 response headers 里的 Content-length; 如果 response 里面有这个 header, 那没有任何问题, 但是如果 response 里面没这个 header, gzip_min_length 设置就失效了;<br>这种情况其实并不少见: <code>Transfer-Encoding: chunked</code>; </p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="http://nginx.org/en/docs/http/ngx_http_gzip_module.html" target="_blank" rel="noopener">Module ngx_http_gzip_module</a></li><li><a href="http://www.jianshu.com/p/af6304f7cbd6" target="_blank" rel="noopener">nginx の gzip 使用</a></li><li><a href="https://www.darrenfang.com/2015/01/setting-up-http-cache-and-gzip-with-nginx/" target="_blank" rel="noopener">加速nginx: 开启gzip和缓存</a></li><li><a href="http://www.webkaka.com/blog/archives/how-to-set-Vary-Accept-Encoding-header.html" target="_blank" rel="noopener">标头 “Vary:Accept-Encoding” 指定方法及其重要性分析</a></li><li><a href="https://serverfault.com/questions/253074/what-is-the-best-nginx-compression-gzip-level" target="_blank" rel="noopener">What is the best nginx compression gzip level</a></li><li><a href="https://segmentfault.com/q/1010000002686639" target="_blank" rel="noopener">关于 nginx 配置文件 gzip 的配置问题: 不太明白这个 gzip_proxied 的作用是什么, 应该如何正确配置</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ngx_http_gzip_module 是十分有用的 nginx 模块, 其有效压缩了 http 请求大小, 节省了流量, 加快了传输速度, 提升了用户体验;&lt;br&gt;当然, 其在使用上也有一些坑, 本文将具体讨论一下相关内容;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="nginx" scheme="http://zshell.cc/categories/nginx/"/>
    
      <category term="module" scheme="http://zshell.cc/categories/nginx/module/"/>
    
    
      <category term="nginx" scheme="http://zshell.cc/tags/nginx/"/>
    
      <category term="nginx:module" scheme="http://zshell.cc/tags/nginx-module/"/>
    
  </entry>
  
  <entry>
    <title>sysvinit / systemd 命令使用与对比</title>
    <link href="http://zshell.cc/2017/11/12/linux-init--sysvinit_systemd%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%AF%B9%E6%AF%94/"/>
    <id>http://zshell.cc/2017/11/12/linux-init--sysvinit_systemd命令使用与对比/</id>
    <published>2017-11-12T09:18:06.000Z</published>
    <updated>2018-04-23T15:34:44.269Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>当用户空间引导程序 systemV init 被 systemd 所取代, centos 7 下操纵与查看 daemon service 的命令也随之而改变;<br>不过, 由于 systemd 的庞大复杂, 命令选项繁多, 本文对 systemd 的整理主要集中于与 sysvinit 所提供的功能重合度最高的 systemctl 命令;</p></blockquote><a id="more"></a><hr><h2 id="传统的-sysvinit-相关命令"><a href="#传统的-sysvinit-相关命令" class="headerlink" title="传统的 sysvinit 相关命令"></a><strong>传统的 sysvinit 相关命令</strong></h2><p>与传统的 systemV init 引导程序相匹配的 daemon service 操纵命令主要是 service 与 chkconfig/ntsysv;<br>其中:<br>service 命令用于 启, 停, 查看 具体的 daemon service;<br>chkconfig 命令用于 修改, 查看 具体 daemon service 的 runlevel 及启停信息;<br>ntsysv 命令提供了一个 GUI 界面用于操纵各个 runlevel 上各 daemon service 的启停;</p><h3 id="service-的使用方式"><a href="#service-的使用方式" class="headerlink" title="service 的使用方式"></a><strong>service 的使用方式</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动, 停止, 重启, 查看</span></span><br><span class="line">sudo service ntpd start</span><br><span class="line">sudo service ntpd stop</span><br><span class="line">sudo service ntpd restart</span><br><span class="line">sudo service ntpd status</span><br></pre></td></tr></table></figure><h3 id="chkconfig-的使用方式"><a href="#chkconfig-的使用方式" class="headerlink" title="chkconfig 的使用方式"></a><strong>chkconfig 的使用方式</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列举所有的 daemon service 在各个 runlevel 上的启停状态</span></span><br><span class="line">sudo chkconfig --list</span><br><span class="line"><span class="comment"># 列举指定的 daemon service 在各个 runlevel 上的启停状态</span></span><br><span class="line">&gt; sudo chkconfig --list ntpd</span><br><span class="line">ntpd           0:on1:on2:on3:on4:on5:on6:off</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一个 daemon service</span></span><br><span class="line">sudo chkconfig --add mysqld</span><br><span class="line"><span class="comment"># 在默认的 2, 3, 4, 5 四个 runlevel 上自动启动 mysqld</span></span><br><span class="line">sudo chkconfig mysqld on</span><br><span class="line"><span class="comment"># 在指定的 3, 5 两个 runlevel 上自动启动 mysqld</span></span><br><span class="line">sudo chkconfig --level 35 mysqld on</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除一个 daemon service</span></span><br><span class="line">sudo chkconfig --del rngd</span><br></pre></td></tr></table></figure><h3 id="ntsysv-的使用方式"><a href="#ntsysv-的使用方式" class="headerlink" title="ntsysv 的使用方式"></a><strong>ntsysv 的使用方式</strong></h3><p>ntsysv 在 centos 7 之前的各发行版本上都默认安装, 不过从 centos 7 之后, 该命令的 GUI 形式已经不再默认提供, 只提供了 chkconfig 命令用于兼容照顾老的 systemV init 方式;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认情况下 ntsysv 配置的是当前 user session 所在的 runlevel</span></span><br><span class="line">sudo ntsysv</span><br><span class="line"><span class="comment"># 配置 runlevel = 5 的 daemon service</span></span><br><span class="line">sudo ntsysv --level 5</span><br></pre></td></tr></table></figure></p><h2 id="主流的-systemd-相关命令"><a href="#主流的-systemd-相关命令" class="headerlink" title="主流的 systemd 相关命令"></a><strong>主流的 systemd 相关命令</strong></h2><p>systemd 相比 sysvinit 就要复杂多了, 同时也比 sysvinit 强大多了;<br>systemd 相比 sysvinit 更强大的其中一个重要点是, systemd 不仅仅管理系统中的各进程, 它管理 linux 系统中的所有资源, 并把不同的资源称为 unit:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">service unit: 系统服务</span><br><span class="line">target unit: 多个 unit 构成的一个组</span><br><span class="line">device unit: 硬件设备</span><br><span class="line">mount unit: 文件系统的挂载点</span><br><span class="line">automount unit: 自动挂载点</span><br><span class="line">path unit: 文件或路径</span><br><span class="line">scope unit: 不是由 systemd 启动的外部进程</span><br><span class="line">slice unit: 进程组</span><br><span class="line">snapshot unit: systemd 快照, 可以切回某个快照</span><br><span class="line">socket unit: 进程间通信的 socket</span><br><span class="line">swap unit: swap 文件</span><br><span class="line">timer unit: 定时器, 可与 crond 相对比, 可圈可点</span><br></pre></td></tr></table></figure></p><p>其中, <strong>service unit</strong> 在 12 类 unit 中是最主要的一类, 也是日常操作中最频繁接触的一类, 当然也是与传统的 sysvinit 可以直接比较的对象;<br>另外, systemd 里另外一个比较有意思的是 timer unit, 关于此的详细内容可以参见: <a href="">systemd 的定时器功能</a>;<br>&nbsp;<br>systemd 主要涉及到的命令有: <code>systemctl</code>, <code>hostnamectl</code>, <code>localectl</code>, <code>timedatectl</code>, <code>loginctl</code>, <code>journalctl</code> 等, 其中:</p><ul><li><code>systemctl</code> 是最重要的命令, 最核心的操作都与此命令有关, 比如启停服务, 管理各 unit 等;</li><li><code>hostnamectl</code> 用于管理主机信息等;</li><li><code>localectl</code> 用于本地化设置管理;</li><li><code>timedatectl</code> 用于时区管理;</li><li><code>loginctl</code> 用于管理当前登录的用户;</li><li><code>journalctl</code> 用于管理 systemd 与各 unit 的输出日志, 用于辅助其余命令查看状态与日志;</li></ul><p>&nbsp;<br>本主要整理与 systemctl 有关的内容, 其余的如 timedatectl, journalctl 请参见另一篇文章: <a href="">sysvinit / systemd 日志系统的使用与对比</a>;</p><h3 id="systemctl-的常用命令列表"><a href="#systemctl-的常用命令列表" class="headerlink" title="systemctl 的常用命令列表"></a><strong>systemctl 的常用命令列表</strong></h3><p>systemctl 的使用场景十分广泛, 从大的角度来说, 可以分为 <strong>系统管理</strong> 和 <strong>unit 管理</strong> 两大类;<br>系统管理类的命令如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重启系统</span></span><br><span class="line">sudo systemctl reboot</span><br><span class="line"><span class="comment"># 关闭系统, 切断电源</span></span><br><span class="line">sudo systemctl poweroff</span><br><span class="line"><span class="comment"># CPU 停止工作</span></span><br><span class="line">sudo systemctl halt</span><br><span class="line"><span class="comment"># 暂停系统</span></span><br><span class="line">sudo systemctl <span class="built_in">suspend</span></span><br><span class="line"><span class="comment"># 让系统进入冬眠状态</span></span><br><span class="line">sudo systemctl hibernate</span><br><span class="line"><span class="comment"># 让系统进入交互式休眠状态</span></span><br><span class="line">sudo systemctl hybrid-sleep</span><br><span class="line"><span class="comment"># 启动进入救援状态 (单用户状态, runlevel = 1)</span></span><br><span class="line">sudo systemctl rescue</span><br></pre></td></tr></table></figure></p><p>unit 管理类 的命令种类繁多, 大致可以再细分为两小类: <strong>查看管理类</strong> 与 <strong>操纵动作类</strong>;<br><strong>查看管理类仅仅是统计与查看, 并不改变 unit 的状态:</strong><br>(1) 从整体角度管理 units<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认列出正在运行的 unit</span></span><br><span class="line">sudo systemctl list-units</span><br><span class="line"><span class="comment"># 列出所有 unit, 包括没有找到配置文件的或者启动失败的</span></span><br><span class="line">sudo systemctl list-units --all</span><br><span class="line"><span class="comment"># 列出所有没有运行的 unit</span></span><br><span class="line">sudo systemctl list-units --all --state=inactive</span><br><span class="line"><span class="comment"># 列出所有加载失败的 unit</span></span><br><span class="line">sudo systemctl list-units --failed</span><br><span class="line"><span class="comment"># 列出所有正在运行的, 类型为 service 的 unit; -t: --type</span></span><br><span class="line">sudo systemctl list-units --<span class="built_in">type</span>=service</span><br></pre></td></tr></table></figure></p><p>(2) 管理具体的某个 unit<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示某个 unit 的状态</span></span><br><span class="line">sudo systemctl status rsyslog.service</span><br><span class="line"><span class="comment"># 显示某个 unit 是否正在运行</span></span><br><span class="line">sudo systemctl is-active rsyslog.service</span><br><span class="line"><span class="comment"># 显示某个 unit 是否处于启动失败状态</span></span><br><span class="line">sudo systemctl is-failed rsyslog.service</span><br><span class="line"><span class="comment"># 显示某个 unit 服务是否建立了启动链接</span></span><br><span class="line">sudo systemctl is-enabled rsyslog.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示某个 unit 的启动是否依赖其他 unit 的启动, --all 展开所有 target unit 下的每一个详细 unit</span></span><br><span class="line">sudo systemctl list-dependencies --all rsyslog.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示某个 unit 的所有底层参数</span></span><br><span class="line">sudo systemctl show rsyslog.service</span><br><span class="line"><span class="comment"># 显示某个 unit 的指定属性的值</span></span><br><span class="line">sudo systemctl show -p CPUShares rsyslog.service</span><br></pre></td></tr></table></figure></p><p><strong>操纵动作类, 主要是针对 service unit:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置为开机启动</span></span><br><span class="line">sudo systemctl <span class="built_in">enable</span> nginx.service</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">sudo systemctl start nginx.service</span><br><span class="line"><span class="comment"># 停止</span></span><br><span class="line">sudo systemctl stop nginx.service</span><br><span class="line"><span class="comment"># 重启</span></span><br><span class="line">sudo systemctl restart nginx.service</span><br><span class="line"><span class="comment"># 杀死一个服务的所有子进程</span></span><br><span class="line">sudo systemctl <span class="built_in">kill</span> nginx.service</span><br><span class="line"><span class="comment"># 重新加载一个服务的配置文件</span></span><br><span class="line">sudo systemctl reload nginx.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置某个 unit 的指定属性</span></span><br><span class="line">sudo systemctl <span class="built_in">set</span>-property nginx.service CPUShares=500</span><br></pre></td></tr></table></figure></p><h3 id="systemctl-的状态与诊断"><a href="#systemctl-的状态与诊断" class="headerlink" title="systemctl 的状态与诊断"></a><strong>systemctl 的状态与诊断</strong></h3><p>使用 systemctl status 输出服务详情状态:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loaded:   该 unit 配置文件的位置以及是否开机启动</span></span><br><span class="line"><span class="comment"># Active:   运行状态</span></span><br><span class="line"><span class="comment"># Main PID: 父进程 pid</span></span><br><span class="line"><span class="comment"># CGroup:   所有的子进程列表</span></span><br><span class="line"><span class="comment"># 最后是 service 的日志</span></span><br><span class="line">&gt; sudo systemctl status rsyslog.service</span><br><span class="line"></span><br><span class="line">● rsyslog.service - System Logging Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Wed 2017-07-19 16:01:19 CST; 6 months 10 days ago</span><br><span class="line"> Main PID: 504 (rsyslogd)</span><br><span class="line">   CGroup: /system.slice/rsyslog.service</span><br><span class="line">           └─504 /usr/sbin/rsyslogd -n</span><br><span class="line"></span><br><span class="line">Jul 19 16:01:19 localhost.localdomain systemd[1]: Starting System Logging Service...</span><br><span class="line">Jul 19 16:01:19 localhost.localdomain systemd[1]: Started System Logging Service.</span><br></pre></td></tr></table></figure></p><p>使用 journalctl 查看日志:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定查看某个 unit 的日志</span></span><br><span class="line">sudo journalctl -u nagios</span><br><span class="line"><span class="comment"># 指定时间范围 --since=  --until=</span></span><br><span class="line">sudo journalctl -u nagios -S <span class="string">"2017-04-19 09:00:00"</span></span><br><span class="line">sudo journalctl -u nagios -S <span class="string">"2 days ago"</span></span><br><span class="line">sudo journalctl -u nagios -U <span class="string">"2017-12-31 23:59:59"</span></span><br><span class="line"><span class="comment"># 指定某次启动后的所有日志</span></span><br><span class="line">sudo journalctl -u nagios -b [-0] <span class="comment"># 当前启动后</span></span><br><span class="line">sudo journalctl -u nagios -b  -1  <span class="comment"># 上次启动后</span></span><br><span class="line">sudo journalctl -u nagios -b  -2  <span class="comment"># 继续往上追溯</span></span><br></pre></td></tr></table></figure></p><p>关于 journalctl 的详细内容, 请参见另外一篇文章: <a href="">sysvinit / systemd 日志系统的使用与对比</a>;</p><h2 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h2><ul><li><a href="">sysvinit/systemd/upstart 初始化过程梳理</a></li><li><a href="">systemd 的定时器功能</a></li><li><a href="">systemd 相关配置文件整理</a></li><li><a href="">sysvinit / systemd 日志系统的使用与对比</a></li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="https://www.cnblogs.com/panjun-Donet/archive/2010/08/10/1796873.html" target="_blank" rel="noopener">Linux下chkconfig命令详解</a></li><li><a href="http://man.linuxde.net/ntsysv" target="_blank" rel="noopener">ntsysv命令</a></li><li><a href="https://zhangzifan.com/centos-systemctl.html" target="_blank" rel="noopener">CentOS 7 启动, 重启, chkconfig 等命令已经合并为 systemctl</a></li><li><a href="http://blog.csdn.net/catoop/article/details/47318225" target="_blank" rel="noopener">RHEL 7 中 systemctl 的用法 (替代service 和 chkconfig)</a></li><li><a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html" target="_blank" rel="noopener">Systemd 入门教程: 命令篇</a></li><li><a href="https://linux.cn/article-5926-1.html" target="_blank" rel="noopener">systemctl 命令完全指南</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;当用户空间引导程序 systemV init 被 systemd 所取代, centos 7 下操纵与查看 daemon service 的命令也随之而改变;&lt;br&gt;不过, 由于 systemd 的庞大复杂, 命令选项繁多, 本文对 systemd 的整理主要集中于与 sysvinit 所提供的功能重合度最高的 systemctl 命令;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://zshell.cc/categories/linux/"/>
    
      <category term="init" scheme="http://zshell.cc/categories/linux/init/"/>
    
    
      <category term="linux:init" scheme="http://zshell.cc/tags/linux-init/"/>
    
      <category term="systemd" scheme="http://zshell.cc/tags/systemd/"/>
    
  </entry>
  
  <entry>
    <title>bash 结束死循环的方法</title>
    <link href="http://zshell.cc/2017/11/05/linux-shell--bash%E7%BB%93%E6%9D%9F%E6%AD%BB%E5%BE%AA%E7%8E%AF%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://zshell.cc/2017/11/05/linux-shell--bash结束死循环的方法/</id>
    <published>2017-11-04T16:00:00.000Z</published>
    <updated>2017-12-07T16:12:56.677Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>linux 中有很多实用的工具, 采用了这样一种工作方式:<br>定时执行(1/s, 1/3s 等)一次指定逻辑, 当用户按下 ctrl + c 发出 SIGINT 信号时, 结束进程; 如果接收不到 SIGINT/SIGTERM 等信号, 进程则会一直执行下去;<br>类似的工具包括 ioutil, jmap 等;<br>本文整理了实现上述逻辑的一些典型方法;</p></blockquote><a id="more"></a><hr><p>一次偶然的机会, 我不小心写了一个 bash 脚本, 在一个 while 1 循环里调用一个命令; 结果执行的时候发现, 我按下 ctrl + c, 只结束了循环内的命令, 但结束不了 while 循环本身, 造成了该脚本停不下来了, 最后不得不打开另一个终端 kill 掉它;<br>这个事情突然引起了我的兴趣, 于是我总结了一下 bash 结束 while 1 死循环的几种方法;</p><h3 id="方法1-监听命令返回值"><a href="#方法1-监听命令返回值" class="headerlink" title="方法1: 监听命令返回值"></a><strong>方法1: 监听命令返回值</strong></h3><p>根据 <a href="http://www.gnu.org/software/bash/manual/bashref.html#Exit-Status" target="_blank" rel="noopener">GNU 相关规范</a>, 如果一个进程是由于响应信号 signal 而终止, 其返回码必须是 128 + signal_number;<br>那么, 可以通过判断其返回码 $? 是否大于 128 而判断 COMMAND 是否响应了信号;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while [ 1 ]; do</span><br><span class="line">    COMMAND</span><br><span class="line">    test $? -gt 128 &amp;&amp; break</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>更精确的, 如果只想判断 COMMAND 是否响应了 SIGINT 信号, 可以直接判断:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># SIGINT = 2, 128 + SIGINT = 130</span><br><span class="line">test $? -eq 130 &amp;&amp; break</span><br></pre></td></tr></table></figure></p><p>特殊的情况下, COMMAND 忽略了 SIGINT 信号, 可以使用 -e 选项强制其响应 SIGINT 信号:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while [ 1 ]; do</span><br><span class="line">    COMMAND -e</span><br><span class="line">    test $? -gt 128 &amp;&amp; break</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><h3 id="方法2-命令返回值短路"><a href="#方法2-命令返回值短路" class="headerlink" title="方法2: 命令返回值短路"></a><strong>方法2: 命令返回值短路</strong></h3><p>方法2 是方法1 的简化版本:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while [ 1 ]; do</span><br><span class="line">    COMMAND -e || break</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>其本质是监听 COMMAND 的返回值 $? 是否为 0, 如果是 0, 那么 break 中断命令就被短路了; 如果是非 0, 便会执行 break, 跳出死循环;<br>这种方法巧妙得使用 || 逻辑运算符简化了代码, 但是有一个缺陷: 当 COMMAND 并非因为响应 ctrl + c 而是其他错误返回了非 0 的状态时, 循环也会结束;<br>这是方法2 相比 方法1 略显不精准的地方;</p><h3 id="方法3-使用-trap-捕获信号"><a href="#方法3-使用-trap-捕获信号" class="headerlink" title="方法3: 使用 trap 捕获信号"></a><strong>方法3: 使用 trap 捕获信号</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 捕获到 SIGINT 即 exit 0 正常退出</span><br><span class="line">trap &quot;exit 0&quot; SIGINT</span><br><span class="line">while [ 1 ]; do</span><br><span class="line">    COMMAND -e</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="方法4-使用-ctrl-z-配合-SIGTERM-信号"><a href="#方法4-使用-ctrl-z-配合-SIGTERM-信号" class="headerlink" title="方法4: 使用 ctrl + z 配合 SIGTERM 信号"></a><strong>方法4: 使用 ctrl + z 配合 SIGTERM 信号</strong></h3><p>当命令运行在前台, 使用 ctrl + z 挂起进程, 会得到以下输出:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ^Z</span></span><br><span class="line">[1]+  Stopped                 COMMAND</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 是挂起进程的作业号(job number), kill [job_number] 会向该作业发送 SIGTERM 信号</span></span><br><span class="line"><span class="built_in">kill</span> %1</span><br><span class="line"><span class="comment"># 发送 SIGTERM 信号给最近一次被挂起的进程</span></span><br><span class="line"><span class="built_in">kill</span> %%</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行的结果</span></span><br><span class="line">[1]+ Terminated               COMMAND</span><br></pre></td></tr></table></figure></p><h3 id="方法5-使用-e-选项"><a href="#方法5-使用-e-选项" class="headerlink" title="方法5: 使用 -e 选项"></a><strong>方法5: 使用 -e 选项</strong></h3><p>使用 set -e, 开启命令返回码校验功能, 一旦 COMMAND 返回非 0, 立即结束进程;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">set -e</span><br><span class="line">while [ 1 ]; do</span><br><span class="line">    COMMAND -e</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>或者作为 bash 的参数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash -e</span><br><span class="line">while [ 1 ]; do</span><br><span class="line">    COMMAND -e</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="https://unix.stackexchange.com/questions/42287/terminating-an-infinite-loop" target="_blank" rel="noopener">Terminating an infinite loop</a></li><li><a href="http://www.gnu.org/software/bash/manual/bashref.html#Exit-Status" target="_blank" rel="noopener">3.7.5 Exit Status</a></li><li><a href="https://unix.stackexchange.com/questions/48425/how-to-stop-the-loop-bash-script-in-terminal/48465#48465" target="_blank" rel="noopener">How to stop the loop bash script in terminal</a></li><li><a href="http://blog.csdn.net/todd911/article/details/9954961" target="_blank" rel="noopener">Unix/Linux 脚本中 “set -e” 的作用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;linux 中有很多实用的工具, 采用了这样一种工作方式:&lt;br&gt;定时执行(1/s, 1/3s 等)一次指定逻辑, 当用户按下 ctrl + c 发出 SIGINT 信号时, 结束进程; 如果接收不到 SIGINT/SIGTERM 等信号, 进程则会一直执行下去;&lt;br&gt;类似的工具包括 ioutil, jmap 等;&lt;br&gt;本文整理了实现上述逻辑的一些典型方法;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://zshell.cc/categories/linux/"/>
    
      <category term="shell" scheme="http://zshell.cc/categories/linux/shell/"/>
    
    
      <category term="linux:shell" scheme="http://zshell.cc/tags/linux-shell/"/>
    
  </entry>
  
  <entry>
    <title>ulimit 调参与优化</title>
    <link href="http://zshell.cc/2017/10/28/linux-conf--ulimit%E8%B0%83%E5%8F%82%E4%B8%8E%E4%BC%98%E5%8C%96/"/>
    <id>http://zshell.cc/2017/10/28/linux-conf--ulimit调参与优化/</id>
    <published>2017-10-28T15:23:05.000Z</published>
    <updated>2018-01-27T14:53:07.553Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ulimit 未正确设置是很多线上故障的根源:<br><code>Too many open files</code>;<br><code>java.lang.OutOfMemoryError: unable to create new native thread</code>;<br>对于生产环境来说, ulimit 的调参优化至关重要;<br>本文详细介绍并梳理一下与 ulimit 相关的林林总总;</p></blockquote><a id="more"></a><hr><p>ulimit 是 linux 对于每个通过 PAM 登录的用户 ( 每个进程 ) 的资源最大使用限制的设置;<br>注意, 这里仅仅对通过 PAM 登陆的用户起作用, 而对于那些随系统启动而启动的 daemon service, ulimit 是不会去限制其资源使用的;<br>在 <code>/etc/security/limits.conf</code> 文件中的第一段注释如下:</p><blockquote><p>This file sets the resource limits for the users logged in via PAM.<br>It does not affect resource limits of the system services.</p></blockquote><p>关于 linux PAM 相关的内容, 可以前往另外一篇文章: <a href="">pam 认证与配置</a>;</p><h2 id="ulimit-基本信息"><a href="#ulimit-基本信息" class="headerlink" title="ulimit 基本信息"></a><strong>ulimit 基本信息</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有 ulimit 设置</span></span><br><span class="line">&gt; <span class="built_in">ulimit</span> -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 15018</span><br><span class="line">max locked memory       (kbytes, -l) 64             <span class="comment"># 每个进程可以锁住而不被 swap 出去的内存</span></span><br><span class="line">max memory size         (kbytes, -m) unlimited      <span class="comment"># 每个进程可使用的最大内存大小</span></span><br><span class="line">open files                      (-n) 1024           <span class="comment"># 每个进程可打开的文件数</span></span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192           <span class="comment"># 每个进程可使用的最大堆栈大小</span></span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 4096           <span class="comment"># 每个用户的最大进程数</span></span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br></pre></td></tr></table></figure><h2 id="ulimit-需要优化的场景及待优化参数"><a href="#ulimit-需要优化的场景及待优化参数" class="headerlink" title="ulimit 需要优化的场景及待优化参数"></a><strong>ulimit 需要优化的场景及待优化参数</strong></h2><p>linux 默认的 ulimit 限制, 是出于安全考虑, 设置的有些保守; 实际的生产环境下, 往往需要对其作出适当的调整, 方可发挥机器的最大性能;</p><h3 id="场景1-tomcat-web-容器"><a href="#场景1-tomcat-web-容器" class="headerlink" title="场景1: tomcat web 容器 "></a><strong>场景1: tomcat web 容器 </strong></h3><p>一台 4C4G60G 的标准虚拟主机, 其上部署了一个 tomcat 实例, 启动 catalina 进程的是 tomcat:tomcat 用户;<br>如果该服务是一个网络 IO 密集的应用, 需要打开的 socket file 远不止 1024, ulimit 设置的 max open files 就会限制其性能; 另外, 该主机只部署了这一个服务, tomcat 用户是唯一一个需要占用大量资源的用户, ulimit 对单个用户的限制便会造成机器资源闲置, 极低的使用率, 降低 web 服务的性能;<br>所以, 可以对该机器的 ulimit 作出如下调整:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. max memory size -&gt; <span class="built_in">unlimit</span></span><br><span class="line">2. open files -&gt; 65536</span><br><span class="line">3. stack size -&gt; <span class="built_in">unlimit</span></span><br></pre></td></tr></table></figure></p><p>另外, 我们还遇到一种特殊的情况, 用标准配置虚拟机跑 dubbo 的服务治理: 当时发现, 如果服务注册到 zookeeper 的数量达到一定级别, 线上就会报 <code>java.lang.OutOfMemoryError: unable to create new native thread</code> 的异常;<br>最后确定问题的原因是 <code>ulimit -u</code> max user processes 的数量配置过低, 增大后解决问题:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4. max user processes -&gt; 65535</span><br></pre></td></tr></table></figure></p><p>具体的情况可以参见这篇文章: <a href="">dubbo 服务治理系统设计</a>;</p><h3 id="场景2-elasticsearch-data-node"><a href="#场景2-elasticsearch-data-node" class="headerlink" title="场景2: elasticsearch data node"></a><strong>场景2: elasticsearch data node</strong></h3><p>32C64G4T 的配置, 为确保指针压缩特性被打开, 一般我们都会控制 jvm 的最大堆内存与最小堆内存: ‘-Xmx30g -Xms30g’, 并希望能锁住所有的内存, 避免堆内存被 swap 到磁盘, 降低了搜索性能; 这种场景下我们当然不希望 ulimit 限制了 max memory size 以及 max locked memory;<br>所以, 可以对该机器的 ulimit 作出如下调整:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. max locked memory -&gt; unlimit</span><br><span class="line">2. max memory size -&gt; unlimit</span><br><span class="line">3. open files -&gt; 65536</span><br><span class="line">4. stack size -&gt; unlimit</span><br></pre></td></tr></table></figure></p><p>对于 max locked memory, elasticsearch.yml 本身有一个配置项 <code>bootstrap.mlockall</code>/<code>bootstrap.memory_lock</code> = true, 其背后实现就是通过类似于 ulimit -l unlimit 的方法完成的; 只是, elasticsearch 试图自己主动改变该配置能生效的前提, 是 ulimit 配置文件里要允许其这样设置, 具体的逻辑请看本文下下节: <a href="#ulimit-的永久修改">ulimit 的永久修改</a>;</p><p>&nbsp;<br>另外, 还有其他的一些场景, 可能需要调整其他参数以作优化, 此处不一而论;<br>以上是需要调整 ulimit 参数的场景举例, 下面的内容是关于如何 临时/永久 修改 ulimit 设置;</p><h2 id="ulimit-当前-session-下的临时修改"><a href="#ulimit-当前-session-下的临时修改" class="headerlink" title="ulimit 当前 session 下的临时修改"></a><strong>ulimit 当前 session 下的临时修改</strong></h2><p>ulimit 的临时调整, 只对当前 session 下的当前用户, 以及当前用户所起的进程生效;<br>其调整方法也已经在 <code>ulimit -a</code> 中被注明了:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># max locked mem</span></span><br><span class="line"><span class="built_in">ulimit</span> -l <span class="built_in">unlimit</span></span><br><span class="line"><span class="comment"># max mem size</span></span><br><span class="line"><span class="built_in">ulimit</span> -m <span class="built_in">unlimit</span></span><br><span class="line"><span class="comment"># open files</span></span><br><span class="line"><span class="built_in">ulimit</span> -n 65536</span><br><span class="line"><span class="comment"># max user processes</span></span><br><span class="line"><span class="built_in">ulimit</span> -u 65536</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h2 id="ulimit-的永久修改"><a href="#ulimit-的永久修改" class="headerlink" title="ulimit 的永久修改"></a><strong>ulimit 的永久修改</strong></h2><p>上一节的方法, 只能在当前 session 下对当前用户作临时调整, 而 要想对 ulimit 作永久调整, 需要修改一些配置文件:</p><ol><li><code>/etc/security/limits.conf</code>;</li><li><code>/etc/security/limits.d 目录</code>;</li></ol><p>这些文件用于持久化每个用户的资源限制设置;<br>其中, <code>/etc/security/limits.conf</code> 自不必说, 这是配置 ulimit 的主要文件:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">domain  限制的目标:</span><br><span class="line">        username    用户名;</span><br><span class="line">        @groupname  组名, 需加 <span class="string">'@'</span> 前缀;</span><br><span class="line">        *           通配所有用户/组;</span><br><span class="line">        %groupname  这种写法只能用于限制 某个 group 的 maxlogin <span class="built_in">limit</span>, 即最大登陆用户数限制;</span><br><span class="line">        </span><br><span class="line"><span class="built_in">type</span>    限制的属性:</span><br><span class="line">        `soft` 对 domain 给出的用户设置默认值; </span><br><span class="line">        `hard` 限制 domain 给出的用户自己所能设置的最大值; </span><br><span class="line">        `-` 将 soft 与 hard 都设为相同的值;</span><br><span class="line">        </span><br><span class="line">item    限制的资源类型, 与 <span class="built_in">ulimit</span> 所限制的资源类型大致相同:</span><br><span class="line">        - core - limits the core file size (KB)</span><br><span class="line">        - data - max data size (KB)</span><br><span class="line">        - fsize - maximum filesize (KB)</span><br><span class="line">        - memlock - max locked-in-memory address space (KB)</span><br><span class="line">        - nofile - max number of open file descriptors</span><br><span class="line">        - rss - max resident <span class="built_in">set</span> size (KB)</span><br><span class="line">        - stack - max stack size (KB)</span><br><span class="line">        - cpu - max CPU time (MIN)</span><br><span class="line">        - nproc - max number of processes</span><br><span class="line">        - as - address space <span class="built_in">limit</span> (KB)</span><br><span class="line">        - maxlogins - max number of logins <span class="keyword">for</span> this user</span><br><span class="line">        - maxsyslogins - max number of logins on the system</span><br><span class="line">        - priority - the priority to run user process with</span><br><span class="line">        - locks - max number of file locks the user can hold</span><br><span class="line">        - sigpending - max number of pending signals</span><br><span class="line">        - msgqueue - max memory used by POSIX message queues (bytes)</span><br><span class="line">        - nice - max nice priority allowed to raise to values: [-20, 19]</span><br><span class="line">        - rtprio - max realtime priority</span><br><span class="line"></span><br><span class="line">value   限制的具体值;</span><br></pre></td></tr></table></figure></p><p>以下是一个具体的例子:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#&lt;domain&gt;        &lt;type&gt;     &lt;item&gt;     &lt;value&gt;</span></span><br><span class="line">*                 soft      nproc       65536</span><br><span class="line">*                 hard      nproc       65536</span><br><span class="line">*                 -         nofile      65536</span><br><span class="line">%guest            -         maxlogins   10</span><br><span class="line">elastic           -         memlock     <span class="built_in">unlimit</span></span><br><span class="line">@dev              hard      fsize       10737418240</span><br></pre></td></tr></table></figure></p><p>如上所示, 系统允许 elastic 用户的最大 memlock 为 unlimit, 如果这个值被设置为了一个比较小的值, 那么上上节 elasticsearch 试图将其改成 unlimit 便会失败;</p><p>&nbsp;<br>而对于 <code>/etc/security/limits.d</code> 目录的作用,  <code>/etc/security/limits.conf</code> 文件中的第二段与第三段有如下注释:</p><blockquote><p>Also note that configuration files in /etc/security/limits.d directory,<br>which are read in alphabetical order, override the settings in this<br>file in case the domain is the same or more specific.<br>&nbsp;<br>That means for example that setting a limit for wildcard domain here<br>can be overriden with a wildcard setting in a config file in the<br>subdirectory, but a user specific setting here can be overriden only<br>with a user specific setting in the subdirectory.</p></blockquote><p>也就是说, limits.conf 配置文件, 可以在用户级别上被 limits.d 目录下的配置文件覆盖;<br>举一个例子, 在 redhat/centos 各发行版本中, limits.d 目录下就有一个文件 <code>20-nproc.conf</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default limit for number of user's processes to prevent</span></span><br><span class="line"><span class="comment"># accidental fork bombs.</span></span><br><span class="line"><span class="comment"># See rhbz #432903 for reasoning.</span></span><br><span class="line">*          soft    nproc     4096</span><br><span class="line">root       soft    nproc     unlimited</span><br></pre></td></tr></table></figure></p><p>这里面对除了 root 用户之外的所有用户作了一个最大进程/线程数目的 soft 限制;<br>如果修改 limits.conf 文件:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*          hard    nproc     65535</span><br></pre></td></tr></table></figure></p><p>这时会发现, 除非自己试图 <code>ulimit -u</code> 修改 max processes, 否则这个值会依然被限制为 4096;<br>而要想将该值默认放到 65535, 就必须修改 <code>20-nproc.conf</code> 文件方才生效;</p><h3 id="永久修改生效的必要条件"><a href="#永久修改生效的必要条件" class="headerlink" title="永久修改生效的必要条件"></a><strong>永久修改生效的必要条件</strong></h3><h2 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h2><ul><li><a href="">pam 认证与配置</a></li><li><a href="">dubbo 服务治理系统设计</a></li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="http://www.cnblogs.com/zengkefu/p/5649407.html" target="_blank" rel="noopener">ulimit 命令详解</a></li><li><a href="http://blog.csdn.net/taijianyu/article/details/5976319" target="_blank" rel="noopener">linux /etc/security/limits.conf的相关说明</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ulimit 未正确设置是很多线上故障的根源:&lt;br&gt;&lt;code&gt;Too many open files&lt;/code&gt;;&lt;br&gt;&lt;code&gt;java.lang.OutOfMemoryError: unable to create new native thread&lt;/code&gt;;&lt;br&gt;对于生产环境来说, ulimit 的调参优化至关重要;&lt;br&gt;本文详细介绍并梳理一下与 ulimit 相关的林林总总;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://zshell.cc/categories/linux/"/>
    
      <category term="conf" scheme="http://zshell.cc/categories/linux/conf/"/>
    
    
      <category term="linux:conf" scheme="http://zshell.cc/tags/linux-conf/"/>
    
  </entry>
  
  <entry>
    <title>bash 数组与映射</title>
    <link href="http://zshell.cc/2017/10/22/linux-shell--bash%E6%95%B0%E7%BB%84%E4%B8%8E%E6%98%A0%E5%B0%84/"/>
    <id>http://zshell.cc/2017/10/22/linux-shell--bash数组与映射/</id>
    <published>2017-10-22T15:32:19.000Z</published>
    <updated>2018-01-03T15:18:11.226Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>注: bash 映射 (map) 在文档里叫做 <code>关联数组 (associated array)</code>, 使用关联数组的最低 bash 版本是 4.1.2;</p></blockquote><a id="more"></a><h2 id="数组-关联数组-的创建"><a href="#数组-关联数组-的创建" class="headerlink" title="数组/关联数组 的创建"></a><strong>数组/关联数组 的创建</strong></h2><h3 id="静态创建"><a href="#静态创建" class="headerlink" title="静态创建"></a><strong>静态创建</strong></h3><p>使用类型限定 declare 定义:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数组</span></span><br><span class="line"><span class="built_in">declare</span> -a array1=(<span class="string">'a'</span> <span class="string">'b'</span> <span class="string">'c'</span>)</span><br><span class="line"><span class="built_in">declare</span> -a array2=(a b c)</span><br><span class="line"><span class="comment"># 关联数组</span></span><br><span class="line"><span class="built_in">declare</span> -A map1=([<span class="string">"a"</span>]=<span class="string">"aa"</span> [<span class="string">"b"</span>]=<span class="string">"bb"</span> [<span class="string">"c"</span>]=<span class="string">"cc"</span>)</span><br><span class="line"><span class="built_in">declare</span> -A map2=([a]=aa [b]=bb [c]=cc)</span><br></pre></td></tr></table></figure></p><p>如果不带类型限定, bash 不会自动推断 关联数组 类型:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">object1=(a b c)</span><br><span class="line">object2=([<span class="string">"a"</span>]=<span class="string">"aa"</span> [<span class="string">"b"</span>]=<span class="string">"bb"</span> [<span class="string">"c"</span>]=<span class="string">"cc"</span>)</span><br></pre></td></tr></table></figure></p><p>对于以上两者, bash 都将推断为 普通数组 类型, 其中 object2 中有三个 string 元素: [“a”]=”aa”, [“b”]=”bb” 与 [“c”]=”cc”;</p><h3 id="动态创建"><a href="#动态创建" class="headerlink" title="动态创建"></a><strong>动态创建</strong></h3><p>以上展示了 数组/动态数组 的静态创建方式;<br>更复杂的场景是, 由一段其他复杂命令的输出, 赋值构建一个数组类型:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pair_array=(`sed -n -e <span class="string">'6,/&#125;/p'</span> -e <span class="string">'$d'</span> <span class="variable">$&#123;formatted_curl_response_file&#125;</span> | awk -F <span class="string">':'</span> <span class="string">'&#123;</span></span><br><span class="line"><span class="string">    log_length = length($1);</span></span><br><span class="line"><span class="string">    app_code_length = length($2);</span></span><br><span class="line"><span class="string">    log_path = substr($1, 2, log_length - 2);</span></span><br><span class="line"><span class="string">    app_code = substr($2, 2, app_code_length - 2);</span></span><br><span class="line"><span class="string">    map[log_path] = app_code</span></span><br><span class="line"><span class="string">&#125; END &#123;</span></span><br><span class="line"><span class="string">    for (key in map) &#123;</span></span><br><span class="line"><span class="string">        printf ("%10s=%10s ", key, map[key])</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;'</span>`)</span><br></pre></td></tr></table></figure></p><p>以上逻辑, 由 sed 与 awk 两重管道输出目标内容, 作为创建数组的参数, 以达到动态创建的目的;<br>但是, 以上方式只适用于创建 数组, 而不适用于创建 关联数组, 原因与上一节 静态创建数组 中所表述的相同: 即使输出格式符合定义规范, bash 并不会自动推断为 关联数组;<br>&nbsp;<br>另外, 企图通过 declare 强制限定类型去动态创建, 也是不合法的:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="built_in">declare</span> -A map=(`last -n 1 | head -n 1 | awk <span class="string">'&#123;map[$1]=$3&#125; END&#123;for (key in map) &#123;printf ("[%10s]=%10s ", key, map[key])&#125;&#125;'</span>`)</span><br><span class="line"><span class="comment"># 以上语句会报如下错误:</span></span><br><span class="line">-bash: map: [: must use subscript when assigning associative array</span><br><span class="line">-bash: map: zshell.z]=113.44.125.146: must use subscript when assigning associative array</span><br></pre></td></tr></table></figure></p><p>因为, 通过 ``, $() 等命令代换, [zshell.z]=113.44.125.146 这样的输出内容被当作命令执行, 而 [ 这是一个 bash 的内置命令, 用于条件判断;<br>显然 zshell.z]=113.44.125.146 这样的语句是不符合条件判断的参数输入的;</p><h2 id="数组-关联数组-的使用"><a href="#数组-关联数组-的使用" class="headerlink" title="数组/关联数组 的使用"></a><strong>数组/关联数组 的使用</strong></h2><p>单独赋值:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">map[<span class="string">'a'</span>]=<span class="string">'aaa'</span></span><br><span class="line">array[0]=aaa</span><br></pre></td></tr></table></figure></p><p>获取数据:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得所有 values</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;map[@]&#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;array[@]&#125;</span></span><br><span class="line"><span class="comment"># 获得某个单独的值</span></span><br><span class="line">var=<span class="variable">$&#123;map['a']&#125;</span></span><br><span class="line">var=<span class="variable">$&#123;array[0]&#125;</span></span><br><span class="line"><span class="comment"># 获得所有 keys (对于数组而言, 就是获得所有的索引下标)</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> <span class="variable">$&#123;!map[@]&#125;</span>; <span class="keyword">do</span></span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> <span class="variable">$&#123;!array[@]&#125;</span>; <span class="keyword">do</span></span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="http://blog.csdn.net/adermxl/article/details/41145019" target="_blank" rel="noopener">shell中的map使用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;注: bash 映射 (map) 在文档里叫做 &lt;code&gt;关联数组 (associated array)&lt;/code&gt;, 使用关联数组的最低 bash 版本是 4.1.2;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://zshell.cc/categories/linux/"/>
    
      <category term="shell" scheme="http://zshell.cc/categories/linux/shell/"/>
    
    
      <category term="linux:shell" scheme="http://zshell.cc/tags/linux-shell/"/>
    
  </entry>
  
  <entry>
    <title>rsyncd 配置与运维</title>
    <link href="http://zshell.cc/2017/10/14/rsync--rsyncd%E9%85%8D%E7%BD%AE%E4%B8%8E%E8%BF%90%E8%A1%8C/"/>
    <id>http://zshell.cc/2017/10/14/rsync--rsyncd配置与运行/</id>
    <published>2017-10-14T15:20:21.000Z</published>
    <updated>2018-01-27T14:53:07.577Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要梳理 rsync server 的基本配置与使用方式;</p></blockquote><a id="more"></a><h3 id="rsync-server-的几个关键配置文件"><a href="#rsync-server-的几个关键配置文件" class="headerlink" title="rsync server 的几个关键配置文件"></a><strong>rsync server 的几个关键配置文件</strong></h3><ol><li>/etc/rsyncd.conf: 主配置文件;</li><li>/etc/rsyncd.password/rsyncd.secrets: 秘钥文件;</li><li>/etc/rsyncd.motd: rysnc 服务器元信息, 非必须;</li></ol><p>其中, rsyncd.password 秘钥文件的掩码必须是 600:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; ll /etc/ | grep rsyncd</span><br><span class="line">-rw-r--r--   1 root root    361 Apr  6  2017 rsyncd.conf</span><br><span class="line">-rw-------   1 root root     24 Apr  6  2017 rsyncd.password</span><br></pre></td></tr></table></figure></p><h3 id="rsyncd-conf-配置说明"><a href="#rsyncd-conf-配置说明" class="headerlink" title="rsyncd.conf 配置说明"></a><strong>rsyncd.conf 配置说明</strong></h3><p>一个典型的 rsyncd.conf 文件如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rsyncd 守护进程运行系统用户全局配置, 可在具体的块中配置</span></span><br><span class="line">uid=nobody</span><br><span class="line">gid=nobody</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否需要 chroot, 若为 yes, 当客户端连接某模块时, 首先 chroot 到 模块的 path 目录下</span></span><br><span class="line">user chroot = no</span><br><span class="line"></span><br><span class="line">max connections = 200</span><br><span class="line">timeout = 600</span><br><span class="line"></span><br><span class="line">pid file = /data1/trans_file/rsyncd.pid</span><br><span class="line">lock file = /data1/trans_file/rsyncd.lock</span><br><span class="line"><span class="built_in">log</span> file = /data1/trans_file/rsyncd.log</span><br><span class="line"><span class="comment"># 用户秘钥文件, 可在具体的模块中配置</span></span><br><span class="line">secrets file = /etc/rsyncd.password</span><br><span class="line"><span class="comment"># 服务器元信息, 非必选</span></span><br><span class="line"><span class="comment"># motd file = /etc/rsyncd/rsyncd.motd</span></span><br><span class="line"><span class="comment"># 指定不需要压缩就可以直接传输的文件类型</span></span><br><span class="line">dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模块配置</span></span><br><span class="line">[wireless_log]</span><br><span class="line"><span class="comment"># 模块使用的 user, 此模块将使用 rsyncd.password 文件中 sync 用户对应的秘钥进行文件传输</span></span><br><span class="line">auth users = sync</span><br><span class="line">path = /data1/trans_file/files/wireless_log</span><br><span class="line">ignore errors</span><br><span class="line"><span class="comment"># 是否只读</span></span><br><span class="line"><span class="built_in">read</span> only = no</span><br><span class="line"><span class="comment"># 是否允许列出模块里的内容</span></span><br><span class="line">list = no</span><br></pre></td></tr></table></figure></p><h3 id="rsyncd-password-rsyncd-secrets-配置说明"><a href="#rsyncd-password-rsyncd-secrets-配置说明" class="headerlink" title="rsyncd.password / rsyncd.secrets 配置说明"></a><strong>rsyncd.password / rsyncd.secrets 配置说明</strong></h3><p>以 <code>:</code> 分隔, 用户名和密码, 每行一个:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">user1:password1</span><br><span class="line">user2:password2</span><br></pre></td></tr></table></figure></p><h3 id="rsyncd-启动方式"><a href="#rsyncd-启动方式" class="headerlink" title="rsyncd 启动方式"></a><strong>rsyncd 启动方式</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当负载高时, 以守护进程的方式运行 rsyncd</span></span><br><span class="line">sudo /usr/bin/rsync --daemon --config=/etc/rsyncd.conf</span><br></pre></td></tr></table></figure><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="https://segmentfault.com/a/1190000000444614" target="_blank" rel="noopener">centos下配置rsyncd服务器</a></li><li><a href="http://www.cnblogs.com/itech/archive/2009/08/10/1542945.html" target="_blank" rel="noopener">RSync实现文件备份同步</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要梳理 rsync server 的基本配置与使用方式;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="rsync" scheme="http://zshell.cc/categories/rsync/"/>
    
    
      <category term="rsync" scheme="http://zshell.cc/tags/rsync/"/>
    
  </entry>
  
  <entry>
    <title>jstack 命令使用经验总结</title>
    <link href="http://zshell.cc/2017/09/24/jvm-tools--jstack%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    <id>http://zshell.cc/2017/09/24/jvm-tools--jstack命令使用经验总结/</id>
    <published>2017-09-24T09:11:51.000Z</published>
    <updated>2018-06-14T07:28:00.036Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>jstack 在命令使用上十分简洁, 然而其输出的内容却十分丰富, 信息量足, 值得深入分析;<br>以往对于 jstack 产生的 thread dump, 我很少字斟句酌得分析过每一部分细节, 针对 jstack 的性能诊断也没有一个模式化的总结; 今天这篇文章我就来详细整理一下与 jstack 相关的内容;</p></blockquote><a id="more"></a><hr><h2 id="jstack-命令的基本使用"><a href="#jstack-命令的基本使用" class="headerlink" title="jstack 命令的基本使用"></a><strong>jstack 命令的基本使用</strong></h2><p>jstack 在命令使用上十分简洁, 其信息量与复杂度主要体现在 thread dump 内容的分析上;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最基本的使用</span></span><br><span class="line">sudo -u xxx jstack &#123;vmid&#125;</span><br><span class="line"><span class="comment"># 从 core dump 中提取 thread dump</span></span><br><span class="line">sudo -u xxx jstack core_file_path</span><br><span class="line"><span class="comment"># 除了基本输出外, 额外展示 AbstractOwnableSynchronizer 锁的占有信息</span></span><br><span class="line"><span class="comment"># 可能会消耗较长时间</span></span><br><span class="line">sudo -u xxx jstack -l &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><h2 id="jstack-输出内容结构分析"><a href="#jstack-输出内容结构分析" class="headerlink" title="jstack 输出内容结构分析"></a><strong>jstack 输出内容结构分析</strong></h2><p>首先展示几段 thread dump 的典型例子:<br>正在 RUNNING 中的线程:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"elasticsearch[datanode-39][[xxx_index_v4][9]: Lucene Merge Thread #2403]"</span> #<span class="number">45061</span> daemon prio=<span class="number">5</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007fb968213800</span> nid=<span class="number">0x249ca</span> runnable [<span class="number">0x00007fb6843c2000</span>]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">        ...</span><br><span class="line">        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:<span class="number">94</span>)</span><br><span class="line">        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:<span class="number">626</span>)</span><br></pre></td></tr></table></figure></p><p>阻塞在 java.util.concurrent.locks.Condition 上:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"DubboServerHandler-10.64.16.66:20779-thread-510"</span> #<span class="number">631</span> daemon prio=<span class="number">5</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007fb6f4ce5800</span> nid=<span class="number">0x1743</span> waiting on condition [<span class="number">0x00007fb68ed2f000</span>]</span><br><span class="line">   java.lang.Thread.State: WAITING (parking)</span><br><span class="line">        at sun.misc.Unsafe.park(Native Method)</span><br><span class="line">        - parking to wait <span class="keyword">for</span>  &lt;<span class="number">0x00000000e2978ef0</span>&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)</span><br><span class="line">        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:<span class="number">175</span>)</span><br><span class="line">        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:<span class="number">2039</span>)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></p><p>阻塞在内置锁上:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"qtp302870502-26-acceptor-0@45ff00a-ServerConnector@63475ace&#123;HTTP/1.1&#125;&#123;0.0.0.0:9088&#125;"</span> #<span class="number">26</span> prio=<span class="number">5</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007f1830d3a800</span> nid=<span class="number">0xdf64</span> waiting <span class="keyword">for</span> monitor entry [<span class="number">0x00007f16b5ef9000</span>]</span><br><span class="line">   java.lang.Thread.State: BLOCKED (on object monitor)</span><br><span class="line">        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:<span class="number">234</span>)</span><br><span class="line">        - waiting to lock &lt;<span class="number">0x00000000c07549f8</span>&gt; (a java.lang.Object)</span><br><span class="line">        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:<span class="number">377</span>)</span><br><span class="line">        ...</span><br><span class="line">        at java.lang.Thread.run(Thread.java:<span class="number">745</span>)</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"JFR request timer"</span> #<span class="number">6</span> daemon prio=<span class="number">5</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007fc2f6b1f800</span> nid=<span class="number">0x18070</span> in Object.wait() [<span class="number">0x00007fb9aa96b000</span>]</span><br><span class="line">   java.lang.Thread.State: WAITING (on object monitor)</span><br><span class="line">        at java.lang.Object.wait(Native Method)</span><br><span class="line">        - waiting on &lt;<span class="number">0x00007fba6b50ea38</span>&gt; (a java.util.TaskQueue)</span><br><span class="line">        at java.lang.Object.wait(Object.java:<span class="number">502</span>)</span><br><span class="line">        at java.util.TimerThread.mainLoop(Timer.java:<span class="number">526</span>)</span><br><span class="line">        - locked &lt;<span class="number">0x00007fba6b50ea38</span>&gt; (a java.util.TaskQueue)</span><br><span class="line">        at java.util.TimerThread.run(Timer.java:<span class="number">505</span>)</span><br></pre></td></tr></table></figure><p>以上展示了四个线程的 jstack dump, 有 running 状态, 也有阻塞状态, 覆盖面广, 具有典型性; 下面来对 jstack 的输出内容作详细梳理;</p><h3 id="输出内容的结构"><a href="#输出内容的结构" class="headerlink" title="输出内容的结构"></a><strong>输出内容的结构</strong></h3><p>首先还是要说一下 jstack 输出的内容结构, 就以上方举的第四个线程为例:<br>以下是第一部分内容, 记录了线程的一些基本信息, 从左到右每个元素的含义已经以注释标注在元素上方; 其中比较重要的是 <code>nid</code>, 它是 java 线程与操作系统的映射, 在 linux 中它和与其对应的轻量级进程 pid 相同 (需要十六进制与十进制转换), 这将为基于 java 线程的性能诊断带来帮助, 详细请见本文后面段落 <a href="#线程性能诊断的辅助脚本">线程性能诊断的辅助脚本</a>;<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//|-----线程名------| |-线程创建次序-| |是否守护进程| |---线程优先级---| |-------线程 id-------| |-所映射的linux轻量级进程id-| |-------------线程动作--------------|</span></span><br><span class="line">  <span class="string">"JFR request timer"</span> #<span class="number">6</span>              daemon        prio=<span class="number">5</span> os_prio=<span class="number">0</span>  tid=<span class="number">0x00007fc2f6b1f800</span> nid=<span class="number">0x18070</span>                 in Object.wait() [<span class="number">0x00007fb9aa96b000</span>]</span><br></pre></td></tr></table></figure></p><p>以下是第二部分内容, 表示线程当前的状态;<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Thread.State: WAITING (on object monitor)</span><br></pre></td></tr></table></figure></p><p>以下是第三部分内容, 主要记录了线程的调用栈; 其中比较重要的是一些关键调用上的 <a href="#线程的重要调用修饰">动作修饰</a>, 这些为线程死锁问题的排查提供了依据;<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">at java.lang.Object.wait(Native Method)</span><br><span class="line">- waiting on &lt;<span class="number">0x00007fba6b50ea38</span>&gt; (a java.util.TaskQueue)</span><br><span class="line">at java.lang.Object.wait(Object.java:<span class="number">502</span>)</span><br><span class="line">at java.util.TimerThread.mainLoop(Timer.java:<span class="number">526</span>)</span><br><span class="line">- locked &lt;<span class="number">0x00007fba6b50ea38</span>&gt; (a java.util.TaskQueue)</span><br><span class="line">at java.util.TimerThread.run(Timer.java:<span class="number">505</span>)</span><br></pre></td></tr></table></figure></p><h3 id="线程的动作"><a href="#线程的动作" class="headerlink" title="线程的动作"></a><strong>线程的动作</strong></h3><p>线程动作的记录在每个 thread dump 的第一行末尾, 一般情况下可分为如下几类:</p><ol><li><code>runnable</code>, 表示线程在参与 cpu 资源的竞争, 可能在被调度运行也可能在就绪等待;</li><li><code>sleeping</code>, 表示调用了 Thread.sleep(), 线程进入休眠;</li><li><code>waiting for monitor entry [0x...]</code>, 表示线程在试图获取内置锁, 进入了等待区 Entry Set, 方括号内的地址表示线程等待的资源地址;</li><li><code>in Object.wait() [0x...]</code>, 表示线程调用了 object.wait(), 放弃了内置锁, 进入了等待区 Wait Set, 等待被唤醒, 方括号内的地址表示线程放弃的资源地址;</li><li><code>waiting on condition [0x...]</code>, 表示线程被阻塞原语所阻塞, 方括号内的地址表示线程等待的资源地址; 这种和 jvm 的内置锁体系没有关系, 它是 jdk5 之后的 java.util.concurrent 包下的锁机制;</li></ol><h3 id="线程的状态"><a href="#线程的状态" class="headerlink" title="线程的状态"></a><strong>线程的状态</strong></h3><p>线程的状态记录在每个 thread dump 的第二行, 并以 java.lang.Thread.State 开头, 一般情况下可分为如下几类:</p><ol><li><code>RUNNABLE</code>, 这种一般与线程动作 <code>runnable</code> 一起出现;</li><li><code>BLOCKED (on object monitor)</code>, 这种一般与线程动作 <code>waiting for monitor entry</code> 一起出现, 不过在其线程调用栈最末端并没有一个固定的方法, 因为 <code>synchronized</code> 关键字可以修饰各种方法或者同步块;</li><li><code>WAITING (on object monitor)</code> 或者 <code>TIMED_WAITING (on object monitor)</code>, 这种一般与线程动作 <code>in Object.wait() [0x...]</code> 一起出现, 并且线程调用栈的最末端调用方法为 at java.lang.Object.wait(Native Method), 以表示 object.wait() 方法的调用;<br>另外, <code>WAITING</code> 与 <code>TIMED_WAITING</code> 的区别在于是否设置了超时中断, 即 <code>wait(long timeout)</code> 与 <code>wait()</code> 的区别;</li><li><code>WAITING (parking)</code> 或者 <code>TIMED_WAITING (parking)</code>, 这种一般与线程动作 <code>waiting on condition [0x...]</code> 一起出现, 并且线程调用栈的最末端调用方法一般为 at sun.misc.Unsafe.park(Native Method);<br>Unsafe.park 使用的是线程阻塞原语, 主要在 java.util.concurrent.locks.AbstractQueuedSynchronizer 类中被使用到, 很多基于 AQS 构建的同步工具, 如 ReentrantLock, Condition, CountDownLatch, Semaphore 等都会诱发线程进入该状态;<br>另外, <code>WAITING</code> 与 <code>TIMED_WAITING</code> 的区别与第三点中提到的原因一致;</li></ol><h3 id="线程的重要调用修饰"><a href="#线程的重要调用修饰" class="headerlink" title="线程的重要调用修饰"></a><strong>线程的重要调用修饰</strong></h3><p>thread dump 的第三部分线程调用栈中, 一般会把与锁相关的资源使用状态以附加的形式作重点修饰, 这与线程的动作及状态有着密切的联系, 一般情况下可分为如下几类:</p><ol><li><code>locked &lt;0x...&gt;</code>, 表示其成功获取了内置锁, 成为了 owner;</li><li><code>parking to wait for &lt;0x...&gt;</code>, 表示其被阻塞原语所阻塞, 通常与线程动作 <code>waiting on condition</code> 一起出现;</li><li><code>waiting to lock &lt;0x...&gt;</code>, 表示其在 Entry Set 中等待某个内置锁, 通常与线程动作 <code>waiting for monitor entry</code> 一起出现;</li><li><code>waiting on &lt;0x...&gt;</code>, 表示其在 Wait Set 中等待被唤醒, 通常与线程动作 <code>in Object.wait() [0x...]</code> 一起出现;<br>另外, waiting on 调用修饰往往与 locked 调用修饰一同出现, 如之前列举的第四个 thread dump:<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">at java.lang.Object.wait(Native Method)</span><br><span class="line">  - waiting on &lt;<span class="number">0x00007fba6b50ea38</span>&gt; (a java.util.TaskQueue)</span><br><span class="line">  at java.lang.Object.wait(Object.java:<span class="number">502</span>)</span><br><span class="line">  at java.util.TimerThread.mainLoop(Timer.java:<span class="number">526</span>)</span><br><span class="line">  - locked &lt;<span class="number">0x00007fba6b50ea38</span>&gt; (a java.util.TaskQueue)</span><br><span class="line">  at java.util.TimerThread.run(Timer.java:<span class="number">505</span>)</span><br></pre></td></tr></table></figure></li></ol><p>这是因为该线程之前获得过该内置锁, 现在因为 object.wait() 又将其放弃了, 所以在调用栈中会出现先后两个调用修饰;</p><h3 id="死锁检测的展示"><a href="#死锁检测的展示" class="headerlink" title="死锁检测的展示"></a><strong>死锁检测的展示</strong></h3><p>在 jdk5 之前, Doug Lea 大神还没有发布 java.util.concurrent 包, 这个时候提及的锁, 就仅限于 jvm 监视器内置锁; 此时如果进程内有死锁发生, jstack 将会把死锁检测信息打印出来:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Found one Java-level deadlock:</span><br><span class="line">=============================</span><br><span class="line"><span class="string">"Thread-xxx"</span>:</span><br><span class="line">  waiting to lock monitor <span class="number">0x00007f0134003ae8</span> (object <span class="number">0x00000007d6aa2c98</span>, a java.lang.Object),</span><br><span class="line">  which is held by <span class="string">"Thread-yyy"</span></span><br><span class="line"><span class="string">"Thread-yyy"</span>:</span><br><span class="line">  waiting to lock monitor <span class="number">0x00007f0134006168</span> (object <span class="number">0x00000007d6aa2ca8</span>, a java.lang.Object),</span><br><span class="line">  which is held by <span class="string">"Thread-xxx"</span></span><br><span class="line"></span><br><span class="line">Java <span class="built_in">stack</span> information <span class="keyword">for</span> the threads listed above:</span><br><span class="line">===================================================</span><br><span class="line"><span class="string">"Thread-xxx"</span>:</span><br><span class="line">    ...</span><br><span class="line"><span class="string">"Thread-yyy"</span>:</span><br><span class="line">    ...</span><br><span class="line">Found <span class="number">1</span> deadlock.</span><br></pre></td></tr></table></figure></p><p>然而后来 Doug Lea 发布了 java.util.concurrent 包, 当谈及 java 的锁, 除了内置锁之外还有了基于 AbstractOwnableSynchronizer 的各种形式; 由于是新事物, 彼时 jdk5 的 jstack 没有及时提供对以 AQS 构建的同步工具的死锁检测功能, 直到 jdk6 才完善了相关支持;</p><h2 id="常见-java-进程的-jstack-dump-特征"><a href="#常见-java-进程的-jstack-dump-特征" class="headerlink" title="常见 java 进程的 jstack dump 特征"></a><strong>常见 java 进程的 jstack dump 特征</strong></h2><p>首先, 不管是什么类型的 java 应用, 有一些通常都会存在的线程:</p><p><strong>VM Thread 与 VM Periodic Task Thread</strong><br>虚拟机线程, 属于 native thread, 凌驾与其他用户线程之上;<br>VM Periodic Task Thread 通常用于虚拟机作 sampling/profiling, 收集系统运行信息, 为 JIT 优化作决策依据;</p><p><strong>C1 / C2 CompilerThread</strong><br>虚拟机的 JIT 及时编译器线程:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"C1 CompilerThread2"</span> #<span class="number">10</span> daemon prio=<span class="number">9</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007feb34114000</span> nid=<span class="number">0x18b2</span> waiting on condition [<span class="number">0x0000000000000000</span>]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line"><span class="string">"C2 CompilerThread1"</span> #<span class="number">9</span> daemon prio=<span class="number">9</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007feb34112000</span> nid=<span class="number">0x18b1</span> waiting on condition [<span class="number">0x0000000000000000</span>]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line"><span class="string">"C2 CompilerThread0"</span> #<span class="number">8</span> daemon prio=<span class="number">9</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007feb3410f800</span> nid=<span class="number">0x18b0</span> waiting on condition [<span class="number">0x0000000000000000</span>]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br></pre></td></tr></table></figure></p><p><strong>Reference Handler 线程与 Finalizer 线程</strong><br>这两个线程用于虚拟机处理 override 了 Object.finalize() 方法的实例, 对实例回收前作最后的判决;<br>Reference Handler 线程用于将目标对象放入 reference queue:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Reference Handler"</span> #<span class="number">2</span> daemon prio=<span class="number">10</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007f91e007f000</span> nid=<span class="number">0xa80</span> in Object.wait() [<span class="number">0</span>x...]</span><br><span class="line">   java.lang.Thread.State: WAITING (on object monitor)</span><br><span class="line">        at java.lang.Object.wait(Native Method)</span><br><span class="line">        at java.lang.Object.wait(Object.java:<span class="number">502</span>)</span><br><span class="line">        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:<span class="number">157</span>)</span><br><span class="line">        - locked &lt;<span class="number">0x00000000c0495140</span>&gt; (a java.lang.ref.Reference$Lock)</span><br></pre></td></tr></table></figure></p><p>Finalizer 线程用于从 reference queue 中取出对象以执行其 finalize 方法:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Finalizer"</span> #<span class="number">3</span> daemon prio=<span class="number">8</span> os_prio=<span class="number">0</span> tid=<span class="number">0x00007f91e0081000</span> nid=<span class="number">0xa81</span> in Object.wait() [<span class="number">0</span>x...]</span><br><span class="line">   java.lang.Thread.State: WAITING (on object monitor)</span><br><span class="line">        at java.lang.Object.wait(Native Method)</span><br><span class="line">        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:<span class="number">143</span>)</span><br><span class="line">        - locked &lt;<span class="number">0x00000000c008db88</span>&gt; (a java.lang.ref.ReferenceQueue$Lock)</span><br><span class="line">        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:<span class="number">164</span>)</span><br><span class="line">        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:<span class="number">209</span>)</span><br></pre></td></tr></table></figure></p><p><strong>gc 线程</strong><br>这块对于不同的 gc 收集器选型有各自不同的线程状态 (线程数视 cpu 核心数而定);<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parallel Scavenge</span></span><br><span class="line"><span class="string">"GC task thread#0 (ParallelGC)"</span> os_prio=0 tid=0x00007f91e0021000 nid=0xa7a runnable </span><br><span class="line"><span class="string">"GC task thread#1 (ParallelGC)"</span> os_prio=0 tid=0x00007f91e0023000 nid=0xa7b runnable</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ParNew</span></span><br><span class="line"><span class="string">"Gang worker#0 (Parallel GC Threads)"</span> os_prio=0 tid=0x00007feb3401e800 nid=0x18a4 runnable </span><br><span class="line"><span class="string">"Gang worker#1 (Parallel GC Threads)"</span> os_prio=0 tid=0x00007feb34020000 nid=0x18a5 runnable</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMS</span></span><br><span class="line"><span class="string">"Concurrent Mark-Sweep GC Thread"</span> os_prio=0 tid=0x00007feb34066800 nid=0x18a8 runnable</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># G1</span></span><br><span class="line"><span class="string">"G1 Main Concurrent Mark GC Thread"</span> os_prio=0 tid=0x00007fc2f4091800 nid=0x1805e runnable</span><br><span class="line"></span><br><span class="line"><span class="string">"Gang worker#0 (G1 Parallel Marking Threads)"</span> os_prio=0 tid=0x00007fc2f4093800 nid=0x1805f runnable </span><br><span class="line"><span class="string">"Gang worker#1 (G1 Parallel Marking Threads)"</span> os_prio=0 tid=0x00007fc2f4095800 nid=0x18060 runnable</span><br><span class="line"></span><br><span class="line"><span class="string">"G1 Concurrent Refinement Thread#0"</span> os_prio=0 tid=0x00007fc2f4079000 nid=0x1805d runnable </span><br><span class="line"><span class="string">"G1 Concurrent Refinement Thread#1"</span> os_prio=0 tid=0x00007fc2f4077000 nid=0x1805c runnable</span><br></pre></td></tr></table></figure><p>以上便是 java 进程里通常都会存在的线程;</p><h3 id="纯-tomcat-容器"><a href="#纯-tomcat-容器" class="headerlink" title="纯 tomcat 容器"></a><strong>纯 tomcat 容器</strong></h3><h3 id="tomcat-with-dubbo"><a href="#tomcat-with-dubbo" class="headerlink" title="tomcat with dubbo"></a><strong>tomcat with dubbo</strong></h3><h3 id="elasticsearch-datanode-节点"><a href="#elasticsearch-datanode-节点" class="headerlink" title="elasticsearch datanode 节点"></a><strong>elasticsearch datanode 节点</strong></h3><h2 id="相关衍生工具"><a href="#相关衍生工具" class="headerlink" title="相关衍生工具"></a><strong>相关衍生工具</strong></h2><h3 id="使用代码作-thread-dump"><a href="#使用代码作-thread-dump" class="headerlink" title="使用代码作 thread dump"></a><strong>使用代码作 thread dump</strong></h3><p>除了使用 jstack 之外, 还有其他一些方法可以对 java 进程作 thread dump, 如果将其封装为 http 接口, 便可以不用登陆主机, 直接在浏览器上查询 thread dump 的情况;<br><strong>使用 jmx 的 api</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span>  <span class="title">threadDump</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();</span><br><span class="line">   <span class="keyword">for</span> (ThreadInfo threadInfo : threadMxBean.dumpAllThreads(<span class="keyword">true</span>, <span class="keyword">true</span>)) &#123;</span><br><span class="line">       <span class="comment">// deal with threadInfo.toString()</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>使用 Thread.getAllStackTraces() 方法</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">threadDump</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Thread, StackTraceElement[]&gt; stackTrace : Thread.getAllStackTraces().entrySet()) &#123;</span><br><span class="line">        Thread thread = (Thread) stackTrace.getKey();</span><br><span class="line">        StackTraceElement[] stack = (StackTraceElement[]) stackTrace.getValue();</span><br><span class="line">        <span class="keyword">if</span> (thread.equals(Thread.currentThread())) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// deal with thread</span></span><br><span class="line">        <span class="keyword">for</span> (StackTraceElement stackTraceElement : stack) &#123;</span><br><span class="line">            <span class="comment">// deal with stackTraceElement</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="线程性能诊断的辅助脚本"><a href="#线程性能诊断的辅助脚本" class="headerlink" title="线程性能诊断的辅助脚本"></a><strong>线程性能诊断的辅助脚本</strong></h3><p>使用 jstack 还有一个重要的功能就是分析热点线程: 找出占用 cpu 资源最高的线程;<br>首先我先介绍一下手工敲命令分析的方法:</p><ul><li><p>使用 top 命令找出 cpu 使用率高的 thread id:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -p pid: 只显示指定进程的信息</span></span><br><span class="line"><span class="comment"># -H: 展示线程的详细信息</span></span><br><span class="line">top -H -p &#123;pid&#125;</span><br><span class="line"><span class="comment"># 使用 P 快捷键按 cpu 使用率排序, 并记录排序靠前的若干 pid (轻量级进程 id)</span></span><br></pre></td></tr></table></figure></li><li><p>作进制转换:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将记录下的十进制 pid 转为十六进制</span></span><br><span class="line">thread_id_0x=`<span class="built_in">printf</span> <span class="string">"%x"</span> <span class="variable">$thread_id</span>`</span><br><span class="line">`<span class="built_in">echo</span> <span class="string">"obase=16; <span class="variable">$thread_id</span>"</span> | bc`</span><br></pre></td></tr></table></figure></li><li><p>由于 thread dump 中记录的每个线程的 nid 是与 linux 轻量级进程 pid 一一对应的 (只是十进制与十六进制的区别), 所以便可以拿转换得到的十六进制 thread_id_0x, 去 thread dump 中搜索对应的 nid, 定位问题线程;<br>&nbsp;</p></li></ul><p>下面介绍一个脚本, 其功能是: 按照 cpu 使用率从高到低排序, 打印指定 jvm 进程的前 n 个线程;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line">default_lines=10</span><br><span class="line">top_head_info_padding_lines=8</span><br><span class="line">default_stack_lines=15</span><br><span class="line"></span><br><span class="line">jvm_pid=<span class="variable">$1</span></span><br><span class="line">jvm_user=<span class="variable">$2</span></span><br><span class="line">((thread_stack_lines=<span class="variable">$&#123;3:-$default_lines&#125;</span>+top_head_info_padding_lines))</span><br><span class="line"></span><br><span class="line">threads_top_capture=$(top -b -n1 -H -p <span class="variable">$jvm_pid</span> | grep <span class="variable">$jvm_user</span> | head -n <span class="variable">$thread_stack_lines</span>)</span><br><span class="line">jstack_output=$(<span class="built_in">echo</span> <span class="string">"<span class="variable">$(sudo -i -u $jvm_user jstack $jvm_pid)</span>"</span>)</span><br><span class="line">top_output=$(<span class="built_in">echo</span> <span class="string">"<span class="variable">$(echo "$threads_top_capture" | perl -pe 's/\e\[?.*?[\@-~] ?//g' | awk '&#123;gsub(/^ +/,"")</span>;print&#125;' | awk '&#123;gsub(/ +|[+-]/,"</span> <span class="string">");print&#125;' | cut -d "</span> <span class="string">" -f 1,9 )\n "</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"***********************************************************"</span></span><br><span class="line">uptime</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Analyzing top <span class="variable">$top_threads</span> threads"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"***********************************************************"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span> %s <span class="string">"<span class="variable">$top_output</span>"</span> | <span class="keyword">while</span> IFS= <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    pid=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d <span class="string">" "</span> -f 1)</span><br><span class="line">    hexapid=$(<span class="built_in">printf</span> <span class="string">"%x"</span> <span class="variable">$pid</span>)</span><br><span class="line">    cpu=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d <span class="string">" "</span> -f 2)</span><br><span class="line">    <span class="built_in">echo</span> -n <span class="variable">$cpu</span> <span class="string">"% [<span class="variable">$pid</span>] "</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$jstack_output</span>"</span> | grep <span class="string">"tid.*0x<span class="variable">$hexapid</span> "</span> -A <span class="variable">$default_stack_lines</span> | sed -n -e <span class="string">'/0x'</span><span class="variable">$hexapid</span><span class="string">'/,/tid/ p'</span> | head -n -1</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p><p>该脚本有多种版本, 在我司的每台主机上的指定路径下都存放了一个副本; 出于保密协议, 该脚本源码不便于公开, 上方所展示的版本是基于美团点评的技术专家王锐老师在一次 <a href="https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651746699&amp;idx=2&amp;sn=c52feeab2576056e4a65e26a99702206&amp;chksm=bd12a8c68a6521d0de81ac8ab437df1a9e702053b7840af9ac86b29979865c6fc1000286875e&amp;mpshare=1&amp;scene=1&amp;srcid=0610dNiqShEJLkHiQLiIN4z1#rd" target="_blank" rel="noopener">问答分享</a> 中给出的代码所改造的;</p><h3 id="thread-dump-可视化分析工具"><a href="#thread-dump-可视化分析工具" class="headerlink" title="thread dump 可视化分析工具"></a><strong>thread dump 可视化分析工具</strong></h3><p>与 <a href="gceasy.io">gceasy.io</a> 一道, 同出自一家之手: <a href="http://fastthread.io" target="_blank" rel="noopener">fastthread.io</a>;</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="https://www.jianshu.com/p/6690f7e92f27" target="_blank" rel="noopener">如何使用jstack分析线程状态</a></li><li><a href="http://www.cnblogs.com/kongzhongqijing/articles/3630264.html" target="_blank" rel="noopener">java命令–jstack 工具</a></li><li><a href="https://my.oschina.net/dabird/blog/691692" target="_blank" rel="noopener">7 个抓取 Java Thread Dumps 的方式</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651746699&amp;idx=2&amp;sn=c52feeab2576056e4a65e26a99702206&amp;chksm=bd12a8c68a6521d0de81ac8ab437df1a9e702053b7840af9ac86b29979865c6fc1000286875e&amp;mpshare=1&amp;scene=1&amp;srcid=0610dNiqShEJLkHiQLiIN4z1#rd" target="_blank" rel="noopener">你与Java大牛的距离，只差这24个问题</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;jstack 在命令使用上十分简洁, 然而其输出的内容却十分丰富, 信息量足, 值得深入分析;&lt;br&gt;以往对于 jstack 产生的 thread dump, 我很少字斟句酌得分析过每一部分细节, 针对 jstack 的性能诊断也没有一个模式化的总结; 今天这篇文章我就来详细整理一下与 jstack 相关的内容;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="jvm" scheme="http://zshell.cc/categories/jvm/"/>
    
      <category term="tools" scheme="http://zshell.cc/categories/jvm/tools/"/>
    
    
      <category term="jvm:tools" scheme="http://zshell.cc/tags/jvm-tools/"/>
    
  </entry>
  
  <entry>
    <title>nagios 配置文件梳理及运维实践</title>
    <link href="http://zshell.cc/2017/08/25/alert-nagios--nagios%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%A2%B3%E7%90%86%E5%8F%8A%E8%BF%90%E7%BB%B4%E5%AE%9E%E8%B7%B5/"/>
    <id>http://zshell.cc/2017/08/25/alert-nagios--nagios配置文件梳理及运维实践/</id>
    <published>2017-08-25T12:19:32.000Z</published>
    <updated>2018-06-05T12:23:55.904Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>nagios 的优点在于其插件拓展式的设计, 不过 nagios 给 ops 映像最深刻的, 是其出离复杂的配置文件; nagios 真的可以说是把配置文件当数据库使了;<br>作为备忘, 本文主要梳理 nagios 配置文件中的各种角色的关系与交互流程, 并就日常工作的经验总结一些 nagios 配置文件的部署及运维实践;</p></blockquote><a id="more"></a><hr><h2 id="nagios-中的角色梳理"><a href="#nagios-中的角色梳理" class="headerlink" title="nagios 中的角色梳理"></a><strong>nagios 中的角色梳理</strong></h2><p>nagios 的配置文件角色众多, 各角色之间存在依赖关系, 从这个角度上看, nagios 很像是一个 “关系型数据库”;<br>针对我日常工作中遇到的情景, 其中可能涉及到的角色如下:</p><ul><li>service, 最核心的角色, 标识一个完整的检测服务单元;</li><li>command, 报警检测命令;</li><li>contact / contactgroup, 联系人 / 联系组, 当指标异常时的联系对象;</li><li>host / hostgroup, 主机, 主机组, 报警检测的目标机器;</li><li>timeperiod, 报警时间段;</li></ul><h3 id="nagios-各角色之间的关系"><a href="#nagios-各角色之间的关系" class="headerlink" title="nagios 各角色之间的关系"></a><strong>nagios 各角色之间的关系</strong></h3><p>既然 nagios 的配置文件像一个关系型数据库, 那么一定可以作出它的 实体关系 ER 图, 绘制如下:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/alert/nagios/nagios的配置文件及其实践/nagios角色关系ER图2.png" alt="nagios 的角色关系图1" title="">                </div>                <div class="image-caption">nagios 的角色关系图1</div>            </figure><br>这是第一类简单的 ER 图, 其中 service 直接关联到单一联系人或单一主机; 除了这种情况外, service 也可以直接关联到联系人组或主机组, 再由组间接关联到具体的人或主机, 如下图所示:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/zshell-zhang/static-content/master/cs/alert/nagios/nagios的配置文件及其实践/nagios角色关系ER图1.png" alt="nagios 的角色关系图2" title="">                </div>                <div class="image-caption">nagios 的角色关系图2</div>            </figure></p><h3 id="nagios-command-的参数传递"><a href="#nagios-command-的参数传递" class="headerlink" title="nagios command 的参数传递"></a><strong>nagios command 的参数传递</strong></h3><p>由上面展示的两张图可以发现, service 是所有角色的中心, command 则是在整个流程中穿针引线的关键要素; 除了 service 中的 check_command 之外, host 也存在自己检测主机的 check_command, contact 则需要定义触发报警通知的 service_notification_commands 和 host_notification_commands; 所以说, command 的实现与调用十分关键, 而 command 调用的关键又在于其参数传递;<br>nagios 的参数分为两种类型, 一种是其自己的保留参数(宏定义), 常用的列举如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">HOSTNAME            <span class="comment"># 主机名, 对应 host 角色中的 host_name 域</span></span><br><span class="line">HOSTADDRESS         <span class="comment"># 主机地址, 对应 host 角色中的 address 域</span></span><br><span class="line"></span><br><span class="line">NOTIFICATIONTYPE    <span class="comment"># 通知类型, 主要有 PROBLEM, RECOVERY 等</span></span><br><span class="line">SERVICESTATE        <span class="comment"># 服务状态, 主要有 warning, unknown, critical, recovery 等</span></span><br><span class="line"></span><br><span class="line">CONTACTNAME         <span class="comment"># 联系人, 对应 contact 角色中的 contact_name 域</span></span><br><span class="line">CONTACTEMAIL        <span class="comment"># 联系人的邮件, 对应 contact 角色中的 email 域</span></span><br><span class="line"></span><br><span class="line">SERVICEDESC         <span class="comment"># 服务的描述, 对应 service 角色中的 service_description 域</span></span><br><span class="line">SERVICEOUTPUT       <span class="comment"># 服务的详细输出</span></span><br></pre></td></tr></table></figure></p><p>这类宏定义, 在使用时会直接从角色配置项的具体域中直接获取值, 前提条件是配置项对应的域要已经配置了该值;<br>第二种类型是自定义类型, 在使用时是以顺序获取的, 如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">define <span class="built_in">command</span> &#123;</span><br><span class="line">    command_name    check_ping</span><br><span class="line">    command_line    <span class="variable">$USER1</span>$/check_ping -H <span class="variable">$HOSTADDRESS</span>$ -w <span class="variable">$ARG1</span>$ -c <span class="variable">$ARG2</span>$</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>以上 check_ping 命令一共接受了 3 个参数: <code>$HOSTADDRESS$</code>, <code>$ARG1$</code>, <code>$ARG2$</code>, 除了 <code>$HOSTADDRESS$</code> 是由 host 中的 address 域获得的, 其余两个自定义参数则按顺序填充至最终的命令中; 而调用该命令的服务配置如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">define service &#123;</span><br><span class="line">    ...</span><br><span class="line">    check_command   check_ping!50,10%!100,20%</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到, 分别将各个自定义参数按顺序放在命令的后面, 以 <code>!</code> 隔开, 即为调用方式, 而对于 <code>$HOSTADDRESS$</code> 这种宏定义参数, 则不需要主动设置, nagios 自己会带上它;</p><h2 id="nagios-配置文件部署与运维实践"><a href="#nagios-配置文件部署与运维实践" class="headerlink" title="nagios 配置文件部署与运维实践"></a><strong>nagios 配置文件部署与运维实践</strong></h2><p>面对 nagios 这种不太友好, 略显复杂的配置, 想完全依托它实现自动化运维确实有些麻烦, 所以在不断的使用经验总结中, 我们渐渐形成了一套自己的使用方式; 另外, 我们还逐步将一些关键逻辑从 nagios 转移到自己开发的旁路系统中, 简化 nagios 的配置内容, 从而突出它的核心功能;</p><h3 id="三类报警检测类型"><a href="#三类报警检测类型" class="headerlink" title="三类报警检测类型"></a><strong>三类报警检测类型</strong></h3><p>首先从实现方式上分类, 我们使用了两大类: 使用插件拓展方式的 check_nrpe 和直接在 nagios server 上执行的本地检测命令;<br>(1) check_nrpe 对我们来说主要是用于非网络型的机器指标检测, 包括 <code>check_load</code>, <code>check_disk</code>, <code>check_procs</code>, <code>check_swap</code>, <code>check_users</code>, 以及戴尔供应商的硬件检测工具 (仅适用于实体机) <code>check_openmanage</code>, <code>check_dell_temperature</code> 等;<br>这类检测只能在 agent 上执行, 并且对于不同的主机环境, 其报警阈值各不相同, 需在 agent 上个性化定制; 所以, 这类指标检测只适用于 check_nrpe 的方式;<br>以下是一个典型的 nrpe 配置文件:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">command</span>[check_zombie_procs]=/home/nrpe/libexec/check_procs -w 5 -c 10 -s Z</span><br><span class="line"><span class="built_in">command</span>[check_total_procs]=/home/nrpe/libexec/check_procs -w 150 -c 200</span><br><span class="line"><span class="built_in">command</span>[check_disk]=/home/nrpe/libexec/check_disk -w 15% -c 10% -A -l</span><br><span class="line"><span class="built_in">command</span>[check_swap]=/home/nrpe/libexec/check_swap -w 90% -c 80%</span><br><span class="line"><span class="built_in">command</span>[check_load]=/home/nrpe/libexec/check_load -w 12,3.2,8 -c 12,3.6,8</span><br></pre></td></tr></table></figure></p><p>另外除了以上通用的 nrpe 检查项之外, 对于一个标准的生产环境, 还可以定制特定的进程检查项:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">command</span>[check_crond]=/home/nrpe/libexec/check_procs -C crond -c 1:1</span><br><span class="line"><span class="built_in">command</span>[check_collectd]=/home/nrpe/libexec/check_procs -C collectd -c 1:1</span><br><span class="line"><span class="built_in">command</span>[check_ntpd]=/home/nrpe/libexec/check_procs -C ntpd -c 1:1</span><br><span class="line"><span class="built_in">command</span>[check_dnsmasq]=/home/nrpe/libexec/check_dnsmasq.pl -s 127.0.0.1 -H xxx.com -w 250 -c 300</span><br><span class="line"><span class="built_in">command</span>[check_flume-ng]=/home/nrpe/libexec/check_procs -a /home/apache-flume-ng/bin/flume-ng-manage -c 1:1</span><br></pre></td></tr></table></figure></p><p>当然这些都是对辅助系统的检测, 对于生产环境下真正提供业务服务的系统, 则使用其他更灵活的方法监控;</p><p>(2) 直接在 nagios server 上执行的检测命令, 主要是网络型的机器指标检测, 如 <code>check_ping</code>, <code>check_tcp</code>, <code>check_udp</code> 等, 以及第三方的业务指标检测命令 <code>check_graphite</code>;<br>网络型的检测命令自不必多说, 其本身并不依赖于 agent 机器, 在 nagios server 上执行即可; 而第三方的业务指标检测命令 <code>check_graphite</code>, 主要依赖于 graphite_api 提供的接口获取业务指标,  亦不需要依赖 agent 机器;<br>以下为 check_graphite 命令的两种典型举例, 一正一反:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">define <span class="built_in">command</span> &#123;</span><br><span class="line">    command_name    check_graphite</span><br><span class="line">    command_line    /usr/lib64/nagios/plugins/check_graphite --host 10.64.0.49:8888 --metric <span class="variable">$ARG1</span>$ --critical <span class="variable">$ARG2</span>$ --warning <span class="variable">$ARG3</span>$ --name <span class="variable">$ARG4</span>$ --duration <span class="variable">$ARG5</span>$ --<span class="keyword">function</span> <span class="variable">$ARG6</span>$ 2&gt;&amp;1</span><br><span class="line">&#125;</span><br><span class="line">define <span class="built_in">command</span> &#123;</span><br><span class="line">    command_name    check_graphite_invert</span><br><span class="line">    command_line    /usr/lib64/nagios/plugins/check_graphite --invert --host 10.64.0.49:8888 --metric <span class="variable">$ARG1</span>$ --critical <span class="variable">$ARG2</span>$ --warning <span class="variable">$ARG3</span>$ --name <span class="variable">$ARG4</span>$ --duration <span class="variable">$ARG5</span>$ --<span class="keyword">function</span> <span class="variable">$ARG6</span>$  2&gt;&amp;1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>典型的 service 调用如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">define service &#123;</span><br><span class="line">    service_description     xxxx</span><br><span class="line">    use                     service_template</span><br><span class="line">    ...</span><br><span class="line">    check_period            name_1_2_3_4_5_6_7_00:00-23:59</span><br><span class="line">    contact_groups          common-contact</span><br><span class="line">    check_command           check_graphite!<span class="string">'metrics.name.sample'</span>!48!48!<span class="string">'query_time_too_long'</span>!15!<span class="string">'max'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里就要提到第一个被我们转移到旁路系统的功能: <strong>service 配置文件的自动生成</strong>;<br>在 <a href="">应用中心的角色定位与功能总结</a> 这篇文章里曾提及, 我们将设置报警项的过程留给业务线同学在应用中心的界面上完成; 对于新增的报警项, 我们写了一个 service-generator 系统, 按 appcode 划分, 定时将其刷到 nagios 的不同配置文件中;<br>我们使用 gitlab 管理 nagios 的配置文件, 其配置文件所在 base 路径已经关联到远程仓库; 与此同时, 在 nagios 实例所在主机上我们部署了 config-reload-manager 管理工具: 对于 service-generator 新增的配置内容, config-reload-manager 定时 reload nagios 进程, 如果成功加载, 则将本次新增内容 commit, 如果加载失败, 则单独 checkout 一个临时分支记录问题并报警, 重新 reset 到之前最新的版本, 恢复服务;<br>其实上述逻辑涉及到一个比较共性的问题: <a href="">系统动态配置文件的运维经验总结</a>, 详细内容可以参考这篇文章;</p><p>(3) 第三类检测类型严格上说仍然属于直接在 nagios server 上执行的检测命令, 不过将其单独提出来是因为它的特殊功能: 服务状态检测; 在第一类 check_nrpe 检测中曾提到, 对于生产环境下的服务, 使用其他更灵活的方式监控, 这里指的就是基于端口的服务状态检测, 说的再直白一些就是使用 check_tcp / check_udp 命令检测服务端口的状态, 如果服务挂了, 对应的端口就会被探测到 connection refused 或者 timeout;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">define service &#123;</span><br><span class="line">    service_description      alive_appcode_xxx_8080</span><br><span class="line">    use                      service_template</span><br><span class="line">    host_name                l-xxx1.yyy.cn2,l-xxx2.yyy.cn2,l-xxx3.yyy.cn2,l-xxx4.yyy.cn2</span><br><span class="line">    ...</span><br><span class="line">    contact_groups           common-contact</span><br><span class="line">    check_command            check_tcp!8080</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这类检测依然要依赖应用中心: 在 <a href="">应用中心的角色定位与功能总结</a> 这篇文章里曾提及, 通过应用的标准化接入流程, 服务发布或启动的时候需要上报本次启动的相关信息给应用中心, 其中包括了服务所在主机地址及端口号; 每当有新服务注册, 我们会通过类似 service-generator 的方式生成对应的检测配置项;</p><h3 id="淡化-contacts-报警通知逻辑外置"><a href="#淡化-contacts-报警通知逻辑外置" class="headerlink" title="淡化 contacts, 报警通知逻辑外置"></a><strong>淡化 contacts, 报警通知逻辑外置</strong></h3><p>对于一个报警系统而言, 最核心的功能就是检测异常; 除此之外, nagios 还可以通过 contact 配置通知到负责人, 然而这并不是报警系统的核心; 这里就要提到第二个被我们转移到旁路系统的功能: <strong>报警通知逻辑</strong>;<br>一个公司有那么多工程师, 如果完全依靠 nagios 来实现通知, 就得把每个人的信息都配到文件里, 同时还要维护他们的群组关系, 按照 appcode 分类, 这无疑会增加 nagios 的配置量, 为运维带来复杂性; 况且, 通知逻辑并非 nagios 的核心功能, 权衡之下, 我们选择淡化 nagios 的 contact / contactgroup 元素, 让所有 service 的联系人都使用一个泛化的 common-contact, 而这个 common-contact 的通知命令, 会根据报警的内容, 自动选择合适的负责人;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">define contactgroup &#123;</span><br><span class="line">        contactgroup_name               common-contact</span><br><span class="line">        ...</span><br><span class="line">        service_notification_commands   alert-notify</span><br><span class="line">        host_notification_commands      alert-notify</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">define <span class="built_in">command</span> &#123;</span><br><span class="line">command_namealert-notify</span><br><span class="line">command_line/usr/bin/python /etc/nagios/objects/xxx/alert_notify.py <span class="string">"<span class="variable">$NOTIFICATIONTYPE</span>$"</span> <span class="string">"<span class="variable">$HOSTNAME</span>$"</span> <span class="string">"<span class="variable">$HOSTALIAS</span>$"</span> <span class="string">"<span class="variable">$HOSTADDRESS</span>$"</span> <span class="string">"<span class="variable">$HOSTSTATE</span>$"</span> <span class="string">"<span class="variable">$HOSTOUTPUT</span>$"</span> <span class="string">"<span class="variable">$SERVICEDESC</span>$"</span> <span class="string">"<span class="variable">$SERVICESTATE</span>$"</span> <span class="string">"<span class="variable">$LONGDATETIME</span>$"</span> <span class="string">"<span class="variable">$SERVICEOUTPUT</span>$"</span> <span class="string">"<span class="variable">$CONTACTEMAIL</span>$"</span> <span class="string">"<span class="variable">$CONTACTALIAS</span>$"</span> <span class="string">"<span class="variable">$CONTACTNAME</span>$"</span> &gt;&gt; /var/<span class="built_in">log</span>/nagios/alert.log.$(date +%F-%H) 2&gt;&amp;1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在 alert-notify 脚本中, 会根据报警内容查询应用中心, 选择合适的报警联系人, 合适的报警方式, 通知到位; 如果是 nagios 发送通知, 就只有配置的固定一组人会收到报警, 而改成这种外部的方式, 就具有了相当的灵活性, 甚至我们可以根据报警处理的进度反馈择机作报警升级处理, 比如:<br>(1) 报警方式从微信变成短信, 再变成电话语音报警;<br>(2) 报警联系人按照应用树组织架构, 从直接负责人, 逐步升级到 项目TL, 部门主管, 技术总监, VP, CEO 等;</p><h2 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h2><ul><li><a href="">应用中心的角色定位与功能总结</a></li><li><a href="">系统动态配置文件的运维经验总结</a></li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="http://www.ttlsa.com/nagios/nagios-sms-notification/" target="_blank" rel="noopener">nagios 短信报警通知</a></li><li><a href="https://www.linuxidc.com/Linux/2013-05/84848p2.htm" target="_blank" rel="noopener">Nagios搭建与配置详解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;nagios 的优点在于其插件拓展式的设计, 不过 nagios 给 ops 映像最深刻的, 是其出离复杂的配置文件; nagios 真的可以说是把配置文件当数据库使了;&lt;br&gt;作为备忘, 本文主要梳理 nagios 配置文件中的各种角色的关系与交互流程, 并就日常工作的经验总结一些 nagios 配置文件的部署及运维实践;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="alert" scheme="http://zshell.cc/categories/alert/"/>
    
      <category term="nagios" scheme="http://zshell.cc/categories/alert/nagios/"/>
    
    
      <category term="alert:nagios" scheme="http://zshell.cc/tags/alert-nagios/"/>
    
      <category term="devops 实践" scheme="http://zshell.cc/tags/devops-%E5%AE%9E%E8%B7%B5/"/>
    
      <category term="运维自动化" scheme="http://zshell.cc/tags/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>python module 使用总结: MySQLdb</title>
    <link href="http://zshell.cc/2017/08/01/python-module--python_module_%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93_MySQLdb/"/>
    <id>http://zshell.cc/2017/08/01/python-module--python_module_使用总结_MySQLdb/</id>
    <published>2017-08-01T15:06:08.000Z</published>
    <updated>2018-03-24T07:08:03.418Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><code>MySQLdb</code> 模块是 python 与 mysql 交互的较为底层的接口, 不过它依然是在更为底层的 <code>_mysql</code> 模块之上又作了一层包装;<br><code>_mysql</code> 才是真正的直接面向 mysql 原生 C 接口的简单适配层, 而 <code>MySQLdb</code> 则在 <code>_mysql</code> 之上作了更多的关于类型转换等抽象包装;<br>考虑到 <code>MySQLdb</code> 模块与一些 python ORM 框架的关系, <code>MySQLdb</code> 与 python 的关系可以类比为 jdbc 之于 java;<br>如果是复杂的系统, 我们肯定会选择 ORM 框架, 不过对于一些简单的小工具, 定时小任务等, 本身没什么复杂的数据库操作, 那就用 MySQLdb 最方便了;<br>本文基于 <code>MySQL-python 1.2.5</code> 对 MySQLdb 作一些使用上的总结;</p></blockquote><a id="more"></a><hr><h3 id="MySQLdb-的基本操作"><a href="#MySQLdb-的基本操作" class="headerlink" title="MySQLdb 的基本操作"></a><strong>MySQLdb 的基本操作</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"><span class="comment"># 获得 mysql 的一个连接</span></span><br><span class="line">conn = MySQLdb.connect(host=<span class="string">'10.64.0.11'</span>, user=<span class="string">'xxx'</span>, passwd=<span class="string">'yyy'</span>, db=<span class="string">"zzz"</span>, port=<span class="number">3306</span>, charset=<span class="string">"utf8"</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># cursor 游标, 是 MySQLdb 中与 mysql 增删改查数据交互的对象</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line">    <span class="comment"># 数据库操作</span></span><br><span class="line">    cur.execute(<span class="string">"...sql..."</span>)</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># 提交事务</span></span><br><span class="line">    conn.commit()</span><br><span class="line"><span class="keyword">except</span> Exception, e:</span><br><span class="line">    <span class="comment"># 回滚</span></span><br><span class="line">    conn.rollback()</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># 关闭连接, 释放资源</span></span><br><span class="line">    conn.close()</span><br></pre></td></tr></table></figure><p>以上是一个 MySQLdb 使用的完整流程, 下面是具体的使用细节与注意点总结;</p><h3 id="MySQLdb-cursor-execute-cursor-executemany-方法"><a href="#MySQLdb-cursor-execute-cursor-executemany-方法" class="headerlink" title="MySQLdb cursor.execute / cursor.executemany 方法"></a><strong>MySQLdb cursor.execute / cursor.executemany 方法</strong></h3><h4 id="cursor-execute-方法"><a href="#cursor-execute-方法" class="headerlink" title="cursor.execute 方法"></a><strong>cursor.execute 方法</strong></h4><p>MySQLdb 执行数据操纵的关键点就在于 cursor.execute 方法, 所有包括增删改查在内皆是以此方法执行的, 以下是该方法的代码:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(self, query, args=None)</span>:</span></span><br><span class="line">    <span class="keyword">del</span> self.messages[:]</span><br><span class="line">    db = self._get_db()</span><br><span class="line">    <span class="keyword">if</span> isinstance(query, unicode):</span><br><span class="line">        query = query.encode(db.unicode_literal.charset)</span><br><span class="line">    <span class="keyword">if</span> args <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># 针对 args 为 dict 的特殊情况处理</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(args, dict):</span><br><span class="line">            query = query % dict((key, db.literal(item)) <span class="keyword">for</span> key, item <span class="keyword">in</span> args.iteritems())</span><br><span class="line">        <span class="comment"># 其余的情况: args 为 tuple 或单个 value</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            query = query % tuple([db.literal(item) <span class="keyword">for</span> item <span class="keyword">in</span> args])</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = <span class="keyword">None</span></span><br><span class="line">        r = self._query(query)</span><br><span class="line">    <span class="keyword">except</span> TypeError, m:</span><br><span class="line">        <span class="keyword">if</span> m.args[<span class="number">0</span>] <span class="keyword">in</span> (<span class="string">"not enough arguments for format string"</span>, <span class="string">"not all arguments converted"</span>):</span><br><span class="line">            self.messages.append((ProgrammingError, m.args[<span class="number">0</span>]))</span><br><span class="line">            self.errorhandler(self, ProgrammingError, m.args[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.messages.append((TypeError, m)) </span><br><span class="line">            self.errorhandler(self, TypeError, m)</span><br><span class="line">    <span class="keyword">except</span> (SystemExit, KeyboardInterrupt):</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        exc, value, tb = sys.exc_info()</span><br><span class="line">        <span class="keyword">del</span> tb</span><br><span class="line">        self.messages.append((exc, value))</span><br><span class="line">        self.errorhandler(self, exc, value)</span><br><span class="line">    self._executed = query</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._defer_warnings: self._warning_check()</span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure></p><p>该方法接收一个名为 <code>query</code> 的 sql 字符串, 另外还可选附带参数 <code>args</code>, 所以该方法存在两种主要的用法:<br>1.预先格式化好 sql 字符串, 然后不带参数直接 execute:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sql = <span class="string">"select * from xxx where update_time = %s"</span> % datetime.datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">cursor.execute(sql)</span><br></pre></td></tr></table></figure></p><p>这种是保守的方法, 参数处理完全由 python 原生的格式化字符串完成, cursor.execute 方法只管执行 sql 就好;<br>这种方法的优点是省事, 坑少;<br>&nbsp;<br>2.将参数传给 execute 方法的 <code>args</code>, 这种使用方法有几个坑, 需要注意一下;<br>该方法有一段注释, 我单独提了出来, 注释中对 args 参数有如下描述:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    args -- optional sequence or mapping, parameters to use with query.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: If args is a sequence, then %s must be used as the</span></span><br><span class="line"><span class="string">    parameter placeholder in the query. If a mapping is used,</span></span><br><span class="line"><span class="string">    %(key)s must be used as the placeholder.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></p><p>(1) 注释中提到的坑, 就是说无论传的参数是一个 list/tuple, 还是 dict, 参数占位符类型都必须是字符串(%s | %(key)s ):<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不能是 id = %d, 只能是 id = %s</span></span><br><span class="line">sql = <span class="string">'select * from xxx where id  = %s'</span></span><br></pre></td></tr></table></figure></p><p>因为 execute 方法里处理参数时, 会对参数作 <code>db.literal(item)</code> 处理, 将参数首先转为字符串, 这时占位符如果是 %d 等其他类型, 就报错了;</p><p>&nbsp;<br>(2) 注释中另一个隐型的坑, 是这个 <code>args</code> 必须是 list / tuple / dict 中的一个, 哪怕只有一个占位数据, 也必须写成 list 或 tuple 类型:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cursor.execute(sql, (<span class="number">2</span>,))</span><br><span class="line">cursor.execute(sql, [<span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><p>如果希望以 tuple 形式表示唯一一个参数, 必须注意加上 逗号, 因为不加逗号就算外面包了括号也会识别为其本身的类型:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> type((<span class="number">1</span>))</span><br><span class="line">&lt;type <span class="string">'int'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> type((<span class="string">'1'</span>))</span><br><span class="line">&lt;type <span class="string">'str'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> type((<span class="number">1</span>,))</span><br><span class="line">&lt;type <span class="string">'tuple'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> type((<span class="string">'1'</span>,))</span><br><span class="line">&lt;type <span class="string">'tuple'</span>&gt;</span><br></pre></td></tr></table></figure></p><p>其实这个坑是在 MySQL-python 1.2.5 版本中出现的问题; 在 1.2.3 版本中, execute 方法的逻辑是这么写的:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    query = query % db.literal(args)</span><br></pre></td></tr></table></figure></p><p>只要 args 非空, 就一律把它 to string; 而至于参数怎么转, 转成什么样, 就看参数自己了;<br>这么做确实灵活了, 但是也可能带来一些不确定性, 1.2.5 的版本将参数限定为 list / tuple / dict, 然后对集合内的每个元素再针对性 to string, 一定程度上控制了参数的规范性;<br>&nbsp;</p><h4 id="cursor-executemany-方法"><a href="#cursor-executemany-方法" class="headerlink" title="cursor.executemany 方法"></a><strong>cursor.executemany 方法</strong></h4><p>executemany 方法是 execute 方法的批量化, 这个方法的有效使用范围其实很狭窄, 仅针对 insert 操作有性能提升, 其余操作在性能上均与 execute 无异;<br>下面是该方法的代码:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> self.messages[:]</span><br><span class="line">db = self._get_db()</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> args: <span class="keyword">return</span></span><br><span class="line"><span class="keyword">if</span> isinstance(query, unicode):</span><br><span class="line">    query = query.encode(db.unicode_literal.charset)</span><br><span class="line"><span class="comment"># 正则匹配 insert 操作</span></span><br><span class="line">m = insert_values.search(query)</span><br><span class="line"><span class="comment"># 不是 insert 操作, 那就 for 循环挨个执行而已</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> m:</span><br><span class="line">    r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> args:</span><br><span class="line">        r = r + self.execute(query, a)</span><br><span class="line">    <span class="keyword">return</span> r</span><br><span class="line">p = m.start(<span class="number">1</span>)</span><br><span class="line">e = m.end(<span class="number">1</span>)</span><br><span class="line">qv = m.group(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 下面是针对 insert 的处理</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    q = []</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> args:</span><br><span class="line">        <span class="keyword">if</span> isinstance(a, dict):</span><br><span class="line">            q.append(qv % dict((key, db.literal(item))</span><br><span class="line">                               <span class="keyword">for</span> key, item <span class="keyword">in</span> a.iteritems()))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q.append(qv % tuple([db.literal(item) <span class="keyword">for</span> item <span class="keyword">in</span> a]))</span><br><span class="line"><span class="keyword">except</span> TypeError, msg:</span><br><span class="line">    <span class="keyword">if</span> msg.args[<span class="number">0</span>] <span class="keyword">in</span> (<span class="string">"not enough arguments for format string"</span>,</span><br><span class="line">                       <span class="string">"not all arguments converted"</span>):</span><br><span class="line">        self.errorhandler(self, ProgrammingError, msg.args[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.errorhandler(self, TypeError, msg)</span><br><span class="line"><span class="keyword">except</span> (SystemExit, KeyboardInterrupt):</span><br><span class="line">    <span class="keyword">raise</span></span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    exc, value, tb = sys.exc_info()</span><br><span class="line">    <span class="keyword">del</span> tb</span><br><span class="line">    self.errorhandler(self, exc, value)</span><br><span class="line"><span class="comment"># 批量化执行, 提高处理性能</span></span><br><span class="line">r = self._query(<span class="string">'\n'</span>.join([query[:p], <span class="string">',\n'</span>.join(q), query[e:]]))</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> self._defer_warnings: self._warning_check()</span><br><span class="line"><span class="keyword">return</span> r</span><br></pre></td></tr></table></figure></p><p>从代码里可以看到, 方法先对传入的 sql 语句作一次匹配, 判断其是否是 insert 操作, 其中 insert_values 是一个 regex, 专门匹配 insert 语句:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">restr = <span class="string">r"\svalues\s*(\([^()']*(?:(?:(?:\(.*\))|'[^\\']*(?:\\.[^\\']*)*')[^()']*)*\))"</span></span><br><span class="line">insert_values = re.compile(restr, re.S | re.I | re.X)</span><br></pre></td></tr></table></figure></p><p>针对 insert 语句, 其最后的执行是批量的, 以提高执行效率:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = self._query(<span class="string">'\n'</span>.join([query[:p], <span class="string">',\n'</span>.join(q), query[e:]]))</span><br></pre></td></tr></table></figure></p><p>但是而其他的语句, 却只能在一个 for 循环里, 挨个执行 execute 方法, 这就没什么优势了;<br>不过这个方法还有一个大坑: 对于 update 和 delete 操作, 使用 executemany 至少不会比 execute 差, 但是对于 query, 它批量执行完一堆 query 操作后去 fetch 结果集, 只能拿到最后执行的 query 的结果, 前面的都被覆盖了; 所以, query 操作不能使用 executemany 方法;<br>&nbsp;<br>在使用方面, executemany 的坑和 execute 是差不多的, 下面是一个例子:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># executemany 传入的 args 可以是 list 也可以是 tuple</span></span><br><span class="line">cur.executemany(<span class="string">'select * from xxx where yyy = %s'</span>, [(<span class="number">1</span>,), (<span class="number">2</span>,)])</span><br></pre></td></tr></table></figure></p><h3 id="MySQLdb-的-query-结果集操作"><a href="#MySQLdb-的-query-结果集操作" class="headerlink" title="MySQLdb 的 query 结果集操作"></a><strong>MySQLdb 的 query 结果集操作</strong></h3><p>MySQLdb 的 query 操作, 主要有以下三种结果集的获取方法:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cursor.execute(<span class="string">"...sql..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得所有的 tuple 结果集的一个 list</span></span><br><span class="line"><span class="meta">@return list[tuple(elem1, elem2, elem3 ...)]</span></span><br><span class="line">tuple_data_list = cursor.fetchall()</span><br><span class="line"><span class="keyword">for</span> tuple_data <span class="keyword">in</span> tuple_data_list:</span><br><span class="line">    xxx = tuple_data[<span class="number">0</span>]</span><br><span class="line">    yyy = tuple_data[<span class="number">1</span>]</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用迭代器的方式, 返回当前游标所对应的 tuple 结果集, 迭代到最后方法返回 None</span></span><br><span class="line"><span class="meta">@return tuple(elem1, elem2, elem3 ...)</span></span><br><span class="line">tuple_data = cursor.fetchone()</span><br><span class="line"><span class="keyword">while</span> tuple_data:</span><br><span class="line">    <span class="comment"># deal with tuple_data</span></span><br><span class="line">    ...</span><br><span class="line">    tuple_data = cursor.fetchone()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 折中的一种方法, 指定返回 size 个 tuple 结果集 组成一个 list;</span></span><br><span class="line"><span class="comment"># 若 指定 size 小于 总的结果集数量, 则返回全部数据集;</span></span><br><span class="line"><span class="meta">@return list[tuple(elem1, elem2, elem3 ...)]</span></span><br><span class="line">tuple_data_list = cursor.fetchmany(size)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h3 id="MySQLdb-的事务操作"><a href="#MySQLdb-的事务操作" class="headerlink" title="MySQLdb 的事务操作"></a><strong>MySQLdb 的事务操作</strong></h3><p>MySQLdb 默认不会自动 commit, 所有的增删改操作都必须手动 commit 才能真正写回数据库;<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">conn = MySQLdb.connect(host=<span class="string">'10.64.0.11'</span>, user=<span class="string">'xxx'</span>, passwd=<span class="string">'yyy'</span>, db=<span class="string">"zzz"</span>, port=<span class="number">3306</span>, charset=<span class="string">"utf8"</span>)</span><br><span class="line">SQL = <span class="string">'update xxx set yyy = zzz'</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cur.execute(SQL,(<span class="number">2</span>,))</span><br><span class="line">    <span class="comment"># 手动 commit 提交事务</span></span><br><span class="line">    conn.commit()</span><br><span class="line"><span class="keyword">except</span> Exception, e:</span><br><span class="line">    <span class="comment"># 手动回滚</span></span><br><span class="line">    conn.rollback()</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br></pre></td></tr></table></figure></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="https://www.cnblogs.com/franknihao/p/7267182.html" target="_blank" rel="noopener">MySQLdb的安装与使用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;MySQLdb&lt;/code&gt; 模块是 python 与 mysql 交互的较为底层的接口, 不过它依然是在更为底层的 &lt;code&gt;_mysql&lt;/code&gt; 模块之上又作了一层包装;&lt;br&gt;&lt;code&gt;_mysql&lt;/code&gt; 才是真正的直接面向 mysql 原生 C 接口的简单适配层, 而 &lt;code&gt;MySQLdb&lt;/code&gt; 则在 &lt;code&gt;_mysql&lt;/code&gt; 之上作了更多的关于类型转换等抽象包装;&lt;br&gt;考虑到 &lt;code&gt;MySQLdb&lt;/code&gt; 模块与一些 python ORM 框架的关系, &lt;code&gt;MySQLdb&lt;/code&gt; 与 python 的关系可以类比为 jdbc 之于 java;&lt;br&gt;如果是复杂的系统, 我们肯定会选择 ORM 框架, 不过对于一些简单的小工具, 定时小任务等, 本身没什么复杂的数据库操作, 那就用 MySQLdb 最方便了;&lt;br&gt;本文基于 &lt;code&gt;MySQL-python 1.2.5&lt;/code&gt; 对 MySQLdb 作一些使用上的总结;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="python" scheme="http://zshell.cc/categories/python/"/>
    
      <category term="module" scheme="http://zshell.cc/categories/python/module/"/>
    
    
      <category term="python:module" scheme="http://zshell.cc/tags/python-module/"/>
    
      <category term="python" scheme="http://zshell.cc/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>jcmd: jvm 管理的另类工具</title>
    <link href="http://zshell.cc/2017/06/25/jvm-tools--jcmd_jvm%E7%AE%A1%E7%90%86%E7%9A%84%E5%8F%A6%E7%B1%BB%E5%B7%A5%E5%85%B7/"/>
    <id>http://zshell.cc/2017/06/25/jvm-tools--jcmd_jvm管理的另类工具/</id>
    <published>2017-06-25T11:13:05.000Z</published>
    <updated>2018-05-20T11:14:11.159Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>曾经 oracle 向我们提供了一套 jvm 管理与诊断问题的 “工具全家桶”: jps, jstack, jmap, jstat, jhat, jinfo 等等, 我们针对不同的情景使用不同的工具, 解决特定的问题;<br>现在, oracle 在 jdk7 之后又为我们带来了一个全能的工具 jcmd; 它最重要的功能是启动 java flight recorder, 不过 oracle 在设计该命令的时候, “不小心” 为它附加上了一些其他功能, 从而将原本平静的水面搅起了波澜;</p></blockquote><a id="more"></a><hr><h3 id="jcmd-工具的定位"><a href="#jcmd-工具的定位" class="headerlink" title="jcmd 工具的定位"></a><strong>jcmd 工具的定位</strong></h3><p>jcmd 是 jdk7 之后新增的工具, 它是 java flight recorder 的唯一启动方式, 详细的内容请见 <a href="">java flight recorder 的使用</a>; 不过, oracle 顺手又为其附带了一些 “便捷” 小工具:</p><ol><li>列举 jvm 进程 (对标 jps)</li><li>dump 栈信息 (对标 jstack)</li><li>dump 堆信息 (对标 jmap -dump)</li><li>统计类信息 (对标 jmap -histo)</li><li>获取系统信息 (对标 jinfo)</li></ol><p>这样一下子就有意思了, jcmd 似乎有了想要取代其他命令的野心; 下面来具体介绍一下 jcmd 都能顺手做些什么事情;</p><h3 id="jps-类似功能"><a href="#jps-类似功能" class="headerlink" title="jps 类似功能"></a><strong>jps 类似功能</strong></h3><p>jcmd 命令不带任何选项或者使用 -l 选项, 都可以打印当前用户下运行的虚拟机进程;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jps</span></span><br><span class="line">sudo -u xxx jps -l</span><br><span class="line"><span class="comment"># jcmd</span></span><br><span class="line">sudo -u xxx jcmd [-l]</span><br></pre></td></tr></table></figure></p><h3 id="查看-jcmd-对指定虚拟机能做的事情"><a href="#查看-jcmd-对指定虚拟机能做的事情" class="headerlink" title="查看 jcmd 对指定虚拟机能做的事情"></a><strong>查看 jcmd 对指定虚拟机能做的事情</strong></h3><p>jcmd 确实神通广大, 但是再厉害的大夫也得病人配合工作才行, 比如在低版本 jre 上跑的程序肯定无法使用 flight recorder 抓 dump;<br>当使用 jcmd 拿到了目标 vmid 后, 使用如下命令可以查看 jcmd 对目标 jvm 能够使用的功能:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u xxx jcmd &#123;vmid&#125; <span class="built_in">help</span></span><br></pre></td></tr></table></figure></p><p>输出可以使用的功能列举如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># flight recorder 相关功能</span></span><br><span class="line">JFR.stop</span><br><span class="line">JFR.start</span><br><span class="line">JFR.dump</span><br><span class="line">JFR.check</span><br><span class="line"></span><br><span class="line"><span class="comment"># jmx 相关功能</span></span><br><span class="line">ManagementAgent.stop</span><br><span class="line">ManagementAgent.start_local</span><br><span class="line">ManagementAgent.start</span><br><span class="line"></span><br><span class="line"><span class="comment"># jstack 相关功能</span></span><br><span class="line">Thread.print</span><br><span class="line"></span><br><span class="line"><span class="comment"># jmap 相关功能</span></span><br><span class="line">GC.class_stats</span><br><span class="line">GC.class_histogram</span><br><span class="line">GC.heap_dump</span><br><span class="line"></span><br><span class="line"><span class="comment"># jinfo 相关功能</span></span><br><span class="line">VM.flags</span><br><span class="line">VM.system_properties</span><br><span class="line">VM.command_line</span><br><span class="line">VM.version</span><br><span class="line"></span><br><span class="line"><span class="comment"># gc 相关</span></span><br><span class="line">GC.run_finalization <span class="comment"># System.runFinalization()</span></span><br><span class="line">GC.run              <span class="comment"># System.gc()</span></span><br><span class="line">GC.rotate_log       <span class="comment"># 切割 gc log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他</span></span><br><span class="line">VM.native_memory</span><br><span class="line">VM.check_commercial_features</span><br><span class="line">VM.unlock_commercial_features</span><br><span class="line">VM.uptime</span><br></pre></td></tr></table></figure></p><h3 id="jstack-类似功能"><a href="#jstack-类似功能" class="headerlink" title="jstack 类似功能"></a><strong>jstack 类似功能</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u xxx jcmd &#123;vmid&#125; Thread.print</span><br></pre></td></tr></table></figure><p>以上命令输出的内容与以下使用 jstack 命令的输出一致:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u xxx jstack -l &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><h3 id="jmap-类似功能"><a href="#jmap-类似功能" class="headerlink" title="jmap 类似功能"></a><strong>jmap 类似功能</strong></h3><p>与 jmap 相关的功能主要是以下四类:<br>(1) 堆区对象的总体统计<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jmap 的实现</span></span><br><span class="line">sudo -u xxx jmap -heap &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><p>jcmd 没有提供与 jmap -heap 类似的功能;</p><p>(2) 堆区对象的详细直方图统计<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcmd 的实现</span></span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; GC.class_histogram</span><br><span class="line"><span class="comment"># jmap 的实现</span></span><br><span class="line">sudo -u xxx jmap -histo[:live] &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><p>(3) metaspace 的信息统计<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcmd 的实现</span></span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; GC.class_stats</span><br><span class="line"><span class="comment"># jmap 的实现</span></span><br><span class="line">sudo -u xxx jmap -clstats &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><p>虽然都是关于 jdk8 metaspace 的信息统计, 不过 jcmd GC.class_stats 与 jmap -clstats 的输出内容没什么关联;<br>另外, 使用 jcmd GC.class_stats 功能, 需要开启 jvm 选项 <code>UnlockDiagnosticVMOptions</code>;</p><p>(4) 堆区对象的 dump<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcmd 的实现</span></span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; GC.heap_dump &#123;file_path&#125;</span><br><span class="line"><span class="comment"># jmap 的实现</span></span><br><span class="line">sudo -u xxx jmap -dump[:live],format=b,file=&#123;file_path&#125; &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><h3 id="jinfo-类似功能"><a href="#jinfo-类似功能" class="headerlink" title="jinfo 类似功能"></a><strong>jinfo 类似功能</strong></h3><p>与 jinfo 相关的功能主要是以下两类:<br>(1) 打印 jvm 的系统信息, 包括系统参数, 版本等<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcmd 的实现</span></span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; VM.system_properties</span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; VM.version</span><br><span class="line"><span class="comment"># jmap 的实现</span></span><br><span class="line">sudo -u xxx jinfo -sysprops &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><p>(2) 打印 jvm 的选项<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcmd 的实现</span></span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; VM.command_line</span><br><span class="line"><span class="comment"># jmap 的实现</span></span><br><span class="line">sudo -u xxx jinfo -flags &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><p>(3) 修改 jvm 的选项<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jcmd 的部分实现</span></span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; VM.unlock_commercial_features</span><br><span class="line">sudo -u xxx jcmd &#123;vmid&#125; VM.check_commercial_features</span><br><span class="line"><span class="comment"># jmap 的实现</span></span><br><span class="line">sudo -u xxx jinfo -flag [+|-]&#123;option_name&#125; &#123;vmid&#125;</span><br><span class="line">sudo -u xxx jinfo -flag &#123;option_name&#125;=&#123;value&#125; &#123;vmid&#125;</span><br></pre></td></tr></table></figure></p><p>关于修改 jvm 选项, 只能说 jcmd 几乎是没有相关的功能, 其只能操控与 flight recorder 配套的 <code>UnlockCommercialFeatures</code> 选项而已;</p><h3 id="jcmd-使用总结"><a href="#jcmd-使用总结" class="headerlink" title="jcmd 使用总结"></a><strong>jcmd 使用总结</strong></h3><p>往不好听的讲, 除了 java flight recorder 之外, jcmd 其余的功能只能说是 “鸡肋”: 只有 jps, jstack 可以算完全覆盖了其相关功能, jmap 勉强可以算覆盖了其相关功能;<br>除此之外, jinfo 的部分功能没有实现, jstat 的所有功能都没有实现; 而且 jcmd 的选项名字一般都比较长, 不容易记住, 必须依赖 <code>jcmd {vmid} help</code> 打印相关内容, 给使用带来了不便;<br>总体来说, 除了 java flight recorder 必须要使用 jcmd 之外, 其余的功能暂时还是建议使用传统的工具来解决问题; jcmd 的野心还得继续培养, 等以后 oracle 发布新版本的时候再继续观察吧;</p><h3 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h3><ul><li><a href="">java flight recorder 的使用</a></li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="http://fengfu.io/2016/12/14/jcmd%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/" target="_blank" rel="noopener">jcmd命令详解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;曾经 oracle 向我们提供了一套 jvm 管理与诊断问题的 “工具全家桶”: jps, jstack, jmap, jstat, jhat, jinfo 等等, 我们针对不同的情景使用不同的工具, 解决特定的问题;&lt;br&gt;现在, oracle 在 jdk7 之后又为我们带来了一个全能的工具 jcmd; 它最重要的功能是启动 java flight recorder, 不过 oracle 在设计该命令的时候, “不小心” 为它附加上了一些其他功能, 从而将原本平静的水面搅起了波澜;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="jvm" scheme="http://zshell.cc/categories/jvm/"/>
    
      <category term="tools" scheme="http://zshell.cc/categories/jvm/tools/"/>
    
    
      <category term="jvm:tools" scheme="http://zshell.cc/tags/jvm-tools/"/>
    
  </entry>
  
  <entry>
    <title>saltstack cheat sheet</title>
    <link href="http://zshell.cc/2017/05/13/saltstack--saltstack_cheat_sheet/"/>
    <id>http://zshell.cc/2017/05/13/saltstack--saltstack_cheat_sheet/</id>
    <published>2017-05-13T13:42:21.000Z</published>
    <updated>2018-01-27T14:53:07.577Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要整理日常 saltstack 使用时的最常用的一些命令,以供快速查阅;</p></blockquote><a id="more"></a><hr><h3 id="自由度最大的模块-cmd-模块"><a href="#自由度最大的模块-cmd-模块" class="headerlink" title="自由度最大的模块: cmd 模块"></a><strong>自由度最大的模块: cmd 模块</strong></h3><p>适用于登录 salt master 机器, 人工操作时执行;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cmd.run: 在 minions 上执行任意命令</span></span><br><span class="line">sudo salt * cmd.run <span class="string">"ls -l /etc/localtime"</span></span><br><span class="line">sudo salt * cmd.run uptime</span><br><span class="line"></span><br><span class="line"><span class="comment"># cmd.script: 在 master 上下发任意脚本至 minions 执行</span></span><br><span class="line">sudo salt * cmd.script salt://minion_exeute.sh <span class="string">"args1 args2"</span></span><br></pre></td></tr></table></figure></p><h3 id="控制-minions-的定时任务执行情况-cron-模块"><a href="#控制-minions-的定时任务执行情况-cron-模块" class="headerlink" title="控制 minions 的定时任务执行情况: cron 模块"></a><strong>控制 minions 的定时任务执行情况: cron 模块</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看指定用户的 cron 内容</span></span><br><span class="line">sudo salt * cron.raw_cron root</span><br><span class="line"><span class="comment"># 为指定用户添加指定任务</span></span><br><span class="line">sudo salt * cron.set_job root <span class="string">'*'</span> <span class="string">'*'</span> <span class="string">'*'</span> <span class="string">'*'</span> <span class="string">'*'</span> /home/minion_execute.sh 1&gt;/dev/null</span><br><span class="line"><span class="comment"># 为指定用户删除指定任务</span></span><br><span class="line">sudo salt * cron.rm_job root <span class="string">'*'</span> <span class="string">'*'</span> <span class="string">'*'</span> <span class="string">'*'</span> <span class="string">'*'</span> /home/minion_execute.sh 1&gt;/dev/null</span><br></pre></td></tr></table></figure><h3 id="master-与-minions-的文件传输-cp-模块"><a href="#master-与-minions-的文件传输-cp-模块" class="headerlink" title="master 与 minions 的文件传输: cp 模块"></a><strong>master 与 minions 的文件传输: cp 模块</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推送文件到 minions 指定路径 (只能推送文件, 不能推送目录)</span></span><br><span class="line">sudo salt * cp.get_file salt://target_file /minion_path</span><br><span class="line"><span class="comment"># 推送目录到 minions 指定路径</span></span><br><span class="line">suod salt * cp.get_dir salt://target_dir /minion_path</span><br><span class="line"><span class="comment"># 下载指定 url 的内容到 minions 指定路径 (不限于本地路径, 更加广泛)</span></span><br><span class="line">sudo salt * cp.get_url salt://target_file /minion_path</span><br><span class="line">sudo salt * cp.get_url https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz /minion_path</span><br></pre></td></tr></table></figure><h3 id="服务启停控制-systemd-模块"><a href="#服务启停控制-systemd-模块" class="headerlink" title="服务启停控制: systemd 模块"></a><strong>服务启停控制: systemd 模块</strong></h3><p>salt.modules.systemd 模块是以 systemd 与 systemctl 为基础的, 尽管其命令多以 serice 开头, 不过该模块和 sysvinit 的 service 命令应该没什么关系;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别对应了 systemctl [enable, disable, start, stop, status, restart] httpd.service</span></span><br><span class="line">sudo salt * service.enable httpd</span><br><span class="line">sudo salt * service.disable httpd</span><br><span class="line"></span><br><span class="line">sudo salt * service.start httpd</span><br><span class="line">sudo salt * service.stop httpd</span><br><span class="line">sudo salt * service.status httpd</span><br><span class="line">sudo salt * service.restart httpd</span><br></pre></td></tr></table></figure></p><h3 id="远程文件控制相关-file-模块"><a href="#远程文件控制相关-file-模块" class="headerlink" title="远程文件控制相关: file 模块"></a><strong>远程文件控制相关: file 模块</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件</span></span><br><span class="line">sudo salt * file.touch /opt/rsync_passwd</span><br><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">sudo salt * file.mkdir /opt/rsync</span><br><span class="line"><span class="comment"># 删除指定文件</span></span><br><span class="line">sudo salt * file.remove /opt/rsync_passwd</span><br><span class="line"><span class="comment"># 删除目录</span></span><br><span class="line">sudo salt * file.rmdir /opt/rsync</span><br><span class="line"></span><br><span class="line"><span class="comment"># sudo chown root:root /opt/rsync_passwd</span></span><br><span class="line">sudo salt * file.chown /opt/rsync_passwd root root</span><br><span class="line"><span class="comment"># sudo chmod 600 /etc/rsync_passwd</span></span><br><span class="line">sudo salt * file.set_mode /etc/rsync_passwd 600</span><br></pre></td></tr></table></figure><h3 id="salt-常用的状态检测"><a href="#salt-常用的状态检测" class="headerlink" title="salt 常用的状态检测"></a><strong>salt 常用的状态检测</strong></h3><p>包括:<br>master 与 minions 之间的连通性 check_ping 检查;<br>minions salt version, dependency version, system version 检查;<br>minions network ping 外网检查;<br>磁盘容量 check_disk 检查;<br>等等;<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试 salt 主从连通性</span></span><br><span class="line">sudo salt * test.ping</span><br><span class="line"><span class="comment"># 打印 salt 的版本以及 salt 依赖的第三方组件的版本</span></span><br><span class="line">sudo salt * test.versions_report</span><br><span class="line"><span class="comment"># 测试 minions 的网络 ping</span></span><br><span class="line">sudo salt * network.ping www.qunar.com</span><br><span class="line"><span class="comment"># 查看 minions 的磁盘使用情况</span></span><br><span class="line">sudo salt * disk.usage</span><br></pre></td></tr></table></figure></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><ul><li><a href="http://blog.csdn.net/shjh369/article/details/49799269" target="_blank" rel="noopener">服务自动化部署平台之Saltstack总结</a></li><li><a href="https://www.cnblogs.com/MacoLee/p/5753640.html" target="_blank" rel="noopener">Saltstack系列3: Saltstack常用模块及API</a></li><li><a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.file.html#salt.modules.file.rmdir" target="_blank" rel="noopener">SALT.MODULES.FILE</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要整理日常 saltstack 使用时的最常用的一些命令,以供快速查阅;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="saltstack" scheme="http://zshell.cc/categories/saltstack/"/>
    
    
      <category term="运维自动化" scheme="http://zshell.cc/tags/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
      <category term="cheat sheet" scheme="http://zshell.cc/tags/cheat-sheet/"/>
    
      <category term="saltstack" scheme="http://zshell.cc/tags/saltstack/"/>
    
  </entry>
  
  <entry>
    <title>du / df 使用及其区别</title>
    <link href="http://zshell.cc/2017/04/07/linux-disk--du_df%E4%BD%BF%E7%94%A8%E5%8F%8A%E5%85%B6%E5%8C%BA%E5%88%AB/"/>
    <id>http://zshell.cc/2017/04/07/linux-disk--du_df使用及其区别/</id>
    <published>2017-04-07T14:58:04.000Z</published>
    <updated>2018-04-06T13:22:05.098Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要是整理 磁盘使用量 相关的命令, 如 du, df 等;<br>接着, 一般性得总结这两个命令在实际工作中的应用;<br>然后再以 du, df 命令的区别为例, 讨论命令背后的逻辑, 工作中存在的问题, 最后引申出问题解决的工具: lsof;</p></blockquote><a id="more"></a><hr><h2 id="du命令"><a href="#du命令" class="headerlink" title="du命令"></a><strong>du命令</strong></h2><blockquote><p>estimate disk file space usage. — man du</p></blockquote><h3 id="du-的常用选项"><a href="#du-的常用选项" class="headerlink" title="du 的常用选项"></a><strong>du 的常用选项</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不加任何选项, 默认是 列举指定路径下, 每一个目录(递归所有的子目录)的大小</span></span><br><span class="line">sudo du /target_path</span><br><span class="line"><span class="comment"># 列举指定路径下所有的文件(包括目录与文件)的大小</span></span><br><span class="line">sudo du -a /target_path</span><br><span class="line"><span class="comment"># 以 human-readable 的形式, 列举目标路径的文件磁盘占用总大小(将该路径下所有子文件大小求和)</span></span><br><span class="line">sudo du -s /target_path</span><br><span class="line"><span class="comment"># 以指定路径下所有的子一级路径为 target, 以 human-readable 的方式列举其中每一个下的所有子文件大小之和</span></span><br><span class="line"><span class="comment"># (诊断 磁盘满问题 最常用的方式)</span></span><br><span class="line">sudo du -sh /target_path/*</span><br><span class="line"><span class="comment"># 除了其余选项该有的输出之外, 最后一行另附一个给定 target_path 下的 total 总和</span></span><br><span class="line"><span class="comment"># 理论上这与目标路径不含通配符的 -sh 输出结果是相同的</span></span><br><span class="line">sudo du -c /target_path</span><br></pre></td></tr></table></figure><h2 id="df-命令"><a href="#df-命令" class="headerlink" title="df 命令"></a><strong>df 命令</strong></h2><blockquote><p>file system disk space usage. — man df</p></blockquote><h3 id="df-的常用选项"><a href="#df-的常用选项" class="headerlink" title="df 的常用选项"></a><strong>df 的常用选项</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示给定的路径所挂载的磁盘分区的大小及使用量等</span></span><br><span class="line">df /target_path</span><br><span class="line"><span class="comment"># 以 MB 最小单位显示大小及使用量</span></span><br><span class="line">df --block-size=1m /target_path</span><br><span class="line">df -B 1m /target_path</span><br><span class="line"><span class="comment"># 以 human-readable 的方式显示 当前挂载的所有可用健康的文件系统 的大小, 使用量等情况</span></span><br><span class="line">df -h <span class="comment"># 1024</span></span><br><span class="line">df -H <span class="comment"># 1000</span></span><br><span class="line"><span class="comment"># 显示所有的文件系统, 包括 伪文件系统, 重复的, 不可访问的文件系统 (pseudo, duplicate, inaccessible)</span></span><br><span class="line">df -a</span><br><span class="line"><span class="comment"># 过滤 nfs 远程文件系统后的本地文件系统</span></span><br><span class="line">df -l</span><br></pre></td></tr></table></figure><p>&nbsp;<br><strong>一般性总结:</strong><br>df 命令主要关心的是磁盘分区的 size, 而不是具体某文件的占用大小;<br>所以 df 命令的主要运用场景是: <code>df -h</code>, 判断所挂载的每个分区的使用率, 是不是满了;<br>作为先决判断依据, 如果发现磁盘满了, 再接着使用 <code>du -sh</code> 等命令进一步排查;<br>&nbsp;</p><h2 id="du-与-df-命令的区别"><a href="#du-与-df-命令的区别" class="headerlink" title="du 与 df 命令的区别"></a><strong>du 与 df 命令的区别</strong></h2><h3 id="df-命令与-du-命令的工作原理"><a href="#df-命令与-du-命令的工作原理" class="headerlink" title="df 命令与 du 命令的工作原理"></a><strong>df 命令与 du 命令的工作原理</strong></h3><p>df 命令使用 系统调用 <code>statfs</code>, 获取磁盘分区的超级块 (super block) 使用情况;<br>du 命令使用 系统调用 <code>fstat</code>, 获取待统计文件的大小;</p><h3 id="df-命令与-du-命令可接受范围内不一致"><a href="#df-命令与-du-命令可接受范围内不一致" class="headerlink" title="df 命令与 du 命令可接受范围内不一致"></a><strong>df 命令与 du 命令可接受范围内不一致</strong></h3><p>[<strong>问题场景</strong>]: <em>du -s 与 df 核算精确结果总有差异;</em><br>&nbsp;<br>[<strong>原因</strong>]: du -s 命令通过将指定文件系统中所有的目录, 符号链接和文件使用的块数累加得到该文件系统使用的总块数, 这是上层用户级的数据;<br>df 命令通过查看文件系统磁盘块分配图得出总块数与剩余块数, 这是直接从底层获取的数据;<br>所以, 一些元数据信息(inode, super blocks 等)不会被上层的 du 命令计入在内, 而 df 命令由于直接获取的底层超级块的信息, 则会将其计入在内;<br>&nbsp;<br>[<strong>结论</strong>]: <em>这种差异属于系统性的差异, 是由命令的特点决定的, 无法改变;</em></p><h3 id="df-命令与-du-命令显著不一致"><a href="#df-命令与-du-命令显著不一致" class="headerlink" title="df 命令与 du 命令显著不一致"></a><strong>df 命令与 du 命令显著不一致</strong></h3><p>[<strong>问题场景</strong>]: <em>当一个被某进程持有其句柄的文件被删除后, 进程不释放句柄, du 将不会再统计该文件, 而 df 的使用量仍会将其计入在内;</em><br>&nbsp;<br>[<strong>原因</strong>]: 当文件句柄被进程持有, 尽管文件被删除, 目录项已经不存在该文件路径了, 但只要句柄不释放, 文件在磁盘上就不会真正删除该文件;<br>这样一来, 目录项不存在该文件了, du 命令就不会统计到该文件, 但文件没真正删除, 磁盘分区 super block 的信息就不会改变, df 命令仍会将其计入使用量;<br>&nbsp;<br>[<strong>结论</strong>]: <em>这种差异属于第三方因素干扰导致的差异, 且差异十分显著, 需要通过下一节所讨论的方式加以解决;</em></p><h3 id="问题解决方案"><a href="#问题解决方案" class="headerlink" title="问题解决方案"></a><strong>问题解决方案</strong></h3><p>磁盘满了, 但是有进程持有大文件的句柄, 无法真正从磁盘删除掉; 对于这类问题, 有如下两种解决方案:<br>1.配合使用 lsof 找出相关的 <code>幽灵文件</code> 的句柄持有情况(command 与 pid):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; sudo lsof | grep deleted</span><br><span class="line">nginx      4804      nobody   59u      REG253,1    110116  243425480 /usr/<span class="built_in">local</span>/openresty/nginx/client_body_temp/0068359496 (deleted)</span><br><span class="line">nginx      4819      nobody   51u      REG253,1    115876  243425480 /usr/<span class="built_in">local</span>/openresty/nginx/client_body_temp/0068359498 (deleted)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>然后 kill 掉进程 (或 restart 进程), 即可释放文件句柄;<br>当然, 本文是以 nginx 举例, 但实际上 nginx 对于日志文件的文件句柄释放, 有自己专有的方法, 具体内容请见本站另外两篇文章: <a href="https://zshell-zhang.github.io/2017/04/05/linux-process--linux_signals总体认识/#其他信号" target="_blank" rel="noopener">linux signals 总体认识#其他信号</a> 和 <a href="">nginx signals 处理</a>;<br>另外, 磁盘满的问题, 不能总是靠人肉登机器去解决, 我们需要一些自动化的方案来将我们从这种低级的操作中解放出来;<br>所以, 对于所有机器上都会遇到的日志文件不断累积占满磁盘的问题, 这篇文章介绍了解决方案: <a href="">logrotate 配置与运维</a>;<br>&nbsp;<br>2.如果进程很重要, 不能容忍任何时间范围内的服务不可用 (其实理论上这种情况属于单点瓶颈, 未能做到高可用), 则可以采用如下方式:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将文件写空</span></span><br><span class="line">sudo <span class="built_in">echo</span> &gt; file_path</span><br></pre></td></tr></table></figure></p><p>将文件内容间接删除, 这样即便句柄未释放, 但文件本身已经没有内容, 也就不再占用空间了;</p><h2 id="站内相关文章"><a href="#站内相关文章" class="headerlink" title="站内相关文章"></a><strong>站内相关文章</strong></h2><ul><li><a href="https://zshell-zhang.github.io/2018/01/15/linux-varlog--logrotate%E9%85%8D%E7%BD%AE%E4%B8%8E%E8%BF%90%E7%BB%B4/" target="_blank" rel="noopener">logrotate 配置与运维</a></li><li><a href="https://zshell-zhang.github.io/2017/04/05/linux-process--linux_signals总体认识/#其他信号" target="_blank" rel="noopener">linux signals 总体认识#其他信号</a></li><li><a href="">nginx signals 处理</a></li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><ul><li><a href="http://www.cnblogs.com/heyonggang/p/3644736.html" target="_blank" rel="noopener">df和du显示的磁盘空间使用情况不一致的原因及处理</a></li><li><a href="http://blog.csdn.net/guoguo1980/article/details/2324454" target="_blank" rel="noopener">linux lsof 详解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要是整理 磁盘使用量 相关的命令, 如 du, df 等;&lt;br&gt;接着, 一般性得总结这两个命令在实际工作中的应用;&lt;br&gt;然后再以 du, df 命令的区别为例, 讨论命令背后的逻辑, 工作中存在的问题, 最后引申出问题解决的工具: lsof;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://zshell.cc/categories/linux/"/>
    
      <category term="disk" scheme="http://zshell.cc/categories/linux/disk/"/>
    
    
      <category term="linux:disk" scheme="http://zshell.cc/tags/linux-disk/"/>
    
  </entry>
  
</feed>
